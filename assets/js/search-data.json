{
  
    
        "post0": {
            "title": "Fast.ai v3 Lesson 12 Notes: Advanced training techniques; ULMFiT from scratch",
            "content": "Overview . This lesson implements some really important training techniques today, all using callbacks: . MixUp: a data augmentation technique that dramatically improves results, particularly when you have less data, or can train for a longer time. | Label smoothing: which works particularly well with MixUp, and significantly improves results when you have noisy labels | Mixed precision training: which trains models around 3x faster in many situations. | It also implement XResNet: which is a tweaked version of the classic resnet architecture that provides substantial improvements. And, even more important, the development of it provides great insights into what makes an architecture work well. | Finally, the lesson show how to implement ULMFiT from scratch, including building an LSTM RNN, and looking at the various steps necessary to process natural language data to allow it to be passed to a neural network. | . Link to Lesson 12 Lecture . Even Better Image Training: Mixup/Label Smoothing . MixUp . (Notebook: 10b_mixup_label_smoothing.ipynb) . It’s quite possible that we don’t need much data augmentation for images anymore. FastAI’s experiments with a data augmentation called Mixup, they found that they could remove most other data augmentation and get amazingly good results. It’s really simple to do and you can also train with MixUp for a really long time and get really good results. . MixUp comes from the paper: mixup: Beyond Empirical Risk Minimization [2017]. This is quite an easy reading paper. | MixUp was shown to be a very effective training technique in the paper: Bag of Tricks for Image Classification with Convolutional Neural Networks [2019]. (This lesson will refer back to this paper a lot so it’s also worth reading fully.) | . Here is the table of results for the different training tricks tried in that paper: . . (NB with MixUp they ran for more epochs) . What is MixUp? We are going to take two different images and we are going to combine them. How? By simply making a convex combination of the two. So you do 30% of one image and 70% of the other image: . . You also have to do MixUp to the labels. So rather than being a one-hot encoded target, your target would become something like: . y = [0.3 (gas pump), 0.7 (dog)] . When we are generating these mixed up training examples on the fly we we need to pick how much of each image we will use. Let’s define a mixing proportion, $ lambda$, so our mixed image will be $ lambda x_i + (1- lambda) x_j$ . We will pick the $ lambda$ randomly each time, however we don’t want to just naively generate this from a uniform distribution. In the MixUp paper they explore how the mixing parameter affects performance and they get this plot (higher is worse): . . For good values we need to sample from a distribution that is more likely to pick numbers near 0 or near 1. A distribution that looks like this is the beta distribution. . It is one of the weirder distributions and it’s not very intuitive from its formula, but the shape of it looks like this for two different values of the parameters $ alpha$: . . The Beta distribution tends to generate number at the edges. If you compare that to the plot of prediction errors above you can see they are inverses of each other. . (Aside: this SO post about the intuition behind the Beta distribution is quite interesting). . Let’s look at the implementation of MixUp… . Original MixUp Algorithm: In the original article, the authors suggested three things: . Create two separate dataloaders and draw a batch from each at every iteration to mix them up | Draw a $ lambda$ value following a beta distribution with a parameter $ alpha$ (0.4 is suggested in their article) | Mix up the two batches with the same value $ lambda$. | Use one-hot encoded targets | While the approach above works very well, it’s not the fastest way we can do this. The main point that slows down this process is wanting two different batches at every iteration (which means loading twice the amount of images and applying to them the other data augmentation function). To avoid this slow down, we can be a little smarter and mixup a batch with a shuffled version of itself (this way the mixed up images are still different). This was a trick suggested in the MixUp paper. . FastAI MixUp Algorithm: FastAI employs a few tricks to improve it: . Create a single dataloader and draw a single batch, $X$, with labels $y$ from which we can create mixed up images by shuffling this batch. . | For each item in the batch pick a generate a vector of $ lambda$ values (Beta distribution with $ alpha=0.4$). To avoid potential duplicate mixups fix $ lambda$ values with: . λ = max(λ, 1-λ) . | Create a random permutation of the batch $X’$ and labels $y’$. . | Return the linear combination of the original batch and the random permutation: $ lambda X + (1- lambda) X’$. Likewise with the labels: $ lambda y + (1- lambda)y’$. . | The first trick is picking a different $ lambda$ for every image in the batch because fastai found that doing so made the network converge faster. . The second trick is using a single batch and shuffling it for MixUp instead of loading two batches. However, this strategy can create duplicates. Let’s say the batch has two images, we shuffle the batch and first mix Image0 with Image1 with $ lambda_1=0.1$, and then mix Image1 and Image0 with $ lambda=0.9$: . image0 * 0.1 + shuffle0 * (1-0.1) = image0 * 0.1 + image1 * 0.9 image1 * 0.9 + shuffle1 * (1-0.9) = image1 * 0.9 + image0 * 0.1 . These will be the same. Of course, we have to be a bit unlucky but in practice, they saw there was a drop in accuracy by using this without removing those near-duplicates. To avoid them, the tricks is to replace the vector of parameters we drew by: . λ = max(λ, 1-λ) . The beta distribution with the two parameters equal is symmetric in any case, and this way we insure that the biggest coefficient is always near the first image (the non-shuffled batch). . Here is the Callback code for MixUp. The begin_batch method implements the above algorithm: . class MixUp(Callback): _order = 90 #Runs after normalization and cuda def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α])) def begin_fit(self): self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func def begin_batch(self): if not self.in_train: return #Only mixup things during training λ = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device) λ = torch.stack([λ, 1-λ], 1) self.λ = unsqueeze(λ.max(1)[0], (1,2,3)) shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device) xb1,self.yb1 = self.xb[shuffle],self.yb[shuffle] self.run.xb = lin_comb(self.xb, xb1, self.λ) def after_fit(self): self.run.loss_func = self.old_loss_func def loss_func(self, pred, yb): if not self.in_train: return self.old_loss_func(pred, yb) with NoneReduce(self.old_loss_func) as loss_func: loss1 = loss_func(pred, yb) loss2 = loss_func(pred, self.yb1) loss = lin_comb(loss1, loss2, self.λ) return reduce_loss(loss, getattr(self.old_loss_func, &#39;reduction&#39;, &#39;mean&#39;)) . How do we modify the loss function? See the loss_func method above. Like when we coded up the cross-entropy loss, we don’t need to expand the target out into a full categorical distribution, we can instead just write a specialised version of cross-entropy for MixUp: . loss(output, new_target) = t * _loss(output, target1) + (1-t) * _loss(output, target2) . recalling that the cross-entropy formula is: (L =- sum_i x_i log p(z_i) ) . PyTorch loss functions like nn.CrossEntropy have a reduction attribute to specify how to calculate the loss of a whole batch from the individual losses, e.g. take the mean. | We want to do this reduction on the batch after the linear combination of the individual losses has been calculated. | So reduction needs to be turned off for the linear combination, then turned on afterwards. | . Question: Is there an intuitive way to understand why MixUp is better than other data augmentation techniques? . One of the things that’s really nice about MixUp is that it doesn’t require any domain specific thinking about the data augmentation. E.g. can I do vertical/horizontal flipping, how much can we rotate? lossiness: black padding, reflection padding etc. It’s also almost infinite in terms of the number of images it can create. . There are other similar things: . CutOut - delete a square and replace it with black or random pixels | CutMix - patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches | Find four different images and put them in four corners. | . These things actually get really good results and are not used so much. . Label Smoothing . (Jump_to lesson 12 video) . Another regularization technique that’s often used for classification is label smoothing, which deliberately introduces noise for the labels. It’s designed to make the model a little bit less certain of its decision by changing the target labels: instead of the hard prediction of exactly 1 for the correct class and 0 for all the others, we change the objective to prediction $1- epsilon$ for the correct class and $ frac{ epsilon}{k-1}$ for all the others, where $ epsilon$ is a small positive number and $k$ is the number of classes. . We can achieve this by updating the loss to: . [loss = (1- epsilon) ; mbox{ce}(i) + epsilon sum_j mbox{ce}(j) / (k-1)] . where $ mbox{ce}(x)$ is the cross-entropy of $x$ (i.e. $- log(p_x)$), and $i$ is the correct class. Typical value: $ epsilon=0.1$. . This is a really simple, but astonishingly effective way to handle noisy labels in your data. For example, in a medical problem where the diagnostic labels are not perfect. It turns out that if you use label smoothing, noisy labels generally aren’t that big an issue. Anecdotally, people have deliberately permuted their labels so they are 50% wrong, and they still get good results with label smoothing. This also could enable you to get training faster to check something works, before investing a lot of time in cleaning up your data. . Noisy labels not as big an issue as you’d think. . class LabelSmoothingCrossEntropy(nn.Module): def __init__(self, ε:float=0.1, reduction=&#39;mean&#39;): super().__init__() self.ε,self.reduction = ε,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction) nll = F.nll_loss(log_preds, target, reduction=self.reduction) return lin_comb(loss/(c-1), nll, self.ε) . We can just drop this in as a loss function, replacing the usual cross-entropy. . Additional reading: . Label Smoothing [paperswithcode/methods] | When Does Label Smoothing Help? [2019] | . Training in Mixed Precision . (Jump to lesson 12 video); Notebook: 10c_fp16.ipynb . If you are using a modern accelerator you can train with half precision floating point numbers. These are only 16bit floating point numbers (FP16), instead of the usual single precision 32bit floats (FP32). In theory this should speed things up by 10x, in practice however you get 2-3x speed-ups in deep learning. . Using FP16 cuts memory usage in half so you can double the size of your model and double your batch size. Specialized hardware units in modern accelerators, such as tensor cores, can also execute operations on FP16 faster. On Volta generation NVIDIA cards these tensor cores theoretically give an 8x speed-up (sadly, just in theory). . So training at half precision is better for your memory usage, way faster if you have a Volta GPU (still a tiny bit faster if you don’t since the computations are easiest). How do we use it? In PyTorch you simply have to put .half() on all your tensors. Problem is that you usually don’t see the same accuracy in the end, because half-precision is not very precise, funnily enough. . Aside: Some Floating Point Revision . Floating point numbers may seem arcane or like a bit of a dark art, but they are really quite elegant and understandable. One has to first understand that they are basically like scientific notation, except in base 2 instead of base 10: . [ begin{align} x &amp;= 0.1101101 times2^4 x &amp;= (-1)^s times M times 2^E end{align}] . In the IEEE floating point standard floats are represented using the above formula, where: . The sign $s$ determines if the number of negative ($s=0$) or positive ($s=1$) | The significant (AKA mantissa) $M$ is a fractional binary number that ranges between $[1, 2- epsilon]$ (normalized case) or between $[0, 1- epsilon]$ (denormalized case). | The exponent $E$ weights the value by a (possibly negative) power of 2. | . Each of these 3 sections occupy some number of bits. Here are the layouts for 32 bit floats: . (Source) . Here are some links to further introductory material: . A great explanation of how floats work: YouTube. | This video works through adding two floats at the bit level: YouTube | . Problems with Half-Precision . At high precision (FP32, FP64) you have enough breathing room that most of the time you don’t need to worry about the cases where the approximation falls apart and everything goes to hell. At FP16 you have to constantly think about the edge cases. . Let’s look at what FP16 looks like on a bit level: . . (Source) . The exponent has 5 bits, giving it a range [-14, 15]. | Fraction has 10 bits. | FP16 Range: 2^-14 to 2^15 roughly | FP32 Range: 2^-126 to 2^127 | The ‘spaces’ between numbers is increased in FP16. There is a finite number of floats between 1 and 2, and 1 + 0.0001 = 1 in FP16. This will cause problems during training. | When update/param &lt; 2^-11, updates will have no effect. | . You can’t just use half-precision everywhere, because you will almost always get hit by one of the problems above. Instead we do what’s call mixed precision training. This is where you drop down to FP16 in some parts of the training and revert to FP32 to preserve precision in others. . We do the forward pass and the backwards pass in FP16, and pretty much everywhere else we use FP32. For example, when we apply the gradients in the weight update we use full precision. Accumulate in FP32 and store in FP16. . There are still some problems remaining if we do this: . Weight update is imprecise. 1+0.0001 = 1 =&gt; vanishing gradients. | Gradients can underflow. Numbers get too low, get replaced by 0 =&gt; vanishing gradients. | Activations, loss, or reductions can overflow =&gt; Makes NaNs, training diverges. | The following subsections show how these are addressed. . Master Copy of Weights . To solve the first problem listed above - weight update is imprecise - we can store a ‘master’ copy of the weights in FP32. It is this that gets passed to the optimizer: . opt = torch.optim.SGD(master_params, lr=1e-3) . After the optimizer step you then copy the master weights back into the model weights in FP16: master.grad.data.copy_(model.grad.data) . Then, our training loop will look like: . Compute the output with the FP16 model, then the loss | Back-propagate the gradients in half-precision. | Copy the gradients in FP32 precision | Do the update on the master model (in FP32 precision) | Copy the master model in the FP16 model. | Loss Scaling . Next we need to tackle the second problem - gradients can underflow when doing backprop in FP16. To avoid the gradients getting zeroed by the FP16 precision, we multiply the loss by a scale factor. Typically this factor is something like 512 or 128. . We want to do this because the activation gradient values are typically very small and so fall outside of FP16’s representable range. Here is a histogram of the magnitude of activation gradients: . . What we want to do is push that distribution to the right and into the representable range of FP16. We could do this by multiplying the loss by 512 or 1024. . We don’t want these 512-scaled gradients to be in the weight update, so after converting them to FP32, we need to ‘descale’ by dividing by the scale factor. . The training loop changes to: . Compute the output with the FP16 model, then the loss. | Multiply the loss by scale then back-propagate the gradients in half-precision. | Copy the gradients in FP32 precision then divide them by scale. | Do the update on the master model (in FP32 precision). | Copy the master model in the FP16 model. | Accumulate to FP32 . The last problem - Activations, loss, or reductions can overflow - needs to be dealt with in a couple of places. . First, the loss can overflow, so let’s do the reduction calculation that gives the loss in FP32: . y_pred = model(x) # y_pred: fp16 loss = F.mse_loss(y_pred.float(), y.float()) # loss is now FP32 scaled_loss = scale_factor * loss . Another overflow risk is occurs with Batchnorm, which also should do its reduction in FP32. You can recursively go through your model and change all the Batchnorm layers back to FP32 with this function: . bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d) def bn_to_float(model): if isinstance(model, bn_types): model.float() for child in model.children(): bn_to_float(child) return model . You can then convert your Pytorch model to half precision with this function: . def model_to_half(model): model = model.half() return bn_to_float(model) . Dynamic Loss Scaling . The problem with loss scaling is that it has a magic number, the scale_factor, that you have to tune. As the model trains, different values may be necessary. Dynamic loss scaling is a technique that adaptively sets the scale_factor to the right value at runtime. This value will be perfectly fitted to our model and can continue to be dynamically adjusted as the training goes, if it’s still too high, by just halving it each time we overflow. After a while though, training will converge and gradients will start to get smaller, so we also need a mechanism to get this dynamic loss scale larger if it’s safe to do so. . Algorithm: . First initialize scale_factor with a really high value, e.g. 512. | Do a forward and backward pass. | Check if any of the gradients overflowed. | If any gradients overflowed, half the scale_factor, and zero the gradients (thus skipping the optimization step). | If the loop goes 500 steps without an overflow, double the scale_factor. | How do we test for overflow? A useful property of NaNs is that they propagate - add anything to a NaN and the result if NaN. So if we sum a tensor that contains a Nan the result will be NaN. To check if it is NaN we can use the counter-intuitive property that NaN!=NaN and simply check if the result of the sum equals itself. Here is the code: . def test_overflow(x): s = float(x.float().sum()) return (s == float(&#39;inf&#39;) or s == float(&#39;-inf&#39;) or s != s) . Summary . Here is a diagram of the mixed precision training loop containing all the above described steps: . . (Source: NVIDIA - Mixed-Precision Training Techniques Using Tensor Cores for Deep Learning) . In conclusion, here are the 3 problems caused by converting the model with FP16 and how they are mitigated: . Weight update is imprecise =&gt; “Master” weights in FP32 | Gradients can underflow =&gt; Loss (gradient) scaling | Activations or loss can overflow =&gt; Accumulate in FP32 | Sometimes training with half-precision gives better results. More randomness, some regularization. Often the results are similar to what you get with FP32, but just faster. . Implementing Mixed Precision with APEX . APEX is a utility library authored by NVIDIA for doing mixed precision and distributed training in Pytorch. . APEX can convert a model to FP16, keeping the batchnorm’s at FP32, with the function: . import apex.fp16_utils as fp16 model = fp16.convert_network(model, torch.float16) . (apex.fp16_utils docs) . From the model parameters (mostly in FP16), APEX can create a master copy in FP32 that we will use for the optimizer step: . model_p, master_p = fp16.prep_param_lists(model) . After the backward pass, all gradients must be copied from the model to the master params before the optimizer step can be done in FP32: . fp16.model_grads_to_master_grads(model_p, master_p) . After the optimizer step we need to copy back the master parameters to the model parameters for the next update: . fp16.master_params_to_model_params(model_params, master_params) . If you want to use parameter groups then we need to do a bit more work than this. Parameter groups allow you to do things like: . Transfer learning and freeze some layers | Apply discriminative learning rates | Don’t apply weight decay to some layers (like BatchNorm) or the bias terms | . Parameter groups are the business of the optimizer not the model, so we need to define a new prep_param_lists that takes the optimizer and returns the model and master params grouped in a nested list. Then you need too define wrappers for model_grads_to_master_grads and master_params_to_model_params that work on these nested lists. It’s straight-forward, but I won’t reproduce it here. It is shown in the notebook: 10c_fp16.ipynb . Callback Implementation . Mixed precision training as a callback with dynamic loss scaling: . class MixedPrecision(Callback): _order = 99 def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2., scale_wait=500): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; self.flat_master,self.dynamic,self.max_loss_scale = flat_master,dynamic,max_loss_scale self.div_factor,self.scale_wait = div_factor,scale_wait self.loss_scale = max_loss_scale if dynamic else loss_scale def begin_fit(self): self.run.model = fp16.convert_network(self.model, dtype=torch.float16) self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master) #Changes the optimizer so that the optimization step is done in FP32. self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner. if self.dynamic: self.count = 0 def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision def after_pred(self): self.run.pred = self.run.pred.float() #Compute the loss in FP32 def after_loss(self): if self.in_train: self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow def after_backward(self): #First, check for an overflow if self.dynamic and grad_overflow(self.model_pgs): #Divide the loss scale by div_factor, zero the grad (after_step will be skipped) self.loss_scale /= self.div_factor self.model.zero_grad() return True #skip step and zero_grad #Copy the gradients to master and unscale to_master_grads(self.model_pgs, self.master_pgs, self.flat_master) for master_params in self.master_pgs: for param in master_params: if param.grad is not None: param.grad.div_(self.loss_scale) #Check if it&#39;s been long enough without overflow if self.dynamic: self.count += 1 if self.count == self.scale_wait: self.count = 0 self.loss_scale *= self.div_factor def after_step(self): #Zero the gradients of the model since the optimizer is disconnected. self.model.zero_grad() #Update the params from master to model. to_model_params(self.model_pgs, self.master_pgs, self.flat_master) . XResNet . (Jump to Lesson 12 video); Notebook: 11_train_imagenette.ipynb . So far all of the image models we’ve used have been boring convolution models. What we really want to be using is a ResNet model. We will implement XResNet, which is the the mutant/extended version of ResNet. This is a tweaked ResNet taken from the Bag of tricks paper. . Let’s go through the XResNet modifications… . ResNet Stem Trick . ResNetC - don’t do a big 7x7 convolution at the start, because it’s inefficient and is just a single linear model. Instead do three 3x3 convs in a row. The receptive field is still going to be about 7x7, but it has a much richer number of things it can learn because it has 3 layers instead of 1. We call these first layers the stem. . The Conv layer takes in a number of input channels c_in and outputs a number of output channels c_out. . | First layer by default has c_in=3 because normally we have RGB images. | We set the number of outputs to c_out=(c_in+1)*8. This gives the second layer an input of 32 channels, which is what the bag of tricks paper recommends. | The factor of 8 also helps use the GPU architecture more efficiently. This grows / shrinks by itself with the number of input channels, so if you have more inputs then it will have more activations. | . The first few layers are called the stem and it looks like: . nfs = [c_in, (c_out+1)*8, 64, 64] # c_in/c_outs for the 3 conv layers stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1) for i in range(3)] . Where conv_layer is: . act_fn = nn.ReLU(inplace=True) def conv(ni, nf, ks=3, stride=1, bias=False): return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias) def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True): bn = nn.BatchNorm2d(nf) nn.init.constant_(bn.weight, 0. if zero_bn else 1.) # init batchnorm trick layers = [conv(ni, nf, ks, stride=stride), bn] if act: layers.append(act_fn) return nn.Sequential(*layers) . conv_layer is a nn.Sequential object of: . a convolution | followed by a BatchNorm | and optionally an activation (default ReLU) | . Zero BatchNorm Trick . After the stem the remainder of the ResNet’s body is an arbitrary number of ResBlocks. In the ResBlock there is one extra trick with the BatchNorm initialization. We sometimes initialize the BatchNorm weights to be 0 and other times we initialize it to 1. . To get why this would be useful, recall the diagram of the standard ResBlock: . . Each ‘weight layer’ in the above is a Conv/BatchNorm. If the input to a ResBlock is x then its output is x+block(x). If we initialize the final BatchNorm layer in the block to 0, then this is the same as multiplying the input by 0, so block(x)=0. Therefore at the start of training all ResBlocks just return their inputs, and this mimics a network that has fewer layers and is easier to train at the initial stage. . ResBlock . After the stem the remainder of the ResNet’s body is an arbitrary number of ResBlocks. The ResBlock code: . def noop(x): return x # identity operation class ResBlock(nn.Module): def __init__(self, expansion, ni, nh, stride=1): # expansion = 1 or 4 super().__init__() nf,ni = nh*expansion,ni*expansion layers = [conv_layer(ni, nh, 3, stride=stride), conv_layer(nh, nf, 3, zero_bn=True, act=False) ] if expansion == 1 else [ conv_layer(ni, nh, 1), conv_layer(nh, nh, 3, stride=stride), conv_layer(nh, nf, 1, zero_bn=True, act=False) ] self.convs = nn.Sequential(*layers) self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False) self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x))) . There several different types of ResBlocks in a ResNet and these are all contained in the comlete ResBlock code above, conditional on parameters expansion and stride. . There is the standard ResBlock that has expansion=1 and stride=1 that are stacked together in ResNet18/30, like those in this diagram: . . Besides the standard there are two other ResBlocks - the Expansion (AKA Bottleneck) ResBlock, and the Downsampling ResBlock. Let’s go through them and how they are tweaked by the Bag of Tricks paper. . Expansion/BottleNeck ResBlock . For ResNet18/34 the ResBlock looks like the diagram on the left below - a tensor comes in with shape [*, *, 64] and undergoes two 3x3 conv_layers. However, for the deeper ResNets (e.g. 50+) doing all these 3x3 conv_layers is expensive and costs memory. Instead we use a BottleNeck that has a 1x1 convolution to squish number of channels down by 4, then we do a single 3x3 convolution, followed by another 1x1 to project it back up to the original shape. Since we are squishing the number of channels down by a factor of 4 in the 3x3 conv_layer, we expand the normal number of channels in the model by a factor of 4 to get the equivalent size of the convolution as the basic block. See the diagram on the right below: . . (Diagram taken from the original ResNet paper) . In the ResBlock code, this bottleneck layer is implemented through the expansion parameter. expansion is either 1 or 4. We multiple the number of input and output channels by this factor: nf,ni = nh*expansion,ni*expansion. This factor is 4 for ResNet50+. . Downsampling ResBlock . At the start of a new group of ResBlocks we typically half the spatial dimensions with a stride 2 convolution and also double the number of channels. The dimensions have now changed so what happens to the identity connection? In the original paper they use a projection matrix to reduce the dimensions, and other implementions I’ve seen use a 1x1 conv_layer with stride 2. . The way they do it in the bag of tricks paper is to do an AveragePooling layer with stride 2 to half the grid size, followed by a 1x1 conv_layer (stride 1) to increase the number of channels. Here is a diagram of the downsampling ResBlock: . . A further tweak, which is shown above, is putting the stride 2 in the 3x3 conv_layer. Prior to this, people the stride 2 in the first 1x1 conv_layer, which is a terrible thing to do because you are just throwing away 3 quarters of the data. . Putting it Together . (Jump to Lesson 12 video) . Here is the code for creating any ResNet model: . class XResNet(nn.Sequential): @classmethod def create(cls, expansion, layers, c_in=3, c_out=1000): nfs = [c_in, (c_in+1)*8, 64, 64] stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1) for i in range(3)] nfs = [64//expansion,64,128,256,512] res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1], n_blocks=l, stride=1 if i==0 else 2) for i,l in enumerate(layers)] res = cls( *stem, nn.MaxPool2d(kernel_size=3, stride=2, padding=1), *res_layers, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nfs[-1]*expansion, c_out), ) init_cnn(res) return res @staticmethod def _make_layer(expansion, ni, nf, n_blocks, stride): return nn.Sequential( *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1) for i in range(n_blocks)]) . Combined with the ResBlock code that is all that is required for creating any ResNet model. :) . Now we can create all of our ResNets by listing how many blocks we have in each layer and the expansion factor (4 for 50+): . def xresnet18 (**kwargs): return XResNet.create(1, [2, 2, 2, 2], **kwargs) def xresnet34 (**kwargs): return XResNet.create(1, [3, 4, 6, 3], **kwargs) def xresnet50 (**kwargs): return XResNet.create(4, [3, 4, 6, 3], **kwargs) def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs) def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs) . Image Classification: Transfer Learning / Fine Tuning . (Jump_to lesson 12 video), (Notebook: 11a_transfer_learning.ipynb) . Recall the familiar ‘one-two’ training combo from part 1 of fastai for getting good results on Image classification tasks: . Get pretrained ResNet weights | Create a new ‘head’ section of the model for your new task. | Freeze all the layers except the head. | Run a few cycles of training for the head. | Unfreeze all the layers and run a few more training cycles. | Let’s implement the code required to make that possible. . Custom Head . In the notebook they want to use a model pretrained on ImageWoof to fine tune for the Pets dataset. We can save our ImageWoof model to disk as a dictionary of layer_name: tensor. PyTorch model has this readily available with st = learn.model.state_dict(). . Let’s go through the process of loading back in the pretrained ImageWoof model. First we need to create a Learner: . learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) . ImageWoof has 10 activations, so we need to match this so the weights match up: c_out=10. . We can then load the ImageWoof state dictionary and load the weights into our Learner: . st = torch.load(&#39;imagewoof&#39;) m = learn.model m.load_state_dict(st) . This is now just the recovered ImageWoof model. We want to change it so it can be used on the new dataset, so we take off the last linear layer for 10 classes and replace it with one for the 37 classes of the Pets dataset. We can find the point we want to cut the model by searching for the index cut that points to the nn.AdaptiveAvfPool2d layer, which is the penultimate layer before the head. The cut model is then: m_cut = m[:cut]. . The number of outputs of our new head is 37, what about the inputs? We can determine that easily by just running a batch through the cut model: ni = m_cut(xb).shape[1]. . We can now create our new head for the Pets model: . m_new = nn.Sequential( m_cut, AdaptiveConcatPool2d(), Flatten(), nn.Linear(ni*2, 37)) . Where we also use AdaptiveConcatPool2d which is simply average pooling and max pooling catted together into a 2*ni sized vector. This double pooling is a fastai trick that gives a little boost over just doing one of the other. . class AdaptiveConcatPool2d(nn.Module): def __init__(self, sz=1): super().__init__() self.output_size = sz self.ap = nn.AdaptiveAvgPool2d(sz) self.mp = nn.AdaptiveMaxPool2d(sz) def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) . With this simple transfer learning we can get 71% on Pets after 4 epochs. Without the transfer learning we only get 37%. . All the steps together: put this whole process in a function: . def adapt_model(learn, data): cut = next(i for i,o in enumerate(learn.model.children()) if isinstance(o,nn.AdaptiveAvgPool2d)) m_cut = learn.model[:cut] xb,yb = get_batch(data.valid_dl, learn) pred = m_cut(xb) ni = pred.shape[1] m_new = nn.Sequential( m_cut, AdaptiveConcatPool2d(), Flatten(), nn.Linear(ni*2, data.c_out)) learn.model = m_new . Then the weight loading and model adaption is simply becomes: . learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) learn.model.load_state_dict(torch.load(mdl_path/&#39;iw5&#39;)) adapt_model(learn, data) . Freezing Layers . You can freeze layers by turning their gradients off: . for p in learn.model[0].parameters(): p.requires_grad_(False) . Let’s do one-two training combo. . Freezing the body and training the head 3 epochs gets 54%. | Unfreezing and training the rest of the model for another 5 epochs gets 56%(!) | It’s better than not fine tuning, but interestingly when we just fine-tuned without the freezing we got a way better result of 71%. Why doesn’t it work? . Every time something weird happens in your neural net, it’s almost certainly due to batchnorm. Because batchnorm makes everything weird. :D . The batchnorm layers in the pretrained model have learned means and stds for a different dataset (ImageWoof). When we trained froze the body and trained the head, the head was learning with a different set of batch norm statistics. When we unfreeze the body the batchnorm statistics can now change, which effectively causes the ground to shift from underneath the later layers that we just changed. . Fix: Don’t freeze the weights in the batchnorm layers when doing partial layer training. . Here’s the function that does the freezing and unfreezing of layers, which skips batchnorm layers: . def set_grad(m, b): if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return if hasattr(m, &#39;weight&#39;): for p in m.parameters(): p.requires_grad_(b) . We can use PyTorch apply method to apply this function to our model: . learn.model.apply(partial(set_grad, b=False)); . ULMFiT From Scratch . (Jump to lesson 12 video) . ULMFiT is transfer learning applied to AWD-LSTM for NLP . There has been a lot of ground-breaking innovation in the realm of transfer learning applied to NLP recently - e.g. GPT2, BERT. These are all based on Transformers, which are currently very hot, so one could think that LSTMs aren’t used or interesting anymore. However when you look at recent competitive machine learning results (NB recorded 2019), you see ULMFiT beating BERT - from poleval2019: . . Jeremy says…: . It’s definitely not true that RNNs are in the past. Transformers and CNNs for text have a lot of problems. They don’t have state. So if you are doing speech recognition, for every sample you look at you have to do an entire analysis of all the sample around it again and again and again. So it’s rediculously wasteful. Wheras RNNs have state. But they are fiddly and hard to deal with when you want to do research and change things. RNNs, and in particular AWD-LSTM, have had a lot of research done on how to regularize them carefully. Stephen Merity did a huge amount of work on all the different way they can be regularized. There is nothing like that outside the RNN world. At the moment my goto choice is still ULMFiT for most real world tasks. I’m not seeing transformers win competitions yet. . There are lots of things that are sequences that aren’t text - genomics, chemical bonding analysis, and drug discovery. People are finding exciting applications of ULMFiT outside of NLP. . Here is a review of the ULMFiT pipeline that we saw in part 1 of fastai v3: . . We are going to code this up from scratch… . Preprocess Text . (Jump to lesson 12 video) . Notebook: 12_text.ipynb . We will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones. It contains a train folder, a test folder, and an unsup (unsupervised) folder. . First thing we need to do is create a datablocks ItemList subclass for text: . def read_file(fn): with open(fn, &#39;r&#39;, encoding = &#39;utf8&#39;) as f: return f.read() class TextList(ItemList): @classmethod def from_files(cls, path, extensions=&#39;.txt&#39;, recurse=True, include=None, **kwargs): return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs) def get(self, i): if isinstance(i, Path): return read_file(i) return i . This was easy because we reuse much code we wrote previously. We already have the get_files function, which now searches for .txt files instead of images. Then we override get to now call a function, read_file that reads a text file. Now we can load the dataset: . il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) . If we look at one of the items it will be just the raw text of a IMDB movie review. . We can just throw this in to a model - it needs to be numbers. So we need to Tokenize and Numericalize it. . Tokenizing . (Jump_to lesson 12 video) . We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don&#39;t for instance is split between do and n&#39;t. We will use a processor for this, in conjunction with the spacy library. . Before tokenizing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens. . default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br] default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ] . These are: . fixup_text: Fixes various messy things seen in documents. For example, HTML artifacts. | replace_rep: Replace repetitions at the character level: !!!!! -&gt; TK_REP 5 ! | replace_wrep: Replace word repetitions: word word word -&gt; TK_WREP 3 word | spec_add_spaces: Add spaces around / and # | rm_userless_spaces: If we find more than two spaces in a row, replace them with one space | sub_br: Replaces the &lt;br /&gt; by n | . Why do replace_rep and replace_wrep? Let’s image a tweet that said: “THIS WAS AMAZING!!!!!!!!!!!!!!!!!!!!!”. We could treat the exclamation marks as one token, so we would then have a single vocab item that is specifically 21 exclamation marks. You probably wouldn’t see that again so it wouldn’t even end up in your vocab, and if it did it would be so rare that you wouldn’t be able to learn anything interesting about it. It would also absurdly be different from the case where there is 20 or 22 exclamation marks. But some big number of exclamation marks does have a meaning and we know that it is different from the case where there is just a single one. If we instead replace it with ` xxrep 21 ! `, then this is just three tokens where the model can learn that lots of repeating exclamation marks is a general concept that has certain semantics to it. . Another alternative would be to turn our sequence of exclamation marks into 21 tokens in a row, but now we are asking our LSTM to hang onto that state for 21 timesteps, which is a lot more work for it to do and it won’t do as good a job. . What we are trying to do in NLP is to make it so that the things in our vocabulary are as meaningful as possible. . . After tokenizing with spacey we apply a couple more rules: . replace_all_caps: Replace tokens in ALL CAPS by their lower version and insert TK_UP before. | deal_caps: Replace all Captitalized tokens by their lower version and add TK_MAJ before. | add_eos_bos: Add before-stream (BOS) and end-of-stream (EOS) tokens on either side of a list of tokens at the start/end of a document. These tokens turn out to be very important. When the model encounters a EOS token it knows it is at the end of a document and that the next document is something new. So it will have to learn to reset its state somehow. | . For example: . &gt; replace_all_caps([&#39;I&#39;, &#39;AM&#39;, &#39;SHOUTING&#39;]) [&#39;I&#39;, &#39;xxup&#39;, &#39;am&#39;, &#39;xxup&#39;, &#39;shouting&#39;] &gt; deal_caps([&#39;My&#39;, &#39;name&#39;, &#39;is&#39;, &#39;Jeremy&#39;]) [&#39;xxmaj&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;xxmaj&#39;, &#39;jeremy&#39;] . Tokenizing with spacey is quite slow because spacey does things very carefully. spacey has a sophisticated parser based tokenizer and it using it will improve your accuracy a lot, so it’s worth using. Luckily tokenizing is embarrassingly parallel so we can speed things up using multi-processing. . class TokenizeProcessor(Processor): def __init__(self, lang=&quot;en&quot;, chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): self.chunksize,self.max_workers = chunksize,max_workers self.tokenizer = spacy.blank(lang).tokenizer for w in default_spec_tok: self.tokenizer.add_special_case(w, [{ORTH: w}]) self.pre_rules = default_pre_rules if pre_rules is None else pre_rules self.post_rules = default_post_rules if post_rules is None else post_rules def proc_chunk(self, args): i,chunk = args chunk = [compose(t, self.pre_rules) for t in chunk] docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)] docs = [compose(t, self.post_rules) for t in docs] return docs def __call__(self, items): toks = [] if isinstance(items[0], Path): items = [read_file(i) for i in items] chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))] toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers) return sum(toks, []) def proc1(self, item): return self.proc_chunk([item])[0] def deprocess(self, toks): return [self.deproc1(tok) for tok in toks] def deproc1(self, tok): return &quot; &quot;.join(tok) . Here is what an example raw input looks like: . &#39;Comedian Adam Sandler &#39;s last theatrical release &quot;I Now Pronounce You Chuck and Larry&quot; served as a loud and proud plea for tolerance of the gay community. The former &quot;Saturday Night Live&quot; funnyman &#39;s new movie &quot;You Don &#39;t Mess with the Zohan&quot; (*** out o&#39; . And here’s what that looks like after tokenization: . &#39;xxbos • xxmaj • comedian • xxmaj • adam • xxmaj • sandler • &#39;s • last • theatrical • release • &quot; • i • xxmaj • now • xxmaj • pronounce • xxmaj • you • xxmaj • chuck • and • xxmaj • larry • &quot; • served • as • a • loud • and • proud • plea • for • tolerance • of • the • gay • community • . • xxmaj • the • former • &quot; • xxmaj • saturday • xxmaj • night • xxmaj • live • &quot; • funnyman • &#39;s • new • movie •&#39; . Numericalize Tokens . Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the CategoryProcessor). . from collections import Counter class NumericalizeProcessor(Processor): def __init__(self, vocab=None, max_vocab=60000, min_freq=2): self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq def __call__(self, items): #The vocab is defined on the first use. if self.vocab is None: freq = Counter(p for o in items for p in o) self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c &gt;= self.min_freq] for o in reversed(default_spec_tok): if o in self.vocab: self.vocab.remove(o) self.vocab.insert(0, o) if getattr(self, &#39;otoi&#39;, None) is None: self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) return [self.proc1(o) for o in items] def proc1(self, item): return [self.otoi[o] for o in item] def deprocess(self, idxs): assert self.vocab is not None return [self.deproc1(idx) for idx in idxs] def deproc1(self, idx): return [self.vocab[i] for i in idx] . Tokenizing and Numericalizing text takes a while and so it’s best to do it once and then serialize it. . Batching Text for RNN Training . (Jump_to lesson 12 video) . Batching up language model data requires a bit more care than it does with say image data. Let’s take work through batching with an example piece of text: . stream = &quot;&quot;&quot; In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we&#39;ll have another example of the Processor used in the data block API. Then we will study how we build a language model and train it. n &quot;&quot;&quot; . Let’s use a batch-size of 6. This sequences happens to divide into 6 pieces of length 15, so the length of our 6 sequences is 15. . . Every forward pass we will give our model chunks of text 5 tokens long. This is called the backpropagation through time (BPTT). This is a fancy sounding name, but it just means that every batch we view the RNN as being unfolded 5 times through time as shown below (source): . . So the model would make 5 predictions for each mini-batch, accumulate the errors across the 5 time steps, then update the weights. (In other places this is called Truncated BPTT) . With bptt=5 we go over all the sequences in 3 mini-batches: . . To reconstruct the order of the original text read the top row of all the batches, then the second, and so on. . The batch-size is like the number of texts that you train the model on in parallel. Along the rows of the batches, the text is in sequential order. This is essential because the model is creating an internal state that depends on what it is seeing. It needs to be in order. . What’s the difference between sequence length, batch-size, and bptt?? . Here is a brilliant graphic from Stefano Giomo that illustrates what these all mean. | . Lets create a dataloader for language models. At the beginning of each epoch, it’ll shuffle the articles (if shuffle=True) and create a big stream by concatenating all of them. We divide this big stream in bs smaller streams. That we will read in chunks of bptt length. . What about the source x and target y of the language model task? For training our language model using self-supervised learning we want to take in a word and then predict the next word in the sequence. Therefore, the target y will be exactly the same as x, but shifted over by one word. . Let’s create this dataloader: . class LM_Dataset(): def __init__(self, data, bs=64, bptt=70, shuffle=False): self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle total_len = sum([len(t) for t in data.x]) self.n_batch = total_len // bs self.batchify() def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs def __getitem__(self, idx): source = self.batched_data[idx % self.bs] seq_idx = (idx // self.bs) * self.bptt # x, y (x shifted by 1 word) return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1] def batchify(self): texts = self.data.x if self.shuffle: texts = texts[torch.randperm(len(texts))] stream = torch.cat([tensor(t) for t in texts]) self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch) . If we look at this, if x is: . &quot;xxbos well worth watching , &quot; . Then y would be: . &quot;well worth watching , especially&quot; . Question: What are the trade-offs to consider between batch-size and BPTT? For example, BTPP=10 vs BS=100, or BTPP=100 vs BS=10? Both would be passing 1000 tokens at a time to the model. What should you consider when tuning the ratio? . I don’t know the answer. This would make a super great experiment. . The batch-size is the the thing that lets it parallelize. So if your batch-size is small it’s just going to be super slow. On the other hand, a large batch size with a short bptt you may end up with less state that’s being backpropagated. . What the ratio should be - I’m not sure. . Batching Text for Training Classifiers . (Jump to lesson 12 video) . When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from. We’ve already done this for image models, and so we can reuse the code we already wrote for that: . proc_cat = CategoryProcessor() il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;]) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;test&#39;)) ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat) . The target/dependent variable could be the sentiment of the document: e.g. pos or neg. . Are we finished? Not quite. . When we worked with images, by the time we got to modelling they were all the same size (we resized them to a square). For texts, you can’t ignore that some texts are bigger than others. In order to collate a bunch of texts into a batch we will need to apply padding using some padding token to our documents so that documents collated into the same batches have the same length. . However, if we have a mini-batch with a 1000 word document, a 2000 word document, and then a 20 word document. The 20 word document is going to end up with 1980 padding tokens tacked onto the end. As we go through the RNN we are going to be calculating pointlessly on these padding tokens, which is a waste. So the trick is to sort the data first by length. That way your first mini-batch will contain your really long documents and your last mini-batch will contain your really short documents. This will mean that there won’t be much padding and wasted computation in any of the mini-batches. . In fastai this is done by creating a new type of Sampler. Naively, we can create a sampler that just sorts all our documents by length, but this would through away any randomness in constructing our batches - no shuffle. We can instead organize all the documents into buckets such that documents of similar size go in the same bucket. We can then do shuffling inside of those buckets to construct our mini-batches. This sampler is called SortishSampler in the lesson notebook. . Now we have shuffled the documents using SortishSampler we need to collate them into batches - i.e. stick them together into a batch tensor with a fixed known size. We add the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use PyTorch convenience functions that will let us ignore that padding. . def pad_collate(samples, pad_idx=1, pad_first=False): max_len = max([len(s[0]) for s in samples]) res = torch.zeros(len(samples), max_len).long() + pad_idx for i,s in enumerate(samples): if pad_first: res[i, -len(s[0]):] = LongTensor(s[0]) else: res[i, :len(s[0]) ] = LongTensor(s[0]) return res, tensor([s[1] for s in samples]) . So to create the data loader: . bs = 64 train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs) train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate) . An example mini-batch looks like: . . LSTM From Scratch . (Jump_to lesson 12 video) . Now we will create an RNN. An RNN is like a network with many many layers. For, say, a document with 2000 words, it would be a network with 2000 layers. Of course, it is never explicitly code it that way and instead just use a for-loop. . . Between every pair of hidden layers we use the same weight matrix. Problem is, trying to handle 2000 network layers we get vanishing gradients and exploding gradients and it’s really hard to get it to work. We can design more complex networks where the output of one RNN is fed into another RNN (stacked RNNs). . To get this thing to work, we need our hidden layers to do something more than just a matrix multiply. We instead use a LSTM Cell: . . Recall that: . Sigmoid function $ sigma$ - is a smooth function that goes from 0 to 1. | Tanh function $ tanh$ - is a smooth function that goes from -1 to 1. | . How to read this thing? . Starting from the bottom you have the input x and the hidden layer output from the previous layer h coming into the cell. . | x and h are fed into the orange sigmoid and tanh layers simultaneously. The same values go into those layers. . | Each of those layers is basically another little hidden layer. x and h are multiplied by matrices before going through the sigmoid or tanh activations. Each of the layers has its own matrices. . | . This diagram also includes equations that make it more clear: . . (picture from Understanding LSTMs by Chris Olah, definitely read this.) . Let’s follow the path through the cell: . The forget path goes through a sigmoid and then hits the cell value, $C_{t-1}$. This is just a rank 2 tensor (with mini-batch) and represents the state or memory component of the LSTM that is passed on and updated through the timesteps. | We multiply this by the output of the forget sigmoid. So this gate has the ability to zero-out bits of the Cell state. We can look at some of our words coming in and say - based on that we should zero-out some of the Cell state. | We then add the selectively forgotten Cell state to the second little neural net. Here we have the outputs of two layers - a sigmoid and a tanh - which are multiplied together and their product is then added to the selectively forgotten Cell state. | This part is where the LSTM chooses how to update the Cell state. This is carried through to the next time step as $C_t$. | $C_t$ is then also put through another tanh function and multiplied by our fourth and final mini neural net (another sigmoid) to create the new output state $h_t$, that is passed on to the next time step. This sigmoid decides which parts of the Cell state to output. | . It seems pretty weird, but as code it’s very simple to implement. In coding it, rather than have 4 different matrices for each of the internal mini neural nets, it’s more efficient to just stack them all into one 4x matrix and do one matmul. You can then split the output into equal sized chunks using the chunk function in PyTorch. . Here is the code for a LSTM cell: . class LSTMCell(nn.Module): def __init__(self, ni, nh): super().__init__() self.ih = nn.Linear(ni,4*nh) # input2hidden self.hh = nn.Linear(nh,4*nh) # hidden2hidden def forward(self, input, state): h,c = state #One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . Then an LSTM layer just applies the cell on all the time steps in order. . class LSTMLayer(nn.Module): def __init__(self, cell, *cell_args): super().__init__() self.cell = cell(*cell_args) def forward(self, input, state): inputs = input.unbind(1) outputs = [] for i in range(len(inputs)): out, state = self.cell(inputs[i], state) outputs += [out] return torch.stack(outputs, dim=1), state . In practice this is: . lstm = LSTMLayer(LSTMCell, 300, 300) x = torch.randn(64, 70, 300) h = (torch.zeros(64, 300),torch.zeros(64, 300)) . The hidden state and cell states are initialized with zeros at the start of training. . Aside: there are lots of other ways you can setup a layer that has the ability to selectively update and selectively forget things. For example, there is a popular alternative called a GRU, which has one less gate. The thing seems to be giving it some way to make the decision to forget things. Then it has the ability to not push state through all the thousands of time steps. . PyTorch’s LSTM Layer . We can now use the Pytorch’s own LSTM layer: . input_size, hidden_size, num_layers = 300, 300, 1 lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) bs, bptt = 64, 70 x = torch.randn(bs, bptt, input_size) h = (torch.zeros(num_layers, bs, hidden_size), torch.zeros(num_layers, bs, hidden_size)) output, (h1, c1) = lstm(x, h) . It’s worth going over the dimensions because they confused me a lot. . Firstly, remember that this PyTorch module is not an individual LSTM cell, rather it is potentially multiple LSTM layers over multple timesteps. With PyTorch’s LSTM you can specify the num_layers and we also run it with bptt timesteps. The following diagram from this SO post shows how you should picture this: . . The depth is the num_layers and time is bptt. . The dimensions of the inputs: . input: (batch, bptt, input_size) | h_0: (batch, num_layers, hidden_size) | c_0: (batch, num_layers, hidden_size) | . The dimensions of the outputs: . output: (batch, seq_len, hidden_size) | h_n: (batch, num_layers, hidden_size) | c_n: (batch, num_layers, hidden_size) | . (NB this is while using batch_first=True) . AWD-LSTM . We want to use the AWD-LSTM from Stephen Merity et al. [2017]. In this paper, the authors thought about all the ways we can regularize and optimize the LSTM model for NLP. In their paper they test all the different kinds of way they can apply dropout to the LSTM. . AWD-LSTM stands for: (Average Stochastic Gradient Descent)(Weight-Dropped)-LSTM… | . Dropout consists of replacing some coefficients by 0 with probablility p. To ensure that the average of the weights remains constant, we apply a correction factor to the weights that aren’t nullified with value 1/(1-p). . def dropout_mask(x, sz, p): return x.new(*sz).bernoulli_(1-p).div_(1-p) . This looks like: . . Once with have a dropout mask mask, applying the dropout to x is simply done by x = x * mask. . With RNN NLP tasks, a tensor x will have three dimensions: . The batch_size: the number of texts we are training on in parallel. | The bptt: the number of tokens we are looking at at a time. | The emb_size: the size of the vector representation of a token. | . With (2, 3, 5) this could look like: . &gt;&gt; x = torch.randn(2,3,5) tensor([[[ 0.1620, 0.0125, -0.5448, -0.8244, 0.3781], [ 0.2661, -0.7103, -0.5006, 0.5024, 0.7515], [-0.6798, 0.1970, 0.0260, 0.5037, 0.2735]], [[-0.0661, 1.2567, -1.2873, -1.1245, 0.0959], [-0.5627, -0.0315, 0.9382, 0.8043, -1.2791], [ 0.2626, 1.8968, 0.5332, 0.6908, -0.3327]]]) . RNNDropout / Variational Dropout . For each document in the batch we want to have a unique dropout mask, but we also don’t want to randomly apply it on the vocab dimension, so that every token has a different dropout mask. We have a sequence length of say 5, then recall that the RNN will only do a single forward/backward/update pass with those 5. . Therefore the model needs to be the same for all those sequences and so we need to apply the same dropout mask on the entire sequence, otherwise it’s just broken. . The standard dropout can’t do this for us so we need to create our own layer called RNNDropout. We want to have different dropout masks for each member of the batch (document), but replicate it over their sequences. We can elegantly do this with a clever broadcasting trick, where you specify that the dropout mask has shape (x.size(0), 1, x.size(2)). That way when you multiply by the mask, it will be replicated down the sequence dimension. . class RNNDropout(nn.Module): def __init__(self, p=0.5): super().__init__() self.p=p def forward(self, x): if not self.training or self.p == 0.: return x m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p) return x * m . The output looks like: . &gt;&gt; x = torch.randn(5,3,5) &gt;&gt; dp = RNNDropout(0.2) &gt;&gt; dp(x) tensor([[[-1.6986, -0.2667, 0.5033, -0.0000, -0.7446], [-1.5847, -0.1816, 2.8067, -0.0000, 0.1114], [ 0.4601, 0.6553, 1.4082, 0.0000, -1.6506]], [[ 1.7197, -1.0774, 0.0000, 0.1192, -0.0000], [ 0.9075, -0.1597, 0.0000, -1.3051, 0.0000], [ 0.5267, 1.0956, 0.0000, -2.3911, -0.0000]], [[ 0.0000, -0.0000, 0.6468, 1.8105, 0.6677], [ 0.0000, 0.0000, -0.6478, 0.8841, -2.5549], [ 0.0000, 0.0000, 1.1027, 0.7640, -0.4295]], [[-2.8382, 0.0000, 0.6135, 0.1007, 0.0000], [-0.7681, 0.0000, -1.1549, -1.4484, -0.0000], [ 1.5333, 0.0000, 1.0678, -1.4169, -0.0000]], [[ 0.0000, -0.2658, -0.0000, 0.0000, 0.3604], [-0.0000, -1.2424, -0.0000, -0.0000, 3.5358], [ 0.0000, 0.7004, -0.0000, -0.0000, -0.5376]]]) . Note the positions of the 0s. . Weight Dropout / DropConnect . Weight dropout applies dropout not on the activations, but on the weights themselves. But otherwise the aim of regularization is the same as normal dropout. This is also called DropConnect. Here this is applied to the weights of the inner LSTM hidden to hidden matrix - $U^i, U^f, U^o, U^g$. . The DropConnect paper says: . DropConnect is the generalization of Dropout in which each connection, instead of each output unit as in Dropout, can be dropped with probability p. . Since there are many more weights than activations to disable, DropConnect creates many more ways of altering the model.(Also see this Stackoverflow question on the difference between the two.) . The downside is that it requires us to keep a copy of the weights we are using DropConnect on, so the memory requirement will increase for our model. For normal dropout, on the other hand, we just need to store a mask of the activations, of which there are fewer than the weights. . Here’s how to implement it in PyTorch: . import warnings WEIGHT_HH = &#39;weight_hh_l0&#39; class WeightDropout(nn.Module): def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]): super().__init__() self.module,self.weight_p,self.layer_names = module,weight_p,layer_names for layer in self.layer_names: #Makes a copy of the weights of the selected layers. w = getattr(self.module, layer) self.register_parameter(f&#39;{layer}_raw&#39;, nn.Parameter(w.data)) self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False) def _setweights(self): for layer in self.layer_names: raw_w = getattr(self, f&#39;{layer}_raw&#39;) self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training) def forward(self, *args): self._setweights() with warnings.catch_warnings(): #To avoid the warning that comes because the weights aren&#39;t flattened. warnings.simplefilter(&quot;ignore&quot;) return self.module.forward(*args) . It’s not implemented as another layer, like normal dropout is, rather more like a wrapper layer. It takes a module and a list of layer_names in its constructor; these are the layer_names that you want to apply DropConnect to. You can see in the constructor that it stashes the original weight matrix as &quot;{layer}_raw&quot;. Then in the forward method it calls _setweights that loads the stashed weights then applies dropout to the weights and saves the result as &quot;{layer}&quot;. . Embedding Dropout . The next type of dropout is Embedding Dropout. This apples dropout to full rows of the embedding matrix. . class EmbeddingDropout(nn.Module): &quot;Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.&quot; def __init__(self, emb, embed_p): super().__init__() self.emb,self.embed_p = emb,embed_p self.pad_idx = self.emb.padding_idx if self.pad_idx is None: self.pad_idx = -1 def forward(self, words, scale=None): if self.training and self.embed_p != 0: size = (self.emb.weight.size(0),1) mask = dropout_mask(self.emb.weight.data, size, self.embed_p) masked_embed = self.emb.weight * mask else: masked_embed = self.emb.weight if scale: masked_embed.mul_(scale) return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm, self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse) . In practice this looks like: . &gt;&gt; enc = nn.Embedding(100, 5, padding_idx=1) &gt;&gt; enc_dp = EmbeddingDropout(enc, 0.5) &gt;&gt; tst_input = torch.randint(0,100,(6,)) &gt;&gt; enc_dp(tst_input) tensor([[-0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.2812, 2.0295, 2.9520, -2.3344, 3.9739], [-0.0000, 0.0000, -0.0000, -0.0000, 0.0000], [-2.3218, -0.0442, -0.0116, 0.4450, -2.9943], [-2.3218, -0.0442, -0.0116, 0.4450, -2.9943], [-0.9694, 0.0261, -1.3517, 1.7962, 2.3478]], grad_fn=&lt;EmbeddingBackward&gt;) . This is dropping out entire words at a time. . The Main Model . With all that in place we can code up our LSTM model. . Here is a pseudo-code outline of the forward pass of the model: . def forward(input): x = input # Tokenized + numericalized data, (bs, bptt) x = Embedding(x) # (bs, bptt, emb_sz) x = EmbeddingDropout(x) x = RNNDropout(x) # Loop through LSTMs for l in lstm_layers: WeightDropout(l) x, new_h = lstm(x, h[l]) h[l] = new_h if l != last_layer: x = RNNDropout(x) return x # (bs, bptt, hidden_sz) . To see what’s happening at every step of AWD-LSTM visualized with excel, I recommend reading this excellent medium post - Understanding building blocks of ULMFiT [Kerem Turgutlu]. . Here is the PyTorch implementation: . class AWD_LSTM(nn.Module): &quot;AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.&quot; initrange=0.1 def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5): super().__init__() self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) self.emb_dp = EmbeddingDropout(self.emb, embed_p) self.rnns = [nn.LSTM(input_size=emb_sz if l == 0 else n_hid, hidden_size=(n_hid if l != n_layers - 1 else emb_sz), num_layers=1, batch_first=True) for l in range(n_layers)] self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns]) self.emb.weight.data.uniform_(-self.initrange, self.initrange) self.input_dp = RNNDropout(input_p) self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)]) def forward(self, input): bs,sl = input.size() if bs!=self.bs: self.bs=bs self.reset() # Embedding layer raw_output = self.input_dp(self.emb_dp(input)) # Loop through LSTM layers new_hidden,raw_outputs,outputs = [],[],[] for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)): raw_output, new_h = rnn(raw_output, self.hidden[l]) new_hidden.append(new_h) raw_outputs.append(raw_output) if l != self.n_layers - 1: raw_output = hid_dp(raw_output) outputs.append(raw_output) self.hidden = to_detach(new_hidden) return raw_outputs, outputs def _one_hidden(self, l): &quot;Return one hidden state.&quot; nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz return next(self.parameters()).new(1, self.bs, nh).zero_() def reset(self): &quot;Reset the hidden states. Init with zeros&quot; self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)] . The intermediate LSTM layer outputs are also stored, raw_outputs, these are needed later for regularization. The last LSTM layer has hidden_sz=emb_sz. This is because we are trying to make a word prediction so we want it to output a vector that fits in our word embedding space. . We then need another layer to decode the output of the last LSTM and tells us what the next word will be. LinearDecoder is simply a fully connected linear layer that transforms the output of the last LSTM layer to token predictions in our vocabulary - it’s a classifier for words. It uses the same embedding matrix as the encoding layer (tied weights/tied_encoder). . class LinearDecoder(nn.Module): def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True): super().__init__() self.output_dp = RNNDropout(output_p) self.decoder = nn.Linear(n_hid, n_out, bias=bias) if bias: self.decoder.bias.data.zero_() if tie_encoder: self.decoder.weight = tie_encoder.weight else: init.kaiming_uniform_(self.decoder.weight) def forward(self, input): raw_outputs, outputs = input output = self.output_dp(outputs[-1]).contiguous() decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2))) return decoded, raw_outputs, outputs . We also apply RNNDropout to the input to LinearDecoder. . We will create this linear layer with nn.Linear(emb_sz, vocab_size). Recall the last LSTM layer will output something with shape (bs, bptt, emb_sz), so the output of the LinearDecoder will be a tensor of shape (bs, bptt, vocab_size). We can then apply cross-entropy loss to this word prediction to determine how correct the model’s prediction was. . Now we can stack all of this together to make our language model: . class SequentialRNN(nn.Sequential): &quot;A sequential module that passes the reset call to its children.&quot; def reset(self): for c in self.children(): if hasattr(c, &#39;reset&#39;): c.reset() def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True): rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p) enc = rnn_enc.emb if tie_weights else None return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, # output=word emb_sz, # input=emb vector output_p, tie_encoder=enc, bias=bias)) . Gradient Clipping/Rescaling . With the model in place we can now focus on the training and regularization. . AWD-LSTM paper also recommends gradient clipping/rescaling. This is a super good idea for training because it lets you train at higher learning rates and avoid gradients blowing out. This can be especially bad in RNNs because we unroll the RNN and effectively replicated the weight matrix and repeatedly multiply it. This can cause things to grow or shrink exponentially. . The idea is simple - if the gradient gets too large, then we rescale it. We do this by rescaling the norm of the gradient tensor to a hyperparameter clip: ( mathbf{g} leftarrow c cdot mathbf{g}/ | mathbf{g} | ) PyTorch has a clip_grad_norm_ function that does this. Here it is wrapped in a callback: . class GradientClipping(Callback): def __init__(self, clip=None): self.clip = clip def after_backward(self): if self.clip: nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip) . The value used for clip here is 0.1. . An alternative form is often used where you simply clamp the gradients between some [-clip, clip], where clip may have some value like 5 or 100. This is provided by PyTorch’s clip_grad_value_. . Gradient clipping addresses only the numerical stability of training deep neural network models and does not offer any general improvement in performance. This is likely even more pertinent when using FP16. . More info: How to Avoid Exploding Gradients With Gradient Clipping [Machine Learning Mastery] . Training + More Regularization . At the loss calculation stage AWD-LSTM applies two more types of regularization. . The first is Activation Regularization (AR): this is an L2 penalty (like weight decay) except on final activations, instead of on weights. We add to the loss an L2 penalty (times hyperparameter $ alpha$) on the last activations of the AWD LSTM (with dropout applied). . The second is Temporal Activation Regularization (TAR): we add to the loss an L2 penalty (times hyperparameter $ beta$) on the difference between two consecutive (in terms of words) raw outputs. This checks how much does each activation change by from time step to time step, then takes the square of that. This regularizes the RNN so that it tries not to have things that massively change from time step to time step, because if it’s doing then then it’s probably not a good sign. . Code for the RNNTrainer: . class RNNTrainer(Callback): def __init__(self, α, β): self.α,self.β = α,β def after_pred(self): #Save the extra outputs for later and only returns the true output (decoded into words). self.raw_out,self.out = self.pred[1],self.pred[2] self.run.pred = self.pred[0] def after_loss(self): #AR and TAR if self.α != 0.: self.run.loss += self.α * self.out[-1].float().pow(2).mean() if self.β != 0.: h = self.raw_out[-1] if h.size(1)&gt;1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean() def begin_epoch(self): #Shuffle the texts at the beginning of the epoch if hasattr(self.dl.dataset, &quot;batchify&quot;): self.dl.dataset.batchify() . We set up our loss function normal cross_entropy and a accuracy metrics. We just need to make sure that then batch and sequence dimensions are all flattened into one dimension: . def cross_entropy_flat(input, target): bs,sl = target.size() return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl)) def accuracy_flat(input, target): bs,sl = target.size() return accuracy(input.view(bs * sl, -1), target.view(bs * sl)) . Now we are ready to go: . emb_sz, nh, nl = 300, 300, 2 model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, embed_p=0.1, hidden_p=0.2) cbs = [partial(AvgStatsCallback,accuracy_flat), CudaCallback, Recorder, partial(GradientClipping, clip=0.1), partial(RNNTrainer, α=2., β=1.), ProgressCallback] learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt()) learn.fit(1) . ULMFiT . (Jump to Lesson 12 video) . Pretraining (Wikitext) . (Notebook: 12b_lm_pretrain.ipynb) . We now have a language model trainer using AWD-LSTM. We can now use what we have above to train a language model using Wikitext. This is covered in the course notebook. . Here’s how you can download WikiText103: . wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-{version}-v1.zip -P {path} unzip -q -n {path}/wikitext-{version}-v1.zip -d {path} mv {path}/wikitext-{version}/wiki.train.tokens {path}/wikitext-{version}/train.txt mv {path}/wikitext-{version}/wiki.valid.tokens {path}/wikitext-{version}/valid.txt mv {path}/wikitext-{version}/wiki.test.tokens {path}/wikitext-{version}/test.txt . Split it into articles: . def istitle(line): return len(re.findall(r&#39;^ = [^=]* = $&#39;, line)) != 0 def read_wiki(filename): articles = [] with open(filename, encoding=&#39;utf8&#39;) as f: lines = f.readlines() current_article = &#39;&#39; for i,line in enumerate(lines): current_article += line if i &lt; len(lines)-2 and lines[i+1] == &#39; n&#39; and istitle(lines[i+2]): current_article = current_article.replace(&#39;&lt;unk&gt;&#39;, UNK) articles.append(current_article) current_article = &#39;&#39; current_article = current_article.replace(&#39;&lt;unk&gt;&#39;, UNK) articles.append(current_article) return articles . Create training and validation datasets: . train = TextList(read_wiki(path/&#39;train.txt&#39;), path=path) valid = TextList(read_wiki(path/&#39;valid.txt&#39;), path=path) sd = SplitData(train, valid) . Then, as before, tokenize and numericalize, then databunchify. You can then train it on a GPU for about 5 hours. Language models take a long time to train, but luckily we only need to do it once and then we can reuse that model for much faster fine-tuning. . Finetuning (IMDb) . (Notebook: 12c_ulmfit.ipynb) . You can download a small model pretrained on wikitext 103 using: . wget http://files.fast.ai/models/wt103_tiny.tgz -P {path} tar xf {path}/wt103_tiny.tgz -C {path} . This language model can be used to fine-tune on other NLP tasks. For example the IMDb dataset. If we now create the dataloaders for IMDb, like before, it will will create a vocabulary based on that data alone. This won’t be the same vocabulary as wikitext language model. There will be different tokens and tokens will be numericalized differently. . We somehow need to match our pretrained weights to the new vocabulary. This is done on the embeddings and the decoder (since the weights between embeddings and decoders are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order. . We just go through each vocab item in the IMDb vocab, find out if it is in the Wikitext103 vocab, and if it is we copy Wikitext103’s embedding over. Anytime there is a word in IMDb and NOT in Wikitext103 we just set its embedding term to the mean bias and mean weight. . house_wgt = old_wgts[&#39;0.emb.weight&#39;][idx_house_old] house_bias = old_wgts[&#39;1.decoder.bias&#39;][idx_house_old] def match_embeds(old_wgts, old_vocab, new_vocab): wgts = old_wgts[&#39;0.emb.weight&#39;] bias = old_wgts[&#39;1.decoder.bias&#39;] wgts_m,bias_m = wgts.mean(dim=0),bias.mean() new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1)) new_bias = bias.new_zeros(len(new_vocab)) otoi = {v:k for k,v in enumerate(old_vocab)} for i,w in enumerate(new_vocab): if w in otoi: idx = otoi[w] new_wgts[i],new_bias[i] = wgts[idx],bias[idx] else: new_wgts[i],new_bias[i] = wgts_m,bias_m old_wgts[&#39;0.emb.weight&#39;] = new_wgts old_wgts[&#39;0.emb_dp.emb.weight&#39;] = new_wgts old_wgts[&#39;1.decoder.weight&#39;] = new_wgts old_wgts[&#39;1.decoder.bias&#39;] = new_bias return old_wgts . We then repeat the same process as before to train a fine-tuned language model for IMdB. . Classification (IMDb) . (Jump_to lesson 12 video) . Now we can tackle the IMDb classifier task - positive/negative reviews. We load the data the same way as before, but using the language model vocab: . proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor() il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;]) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;test&#39;)) ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat) bs,bptt = 64,70 data = clas_databunchify(ll, bs) . We again use AWD-LSTM. Reminder: its forward pass outputs: raw_outputs, outputs, mask. (Where mask is the mask for the padding tokens). . Concat Pooling: We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. (Also ignore the padding in the last element/average/maximum). We concatenate all these together into one vector: . class Pooling(nn.Module): def forward(self, input): raw_outputs,outputs,mask = input output = outputs[-1] lengths = output.size(1) - mask.long().sum(dim=1) avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1) avg_pool.div_(lengths.type(avg_pool.dtype)[:,None]) max_pool = output.masked_fill(mask[:,:,None], -float(&#39;inf&#39;)).max(dim=1)[0] x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling. return output,x . People used to just used the final activation. ULMFiT uses all the activations between the layers and it works better. . This gives us the PoolingLinearClassifier, which is just a sequence of batchnorm dropout linear layers that consumes the output of Concat Pooling. This will output a vector that is (batch, n_classes) sized, which can be then passed to a loss function (e.g. cross_entropy). . def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None): layers = [nn.BatchNorm1d(n_in)] if bn else [] if p != 0: layers.append(nn.Dropout(p)) layers.append(nn.Linear(n_in, n_out)) if actn is not None: layers.append(actn) return layers class PoolingLinearClassifier(nn.Module): &quot;Create a linear classifier with pooling.&quot; def __init__(self, layers, drops): super().__init__() mod_layers = [] activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None] for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs): mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn) self.layers = nn.Sequential(*mod_layers) def forward(self, input): raw_outputs,outputs,mask = input output = outputs[-1] lengths = output.size(1) - mask.long().sum(dim=1) avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1) avg_pool.div_(lengths.type(avg_pool.dtype)[:,None]) max_pool = output.masked_fill(mask[:,:,None], -float(&#39;inf&#39;)).max(dim=1)[0] x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling. x = self.layers(x) return x . Now we just need to run the LSTM through the IMDb reviews one bptt at a time, recording the raw_output, output, and mask as it goes. (These all will go into the PoolingLinearClassifier afterwards, whose subsequent output will go into a cross_entropy loss). The code for this is the Sentence Encoder: . class SentenceEncoder(nn.Module): def __init__(self, module, bptt, pad_idx=1): super().__init__() self.bptt,self.module,self.pad_idx = bptt,module,pad_idx def concat(self, arrs, bs): return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))] def forward(self, input): bs,sl = input.size() self.module.bs = bs self.module.reset() raw_outputs,outputs,masks = [],[],[] for i in range(0, sl, self.bptt): r,o,m = self.module(input[:,i: min(i+self.bptt, sl)]) masks.append(pad_tensor(m, bs, 1)) raw_outputs.append(r) outputs.append(o) return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1) . Here is the Full AWD-LSTM Classifier for IMDb: . def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None): &quot;To create a full AWD-LSTM&quot; rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p) enc = SentenceEncoder(rnn_enc, bptt) if layers is None: layers = [50] if drops is None: drops = [0.1] * len(layers) layers = [3 * emb_sz] + layers + [n_out] drops = [output_p] + drops return SequentialRNN(enc, PoolingLinearClassifier(layers, drops)) . To do transfer learning with the pretrained weight we follow the same procedure we saw before with ResNet. We load the fine-tuned weights into the model with module.load_state_dict method from PyTorch. Then we freeze that part of the model and initially train just the head (the PoolingLinearClassifier). We then unfreeze the whole model and continue to train that for a few more epochs. . In the end this gives 92% on IMDb, which was SOTA a few years ago. . Conclusion . There are two more lessons left in FastAI v3: . Lesson 13 (2019) - Basics of Swift for Deep Learning | Lesson 14 (2019) - Swift: C interop; Protocols; Putting it all together | . I don’t plan on producing extensive notes on these two lectures since are focussed on using Swift for deep learning with Swift for Tensorflow. There is excellent discussion on the weaknesses of Python for deep learning and the space for disruption by a better language like Swift or Julia. While this is quite an interesting topic, it’s not so useful for me to invest my energy writing about this at this time (each of these notes was hard work!), and I want to move onto other projects. These lectures are also very code heavy and provide a coding introduction to Swift. It wouldn’t be worth reproducing that here when there are myriad better tutorials available elsewhere. I’m also conscious that it’s over a year since these lectures were published, and a year hence it’s not clear to me whether Swift for Tensorflow has gained much momentum in the community. Julia, in the meantime, seems like it might be a more promising rival to Python. . Q &amp; A . What do you mean by keeping a scientific journal? . When you look at the great scientists in history they all had great scientific journal practices. . In my case it’s a piece of software called Windows Notepad, and I paste things into it at the bottom, and when I want to find something I press Ctrl-F. It just needs to be something that has a record of what you’re doing and what the results are. Because scientists that make the break throughs generally make the break-throughs because they look at something that shouldn’t be and they say “Oh! That’s odd! What’s going on?” . For example, the discovery of the noble gases was because a scientist saw one little bubble left in the beaker, and they were pretty sure there shouldn’t be a bubble there. Most people would have just ignored it, but they studied the bubble and discovered the noble gases. . Another example is penicilin. . I find that in deep learning this is true as well. I spent a lot of time studying batch normalization and transfer learning because a few years ago in Keras I was getting terrible transfer learning results for something I thought should be much more accurate. I thought - “Oh, that’s odd.” - and I spent weeks changing everything I could, and then, almost randomly, tried changing batch norm. . All this fiddling around - 90% doesn’t lead anywhere, but it’s the other 10% that you won’t be able to figure out, unless you can go back confirm that the result of an experiment really did happen. . You can record the dates, github commits, logs, command lines, whatever to ensure that you could go back and reproduce the experiment later on. . | Why are you against using cross-validation for deep learning? . Cross-validation is a very useful technique for getting a reasonably sized validation set if you don’t have enough data to otherwise create a reasonably sized validation set. . It was particularly popular in the days when most studies were say 50-60 rows. If you have a few 1000 rows it’s just pointless - the statistical significance is going to be there regardless. I’m not against it, it’s just most of the time you don’t need it. Because if you have a 1000 things in the validation set and you only care whether it’s plus or minus 1% it’s totally pointless. . Have a look to see how much your validation set accuracy is varying from run to run and if it’s too much that you can’t make the decisions you need to make, then you can add cross-validation. . | What are you best tips for debugging deep learning? . Don’t make mistakes in the first place(!) . The only way to do that is to make your code so simple that it can’t possibly have a mistake and check every single intermediate result along the way to make sure it doesn’t have a mistake. . Otherwise you could end up spending a month of your time like I did last month. A month ago I got 94.1% accuracy on ImageNet. Then I started trying various tweaks, but none of the tweaks seemed to help. As a sanity check I decided to repeat the previous training and I couldn’t repeat it - I was getting 93.5%. (Every training run took 6 hours cost me $150 on AWS!). . So it’s a big process to even realise that it’s broken. When you’ve written deep learning code wrong, it gets broken in ways you don’t even notice. . You need to be a great scientist to do deep learning - keep excellent journals of your results. I could go back to my journal to see when I got the 94.1% result, so I could revert fastai back to a commit of that time and reran and successfully reproduce the result. . I could then bisect the changes made to fastai in the meantime until I found the bug in the mixed-precision module. The bug was subtle and didn’t show up until epoch 50! Finding it cost $5000… . The tiny difference is so insignificant that noone using fastai noticed the error and it was only noticible when trying to get a SOTA imagenet result. . These types of ‘soft-bugs’ are common in deep learning, really hard to detect and tedious to track down!! . | In NLP, do you do any other preprocessing such as removing stop words, stemming, or lemmatization? . In traditional NLP those are importanting things to do. Stop words are things like: ‘a’, ‘on’, ‘the’. Stemming is getting rid of the ‘ing’ suffix and stuff like that. These are pretty universal in NLP… It’s an absolutely terrible idea. Never do this. . Why would you remove information from your neural net that might be useful? It is useful. Your use of stop words tells you a lot about your style of language. For example, you’ll often have a lot fewer articles if you are really angry and speaking quickly. The tense you are talking about is important, and stemming gets rid of it. . All these kinds of preprocessors are in the past. In general, preprocessing data in neural nets - the rule of thumb is to leave it as raw as you can. . | . Links and References . Lesson 12 video lecture | Label Smoothing [paperswithcode/methods] | Mixed Precision Training [NVIDIA Blog] | A great explanation of How floats work [YouTube]. | This video works through Adding two floats at the bit level [YouTube] | Backpropagation Through Time Blog post [Machine Learning Mastery] | FastAI forum post by Stefano Giomo the demystifies the difference between sequence length, BPTT, and batch size in RNNs. | RNN Refresher: Visualizing RNNs [Josh Varty] | Understanding LSTM Networks [Chris Olah] | Understanding building blocks of ULMFiT [Kerem Turgutlu; Medium] | Lesson Notebooks: 10_augmentation.ipynb | 10b_mixup_label_smoothing.ipynb | 10c_fp16.ipynb | 11_train_imagenette.ipynb | 11a_transfer_learning.ipynb | 12_text.ipynb | 12b_lm_pretrain.ipynb | 12c_ulmfit.ipynb | . | Papers: Fast Image Augmentation: Mixup paper [2017], CutOut paper [2017], CutMix [2019] | When Does Label Smoothing Help? [2019] | Original ResNet paper [2015] | Bag of Tricks paper [2019] | AWD-LSTM - Regularizing and Optimizing LSTM Language Models, Merity et al. [2017] | . | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fast.ai v3 Lesson 11 Notes: Data Block API, the Generic Optimizer, Data Augmentation",
            "content": "Overview . This lesson starts with introducing a simple initialization technique called Layer-wise Sequential Unit Variance (LSUV). This technique iteratively sets the weights or each layer in your model so their outputs are normally distributed, without needing to derive any fiddly formulae for each different activation you are using. . Next the lesson shows how to implement fastai’s Data Block API. . After that, the lesson gets into optimization. It implements Optimizer and StatefulOptimizer and shows that nearly all optimizers used in modern deep learning training are just special cases of these classes. They use it to add weight decay, momentum, Adam, and LAMB optimizers. . Finally, the lesson looks at data augmentation, specifically for images. It shows that data augmentation can also be done on the GPU, which speeds things up quite dramatically. . Link to the lesson 11 video . (Update 2020-08-22: fixed typos, fixed incorrect explanation of compose function, expanded on AdamW, included trust ratio in LAMB.) | . Layer-wise Sequential Unit-Variance (LSUV) . It’s really fiddly to get unit variances throughout the layers of the network. If you change one thing like your activation function or the add dropout, change the amount of dropout then you’d have to alter the initialization again to adjust for this. If the variance of a layer is just a little bit different to 1, then it will get exponentially worse in the subsequent layers. You would need to analytically workout how to reinitialize things. . There is a better way. In the paper All you need is a good init [2015] - the authors created a way to let the computer figure out how to reinitialize everything. This technique is called Layer-wise Sequential Unit-Variance (LSUV). . The algorithm is very simple: . Loop through every layer l in the network . While stdev of layer’s output h.std() is not approximately 1.0: Do a forward pass with a mini-batch | Get the layer’s output tensor: h | Update the layer’s weights: W_l = W_l / Var(h).sqrt() | . | While the mean of the layer’s output: h.mean()​ is not approximately 0.0: Do a forward pass with a mini-batch | Get the layer’s output tensor: h | Update the layer’s bias: bias_l = bias_l - h.mean() | . | . | . Here is the PyTorch code to do LSUV using PyTorch hooks to record the statistics of the target module in the model: . def find_modules(m, cond): # recursively walk through the layers in pytorch model # returning a list of all that satisfy `cond` if cond(m): return [m] return sum([find_modules(o,cond) for o in m.children()], []) def lsuv_module(m, xb, mdl): h = Hook(m, append_stat) while mdl(xb) is not None and abs(h.std-1) &gt; 1e-3: m.weight.data /= h.std while mdl(xb) is not None and abs(h.mean) &gt; 1e-3: m.bias -= h.mean h.remove() return h.mean,h.std mdl = learn.model.cuda() mods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer)) for m in mods: print(lsuv_module(m, xb, mdl)) ## output: ## (2.1287371865241766e-08, 1.0) ## (2.5848953200124924e-08, 1.0) ## (-5.820766091346741e-10, 0.9999999403953552) ## (-2.6775524020195007e-08, 1.0) ## (2.2351741790771484e-08, 1.0) . Let’s visualize the layers with the histograms like we did in the last lesson. . No LSUV, normal init: the histograms and proportions of non-zeros of the weights over time during training: . . . With LSUV: the histograms and proportions of non-zeros of the weights over time during training: . . . LSUV is something you run on all the layers at the beginning before starting training. You can also take more than one mini-batch. . Links, interesting forum posts: . https://forums.fast.ai/t/implementing-the-empirical-initialization-from-all-you-need-is-a-good-init/42284 . | https://forums.fast.ai/t/lsuv-improvement/49571 . | . Imagenette Dataset . (Jump to lesson 11 video) . We are getting great results very fast on MNIST. It’s time to put away MNIST and try a dataset that’s a bit harder. We aren’t quite ready to take on the ImageNet dataset, because ImageNet is very large and takes several days to train on one GPU. We need something that has a faster feedback loop than that for practising, learning, or researching. . Another image dataset is CIFAR-10, but this one consists of 32x32 images. It turns out that small images have very different characteristics to large images. Under 96x96 stuff behaves very differently. So stuff that works well on CIFAR-10, tends not to work well on larger images. . The same authors of the ‘All you need is a good init’ paper, Dmytro Mishkin et al., showed in another paper ‘Systematic evaluation of CNN advances on the ImageNet [2017]’ that: . … the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images. . If we do experiments with a dataset of 128x128 sized images the results will be applicable to larger images and we can save heap of time too. But simply resizing Imagenet down to 128x128 still takes too long because there are loads of different classes. . To fill this gap in the market, Jeremy has created Imagenette, which has normal sized images that are trainable in a sane amount of time. Imagenette consists of 3 datasets, which are all subsets of Imagenet. . Imagenette: A subset of 10 easily classified classes from Imagenet. | Imagewoof: A subset of 10 classes from Imagenet that aren’t easy to classify (all dog breeds). | Image网 (or Imagewang): Imagenette and Imagewoof combined with some twists to make it into a tricky semi-supervised unbalanced classification problem. | Each of the datasets is available in 3 sizes: . Full size. | Shortest length resized to 160px (aspect ratio maintained). | Shortest length resized to 320px (aspect ratio maintained). | . Jeremy says… . A big part of a getting good at using deep learning in your domain is knowing how to create small, workable, useful datasets… . Try to come up with a toy problem or two that will give you insight into your full problem. . Data Block API Foundations . (Jump to lesson 11 video) . Imagenette isn’t big, but it’s too big to fit into RAM. We need to read it in one image at a time. We need to design and build fastai’s Data Block API from scratch. . What does the raw image data look like? Here is the directory structure of Imagenette and the number of images for each class: . imagenette2-160 ├── train │   ├── n01440764 [963 entries] │   ├── n02102040 [955 entries] │   ├── n02979186 [993 entries] │   ├── n03000684 [858 entries] │   ├── n03028079 [941 entries] │   ├── n03394916 [956 entries] │   ├── n03417042 [961 entries] │   ├── n03425413 [931 entries] │   ├── n03445777 [951 entries] │   └── n03888257 [960 entries] └── val ├── n01440764 [387 entries] ├── n02102040 [395 entries] ├── n02979186 [357 entries] ├── n03000684 [386 entries] ├── n03028079 [409 entries] ├── n03394916 [394 entries] ├── n03417042 [389 entries] ├── n03425413 [419 entries] ├── n03445777 [399 entries] └── n03888257 [390 entries] . There is are train and val directories. Each contains a subdirectory of JPEG images. The name of these subdirectories comes from Imagenet and is an encoding of the different categories and subcategories of objects. See the ImageNet explorer to get what I mean. The classes are all fairly balanced too. . All the images are roughly the same size, but have their own dimensions and are rectangular. . The first class n01440764 is a tench, which is a kind of fish: . . What does the data of this image look like? If we read it into python with the image library PIL and convert it to a numpy array, we can see it is an array with shape (160, 237, 3) and its numbers look like: . . It’s an RGB image, with each pixel represented by 3 integers between 0 and 255, which say what colour the pixel is. . The first thing to build before the Datablack API is good way of grabbing all the files we need for training and validating our model. For image files, there are a number of different file types available. We can easily get a list of all the standard image file extensions from the python module mimetypes. . . With the list of image file extensions we can do a walk through the dataset’s directory path to grab all these files. Here is the function used in fastai to recursively walk the directory path: . def get_files(path, extensions=None, recurse=False, include=None): path = Path(path) extensions = setify(extensions) extensions = {e.lower() for e in extensions} if recurse: res = [] for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames) if include is not None and i==0: d[:] = [o for o in d if o in include] else: d[:] = [o for o in d if not o.startswith(&#39;.&#39;)] res += _get_files(p, f, extensions) return res else: f = [o.name for o in os.scandir(path) if o.is_file()] return _get_files(path, f, extensions) . Where the helper function _get_files takes a list of files in a directory and selects only the files that have the right extension. get_files also uses the fast low level file functions os.walk and os.scandir. These functions connect to library functions written in C and are orders of magnitudes faster than using something like python’s glob module. . DataBlock API Motivation . (Jump to lesson 11 video) . Why does FastAI have a DataBlock API? The API attempts to systematically define all the steps necessary to prepare data for a deep learning model, and create a mix and match rec ipe book for combining these steps. . To prepare for modeling, the following steps need to be performed: . Get the source items | Splitting the items into training and validation sets e.g. random fraction, folder name, CSV, … | . | Labelling the items, e.g. from folder name, file name/re, CSV, … | . | Processing the items (such as normalization) | (Optional) Doing some Augmentation | Transform items into tensors | Make data into batches (DataLoader) | (Optional) Transform per batch | Combine the DataLoaders together into a DataBunch | (Optional) Add a test set | . Step 1 - ImageList . We need to get the source items and store them in some kind of collection data structure. We already created the ListContainer data structure for storing things in a list in a previous lecture that we can build upon here. What we want to do is not store the loaded source data in our list, rather store the filenames of the source data in a list and load things into memory when they are needed. . We create a base class called ItemList that has a get method, which subclasses override, and this get method should load and return what you put in there. An item is some data point, it could be an image, text sequence, whatever. For the case of the ImageList subclass, get will read the image file and return a PIL image object. . In summary, ItemList: . Is a list of items and a path where they came from . | Optionally has a list of transforms, which are functions. . | The list of transforms is composed and applied every time you get and item. So you get back a transformed item every time. . | . class ItemList(ListContainer): def __init__(self, items, path=&#39;.&#39;, tfms=None): super().__init__(items) self.path,self.tfms = Path(path),tfms def __repr__(self): return f&#39;{super().__repr__()} nPath: {self.path}&#39; def new(self, items, cls=None): if cls is None: cls=self.__class__ return cls(items, self.path, tfms=self.tfms) def get(self, i): return i def _get(self, i): return compose(self.get(i), self.tfms) def __getitem__(self, idx): res = super().__getitem__(idx) if isinstance(res,list): return [self._get(o) for o in res] return self._get(res) . Aside: compose function: takes a list of functions and combines them into a pipeline that chains the outputs of the first function to input of the second and so on. In other words, a deep neural network is just a composition of functions (layers). NB, if you compose a list of functions then the order they are applied is right-to-left. As a one-liner: . compose([f, g, h], x) = f(g(h(x))) . Here is the implementation for ImageList: . class ImageList(ItemList): @classmethod def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs): if extensions is None: extensions = image_extensions return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs) def get(self, fn): return PIL.Image.open(fn) . It defines the class method from_files to get a list of image files from a path. It uses the image_extensions and searches the path using our get_files function. Its get method is overridden and returns a PIL image object. . What about transforms? The first transform we create is make_rgb. When loading in images, if an image is grayscale then PIL will read it in as a rank 2 tensor, when we want it to be rank 3. So the make_rgb transform calls the PIL method to convert it to RGB: . def make_rgb(item): return item.convert(&#39;RGB&#39;) il = ImageList.from_files(path, tfms=make_rgb) . Step 2 - Split Validation Set . (Jump to lesson 11 video) . Next we want to split the data into train and validation sets. For Imagenette training and validation sets have already been created for us and live in different directories. These are the train and val subdirectories. The path of an image in the dataset is something like this: . imagenette2-160/val/n02102040/n02102040_850.JPEG . The parent of the image is its label, and the parent of its parent (grandparent) denotes whether it is in the training or validation set. Therefore we will create a splitter function that splits on an image path’s grandparent: . def grandparent_splitter(fname, valid_name=&#39;valid&#39;, train_name=&#39;train&#39;): gp = fname.parent.parent.name return True if gp==valid_name else False if gp==train_name else None . Let’s go further and encapsulate this into a SplitData class that can apply a splitter function to any kind of ItemList object: . class SplitData(): def __init__(self, train, valid): self.train,self.valid = train,valid def __getattr__(self,k): # This is needed if we want to pickle SplitData and be able to load it back without recursion errors return getattr(self.train,k) def __setstate__(self,data:Any): self.__dict__.update(data) @classmethod def split_by_func(cls, il, f): lists = map(il.new, split_by_func(il.items, f)) return cls(*lists) def __repr__(self): return f&#39;{self.__class__.__name__} nTrain: {self.train} nValid: {self.valid} n&#39; . This has a split_by_func, which uses ItemList.new to coerce the train and test items back into their original ItemList type. So in the end we will get two ImageList objects for training and validation image sets. . This looks like this: . SplitData Train: ImageList (12894 items) [PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_9403.JPEG&#39;), PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_6402.JPEG&#39;), PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_4446.JPEG&#39;), PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_13476.JPEG&#39;)...] Path: /home/ubuntu/.fastai/data/imagenette-160 Valid: ImageList (500 items) [PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00016387.JPEG&#39;), PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00034544.JPEG&#39;), PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00009593.JPEG&#39;), PosixPath(&#39;/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00020698.JPEG&#39;)...] Path: /home/ubuntu/.fastai/data/imagenette-160 . Step 3 - Labelling . (Jump to lesson 11 video) . Labelling is a little more tricky because it has to be done after splitting, at it uses training set information to apply to the validation set. To do this we need to create something called a Processor. For example, we could have a processor whose job it was to encoded the label strings into numbers: . “tench” =&gt; 0 | “french horn” =&gt; 1 | . We would need the training set to have the same mapping as the validation set. So we need to create a vocabulary which encodes our classes to numbers and tells us the order they are in. We create this vocabulary from the training set and use that to transform the training and the validation sets. . Other examples of processors would be processing texts to tokenize and numericalize them. Text in the validation set should be numericalized the same way as the training set. Or in another case with tabular data, where we wish to fill missing values with, for instance, the median computed on the training set. The median is stored in the inner state of the Processor and applied on the validation set. . Here we label according to the folders of the images, so simply fn.parent.name. We label the training set first with a newly created CategoryProcessor so that it computes its inner vocab on that set. Then we label the validation set using the same processor, which means it uses the same vocab. . class Processor(): def process(self, items): return items class CategoryProcessor(Processor): def __init__(self): self.vocab=None def __call__(self, items): #The vocab is defined on the first use. if self.vocab is None: self.vocab = uniqueify(items) self.otoi = {v:k for k,v in enumerate(self.vocab)} return [self.proc_(o) for o in items] def proc_(self, item): return self.otoi[item] def deprocess(self, idxs): assert self.vocab is not None return [self.deproc_(idx) for idx in idxs] def deproc_(self, idx): return self.vocab[idx] . def label_by_func(sd, f, proc_x=None, proc_y=None): train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y) valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y) return SplitData(train,valid) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) . LabeledData is an object that has two ItemList objects: x and y. In this case x is an ImageList (basically a list of file paths) and y is a ItemList (a generic container, here it contains labels: 0, 1, etc). . This output ll looks like: . SplitData Train: LabeledData x: ImageList (12894 items) [...] Path: ... y: ItemList (12894 items) [0,0,0,0,0,0...] Path: ... Valid: LabeledData x: ImageList (500 items) [...] Path: ... y: ItemList (500 items) [0,0,0,0,0,0...] Path: ... . Question: How do could we handle unseen labels? . You could group together rare labels into a single label called ‘other’/’unknown’ . Step 4 - DataBunch . (Jump_to lesson 11 video) . A DataBunch has a training dataloader and a validation data loader. Here is the class: . class DataBunch(): def __init__(self, train_dl, valid_dl, c_in=None, c_out=None): self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out @property def train_ds(self): return self.train_dl.dataset @property def valid_ds(self): return self.valid_dl.dataset def databunchify(sd: SplitData, bs: int, c_in=None, c_out=None, **kwargs): dls = get_dls(sd.train, sd.valid, bs, **kwargs) return DataBunch(*dls, c_in=c_in, c_out=c_out) SplitData.to_databunch = databunchify . All the steps . (Jump_to lesson 11 video) . Here’s the fully dataloading pipeline using the Data Block API: grab the path, untar the data, list the transforms, get item list, split the data, label the data, create a databunch. . path = datasets.untar_data(datasets.URLs.IMAGENETTE_160) tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . New CNN Model . (Jump_to lesson 11 video) . Let’s train a CNN using our databunch. . Get the callbacks: . cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback] . Next we need to normalize all the images for training. With colour images we need to normalize all three channels so we need means and standard deviations for each of channels. We can get these statistics from a batch/batches from the training set. . def normalize_chan(x, mean, std): return (x-mean[...,None,None]) / std[...,None,None] _m = tensor([0.47, 0.48, 0.45]) _s = tensor([0.29, 0.28, 0.30]) norm_imagenette = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda()) cbfs.append(partial(BatchTransformXCallback, norm_imagenette)) . We build our model using Bag of Tricks for Image Classification with Convolutional Neural Networks, in particular: we don’t use a big conv 7x7 at first but three 3x3 convs, and don’t go directly from 3 channels to 64 but progressively add those. The first 3 layers are very important. Back in the old days people would use 5x5 and 7x7 kernels for the first layer. However the Bag of Tricks paper shows that this isn’t a good idea, which refers to many previous citations and competition winning models. The message is clear - 3x3 kernels give you more bang for your buck. You get deeper, you get the same receptive field, and it’s also faster because you have less working going on. The 7x7 conv layer also is over 5 times slower than a single 3x3 as well. . (Recall - a conv_layer composes a Conv2d, Generalized ReLU, and a normalization (e.g. batchnorm)) . The first layer is a 3x3 kernel and a known number of channels data.c_in, which in this case is 3 (RGB). What about the number of outputs? The kernel has 9*c_in numbers. We want to make sure that our kernal has something useful to do. You don’t want more numbers coming out than are coming in, because its a waste of time. We set the number of outputs to the closest power of 2 below 9*c_in. (For 9*3 that is 16). The stride of the first layer is also 1, so the first layer doesn’t downsample. . Then for the next two layers we successively mutiply the number of outputs by 2 and set stride to 2. . (Anywhere you see something that isn’t a 3x3 kernel - have a big think as to whether it makes sense.) . We use 4 conv_layers in the body of the model with sizes: nfs = [64,64,128,256] . Here is the code for the model: . import math def prev_pow_2(x): return 2**math.floor(math.log2(x)) def get_cnn_layers(data, nfs, layer, **kwargs): def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs) l1 = data.c_in l2 = prev_pow_2(l1*3*3) layers = [f(l1 , l2 , stride=1), f(l2 , l2*2, stride=2), f(l2*2, l2*4, stride=2)] nfs = [l2*4] + nfs layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)] layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)] return layers def get_cnn_model(data, nfs, layer, **kwargs): return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs)) . We run this with cosine 1cycle annealing: . sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05)) learn,run = get_learn_run(nfs, data, 0.2, conv_layer, cbs=cbfs+[ partial(ParamScheduler, &#39;lr&#39;, sched) ]) . This gives performance 72.6% for Imagenette, which is not bad and on the right track. . Universal Optimizer . (Jump to lesson 11 video) . Every other deep learning library treats every optimizer algorithm as a totally different object. But this is an artificial categorization - there is however only one optimizer and lots of stuff you can add to it. . We are going to implement this paper: Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. We will implement this equation set from the paper: . . This looks scary because of the mathematical notation and greek symbols, but we will find when we turn it into code it is actually very simple. All these terms are separable parts or ‘steppers’ of a more general optimizer class. . All experiments will be done with our CNN model using the Imagenette dataset. . The Optimizer Class . Let’s build own Optimizer class. It needs to have a zero_grad method to set the gradients of the parameters to zero and a step method that does some kind of step. The thing we will do differently from all other libraries is that the functionality of step will be abstracted out into a composition of stepper functions. The Optimizer class will simply have a list of steppers to iterate through. . In order to optimize something we need to know what all the parameter tensors are in a model. However we might want to say: “the last two layers should have a different learning rate to the rest of the layers.” We can instead decide group different parameters into param_groups, which would basically be a list of lists. Each parameter group can have its own set of hyperparameters (e.g. learning rate, weight decay, etc) and each parameter group will have its own dictionary to store these hyperparameters. . Code for the Optimizer class, with a way of getting default hyperparameters for the steppers: . class Optimizer(): def __init__(self, params, steppers, **defaults): self.steppers = listify(steppers) maybe_update(self.steppers, defaults, get_defaults) # might be a generator self.param_groups = list(params) # ensure params is a list of lists if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups] self.hypers = [{**defaults} for p in self.param_groups] def grad_params(self): # return flattened list of parameters from all layers return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers) for p in pg if p.grad is not None] def zero_grad(self): for p,hyper in self.grad_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper) def maybe_update(os, dest, f): for o in os: for k,v in f(o).items(): if k not in dest: dest[k] = v def get_defaults(d): return getattr(d,&#39;_defaults&#39;,{}) . This is basically the gist of PyTorch’s optim.Optimizer, but with the steppers. A stepper is a function that forms part of the optimizer recipe. An example of stepper is sgd_step: . def sgd_step(p, lr, **kwargs): &quot;&quot;&quot; SGD step p : parameters lr : learning rate &quot;&quot;&quot; p.data.add_(-lr, p.grad.data) return p . In other words we can create an optimizers like this: . opt_func = partial(Optimizer, steppers=[sgd_step]) . When we call step, it loops through all our parameters and composes all our steppers then calls that composition on the parameters. . Weight Decay . (This subsection is combines explanations from the 09_optimizers.ipynb notebook and this fastai blog post) . By letting our model learn high parameters, it might fit all the data points in the training set with an over-complex function that has very sharp changes, which will lead to overfitting. . . Weight decay comes from the idea of L2 regularization, which consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible. . Classic L2 regularization consists of adding the sum of all the weights squared to the loss multiplied by a hyperparameter, wd. The intuition is that large weight values get ‘exploded’ when they are squared which will contribute to a much larger loss. The optimizer will therefore shy away from such regions of parameter space. In theory, this would be like adding this big sum to the total loss at the end of the forward pass: . loss_with_wd = loss + wd * all_weights.pow(2).sum() / 2 . This is never how this is implemented in practice however. The sum would require a massive reduction of all the weight tensors at every update step, which would be expensive and potentially numerically unstable (more so with lower precision). We only need the derivative of that wrt to each of the weights, and remembering that $ frac{ partial}{ partial w_j} sum_i w_i^2 = 2 w_j$, you can see that adding the big sum to the loss is equivalent to locally updating the gradients of the parameters like so: . weight.grad += wd * weight . For the case of vanilla SGD this is equivalent to updating the parameters with: . weight = weight - lr * (weight.grad + wd.weight) . This technique is called weight decay, as each weight is decayed by a factor lr * wd, as it’s shown in this last formula. . This is a slightly confusing thing - Aren’t L2 regularization and Weight decay the same thing? – Not exactly. Only in the case of vanilla SGD are they the same. . For algorithms such as momentum, RMSProp, and Adam, the update has some additional formulas around the gradient. For SGD with momentum the formula is: . moving_avg = alpha * moving_avg + (1 - alpha) * w.grad w = w - lr * moving_avg . If we did L2 regularization this would become: . moving_avg = alpha * moving_avg + (1 - alpha) * (w.grad + wd*w) w = w - lr * moving_avg . Whereas with weight decay it would be: . moving_avg = alpha * moving_avg + (1 - alpha) * w.grad w = w - lr * moving_avg - lr * wd * w . We can see that the part subtracted from w linked to regularization isn’t the same in the two methods, and the wd is polluted by the (1-alpha) factor. When using something more complicated like the Adam optimizer, it gets even more polluted. Most libraries use the first formulation, but as it was pointed out in Decoupled Weight Regularization by Ilya Loshchilov and Frank Hutter, it is better to use the second one with the Adam optimizer, which is why the fastai library made it its default. This implemation of Adam and decoupled weight decay is often called AdamW. . . The above is a comparison between the two done by Jeremy and Sylvain. The weight decay formulation gives slightly better results. . Weight decay is also super simple to implement too - you simply subtract lr*wd*weight from the weights before the optimizer step. We could create some abstract base class for stepper or just use a function in python: . def weight_decay(p, lr, wd, **kwargs): p.data.mul_(1 - lr*wd) return p weight_decay._defaults = dict(wd=0.) . In python you can attach an attribute to any object in python including functions. Here we attach a dictionary _defaults for default hyper-parameter values. Alternatively, if you were using an abstract base class you would just have a class attribute _defaults to get the sam effect. . Similarly, if you wanted to use L2 regularization then the implementation is also simply - add wd*weight to the gradients: . def l2_reg(p, lr, wd, **kwargs): p.grad.data.add_(wd, p.data) # add is actually scaled-add return p l2_reg._defaults = dict(wd=0.) . Momentum . Momentum will require an optimizers that has some state because it needs to remember what it did in the last update to do the current update. . Momentum requires to add some state. We need to save the moving average of the gradients to be able to do the step and store this inside the optimizer state. We need to track, for every single parameter, what happened last time. This is actually quite a bit of state - if you had 10 million activations in your network, you now have 10 million more floats that you have to store. . To implement this we need to create a new subclass of Optimizer which maintains a state attribute which can store running Stats of things, which are updated every step. A Stat is an object that has two methods and an attribute: . init_state, that returns the initial state (a tensor of 0. for the moving average of gradients) | update, that updates the state with the new gradient value. Takes a state dict and returns an updated state dict. . | We also read the _defaults values of those objects, to allow them to provide default values to hyper-parameters. | . The StatefulOptimizer: . class StatefulOptimizer(Optimizer): def __init__(self, params, steppers, stats=None, **defaults): self.stats = listify(stats) maybe_update(self.stats, defaults, get_defaults) super().__init__(params, steppers, **defaults) self.state = {} def step(self): for p,hyper in self.grad_params(): if p not in self.state: # Create a state for p and # call all the statistics to initalize it self.state[p] = {} maybe_update(self.stats, self.state[p], lambda o: o.init_state(p)) state = self.state[p] for stat in self.stats: state = stat.update(p, state, **hyper) # run the steppers compose(p, self.steppers, **state, **hyper) self.state[p] = state . For momentum we are mainting an moving average of the parameter gradients. The Stat for this would be: . class AverageGrad(Stat): # with NO dampening _defaults = dict(mom=0.9) def init_state(self, p): return {&quot;grad_avg&quot;: torch.zeros_like(p.grad.data)} def update(self, p, state, mom, **kwargs): state[&quot;grad_avg&quot;].mul_(mom).add_(p.grad.data) return state . With this we can now implement MomentumSGD a new stepper, momentum_step: . def momentum_step(p, lr, grad_avg, **kwargs): p.add_(-lr, grad_avg) return p sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step, weight_decay], stats=AverageGrad(), wd=0.01) . Aside: Python’s Wonderful kwargs . One of the features of python that makes this work is the wonderfully flexible way that python handles parameters and lists of keyword arguments. All the different stepper functions take a weight tensor plus some individual set of positional arguments. It would be complicated as hell trying to call a list of stepper functions with a list of which of all their positional arguments. However if you stick on a **kwargs to a stepper’s parameter list then it enables you to throw a dictionary of all the parameters name/value pairs to all the stepper functions, and when it comes time to call the stepper it will simply take what it needs from kwargs and ignore everything else! . This trivial example shows what I mean: . def foo(bar, lol, baz, **kwargs): print(bar, lol, baz) def boo(biz, **kwargs): print(biz) params = {&quot;lol&quot;: 2, &quot;baz&quot;: 3, &quot;biz&quot;: 5} foo(1, **params) boo(**params) ## This outputs: ## 1 2 3 ## 5 . params has all of the kwargs for all of the functions. Functions foo and boo only take what they need from params. The only thing you need to be careful of here is that you don’t have any stepper functions whose parameters share the same name, but are semantically different things. You could perhaps have a check on params to throw and exception if a key is overwritten to prevent this silent bug. . Weight Decay + Batch Norm: A Surprising Result . Jump to lesson 11 video . Weight decay scales the weights by a factor of (1-wd), however batch norm is invariant to weight scaling, so weight decay followed by batch norm effectively undoes the weight decay. . This was pointed out in the paper: L2 Regularization versus Batch and Weight Normalization. . Empirically, however, it has been found that weight decay and batch norm is actually anyway better than batch norm and no weight decay. This blog post explores this for vanillia SGD: . …without an L2 penalty or other constraint on weight scale, introducing batch norm will introduce a large decay in the effective learning rate over time. But an L2 penalty counters this. . This paper - Three Mechanisms of Weight Decay Regularization - identifies three different ways weight decay exerts a regularization effect, depending on the different optimization algorithm and architecture. . In reality, we don’t really know why weight decay works, but empirically it seems to help and basically all models use it. :-) . Momentum Experiments . Momentum is also interesting, and we really don’t understand it works. . Let’s create some fake data series of 200 normally distributed points and plot the moving average of this series with different beta values: [0.5, 0.7, 0.9, 0.99] . The regular momentum: . def mom1(avg, beta, yi, i): if avg is None: avg=yi res = beta*avg + yi return res . Here is a plot of the data (blue) and moving average (red): . . With very little momentum (small beta) it is very bumpy/highly variant. When you get up to larger values of momentum it shoots off and the new values its seeing can’t slow it down. So you have to be really careful when it comes to high momentum values. . . This is a rather naive implementation. We can fix it by instead using a Exponentially Weighted Moving Average (or EWMA, also called lerp in PyTorch): . def ewma(v1, v2, beta): return beta*v1 + (1-beta)*v2 def mom2(avg, beta, yi, i): if avg is None: avg=yi avg = ewma(avg, yi, beta) return avg . This helps to dampen the incoming data point which stops it being so bumpy for lower momentum values. . Plotting the same again: . . This works much better. So we’re done? - Not quite. . . What if the thing we are trying to match isn’t just random, but is some function like a polynomial. We’ve also added an outlier at the start. . def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2 def mom2(avg, beta, yi, i): if avg is None: avg=yi avg = lin_comb(avg, yi, beta) return avg . Let’s see how EWMA does here: . . The outlier at the start causes trouble with the higher momentum values. The first item is massively biasing the start. . . We need to do something called Debiasing (aka bias correction). We want to make sure that no observation is weighted too highly. Normal way of doing EWMA gives the first point far too much weight. These first points are all zero, so the running averages are all biased low. Add a correction factor dbias: $x_i = x_i/(1 - beta^{i+1})$. When $i$ is large this correction factor tends to 1 - it only pushes up the initial values. . def mom3(avg, beta, yi, i): if avg is None: avg=0 avg = lin_comb(avg, yi, beta) return avg/(1-beta**(i+1) . Plot that: . . This is pretty good. It debiases pretty well even if we have a bad starting point. You can see why beta=0.9 is a popular value. . . Adam Algorithm . Let’s use what we’ve learned to implement the optimizer Adam. The algorithm definition from the Adam paper (2014) is: . . If we look inside the while loop, and stare at the maths there is nothing in there we haven’t seen already. $g$ is the gradients of the weights, $m$ is the EWMA of the gradients, and $v$ is the EMWA of the square of the gradients. $m$ and $v$ are then debiased, as we have seen above. . Adam is just dampened debiased momentum divided by dampened debiased root sum of squared gradients. . To implement Adam we will need to implement the following: . EWMA of the gradients - a Stat subclass. | EWMA of the square of the gradients - a Stat subclass. | A debiasing function. This will need to know which step we are on. | A step counter - a Stat subclass | . class AverageGrad(Stat): _defaults = dict(mom=0.9) def __init__(self, dampening:bool=False): self.dampening = dampening def init_state(self, p): return {&#39;grad_avg&#39;: torch.zeros_like(p.grad.data)} def update(self, p, state, mom, **kwargs): state[&#39;mom_damp&#39;] = 1 - mom if self.dampening else 1. state[&#39;grad_avg&#39;].mul_(mom).add_(state[&#39;mom_damp&#39;], p.grad.data) return state class AverageSqrGrad(Stat): _defaults = dict(sqr_mom=0.99) def __init__(self, dampening:bool=False): self.dampening = dampening def init_state(self, p): return {&#39;sqr_avg&#39;: torch.zeros_like(p.grad.data)} def update(self, p, state, sqr_mom, **kwargs): state[&#39;sqr_damp&#39;] = 1 - sqr_mom if self.dampening else 1. state[&#39;sqr_avg&#39;].mul_(sqr_mom).addcmul_(state[&#39;sqr_damp&#39;], p.grad.data, sp.grad.data) return state class StepCount(Stat): def init_state(self, p): return {&#39;step&#39;: 0} def update(self, p, state, **kwargs): state[&#39;step&#39;] += 1 return state def debias_term(mom, damp, step): # if we don&#39;t use dampening (damp=1) we need to divide by 1-mom because # that term is missing everywhere return damp * (1 - mom**step) / (1-mom) . Adam as a stepper is now: . def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs): debias1 = debias_term(mom, mom_damp, step) debias2 = debias_term(sqr_mom, sqr_damp, step) p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps) adam_step._defaults = dict(eps=1e-5) def adam_opt(xtra_step=None, **kwargs): return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step), stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs) . Note that the weight decay and Adam step are totally decoupled. This is an implemention the AdamW algorithm, mentioned above in the weight decay subsection. First you decay the weights, then you do the Adam step. . . The epsilon eps in Adam is super important to think about. What if we set eps=1? Most of the time the gradients are going to be smaller than 1 and the squared gradients are going to be much smaller than 1. So eps=1 is going to be much bigger than (sqr_avg/debias2).sqrt(), so eps will dominate and the optimizer will be pretty close to being SGD with debiased-dampened momentum. . Whereas, if eps=1e-7 then we are really using the (sqr_avg/debias2).sqrt() term. If you have some activation that has had a very small squared gradients for a while, the value of this term could well be 1e-6. Dividing by this is equivalent to multiplying by a million, which would kill your optimizer. The trick getting Adam and friends working well is a value between eps=1e-3 and eps=1e-1. . Most people use 1e-7, which is equivalent to multiplying by 10 million. Here eps is basically just a small hack number put in to avoid a possible division by zero. We can instead treat eps as a kind of smoothing factor that enables the optimizer to behave more like momentum SGD sometimes and normal Adam at other times. . LAMB Algorithm . Jump to lesson 11 video . It’s then super easy to implement a new optimizer. This is LAMB from a very recent paper (2019): . [ begin{align} g_{t}^{l} &amp;= nabla L(w_{t-1}^{l}, x_{t}) m_{t}^{l} &amp;= beta_{1} m_{t-1}^{l} + (1- beta_{1}) g_{t}^{l} v_{t}^{l} &amp;= beta_{2} v_{t-1}^{l} + (1- beta_{2}) g_{t}^{l} odot g_{t}^{l} m_{t}^{l} &amp;= m_{t}^{l} / (1 - beta_{1}^{t}) v_{t}^{l} &amp;= v_{t}^{l} / (1 - beta_{2}^{t}) r_{1} &amp;= |w_{t-1}^{l}|{2} s{t}^{l} &amp;= frac{m_{t}^{l}}{ sqrt{v_{t}^{l}} + epsilon} + lambda w_{t-1}^{l} r_{2} &amp;= | s_{t}^{l} |{2} eta^{l} &amp;= eta * r{1}/r_{2} w_{t}^{l} &amp;= w_{t-1}^{l} - eta_{l} * s_{t}^{l} end{align}] . This is stuff we’ve seen before in Adam plus a few extras: . $m$ and $v$ are the debiased dampened momentum and the debiased dampened square of the gradients exactly like Adam. | $|w^l_{t-1}|_2$ is the layerwise l2-norm of the weights in layer $l$. | The learning rate $ eta^l$ is adapted individually for every layer. | It requires the same amount of state as Adam. | . As code: . def lamb_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, wd, **kwargs): debias1 = debias(mom, mom_damp, step) debias2 = debias(sqr_mom, sqr_damp, step) r1 = p.data.pow(2).mean().sqrt() # layerwise L2 norm step = (grad_avg/debias1)/((sqr_avg/debias2).sqrt()+eps) + wd*p.data r2 = step.pow(2).mean().sqrt() # layerwise L2 p.data.add_(-lr * min(r1/r2,10), step) return p lamb_step._defaults = dict(eps=1e-6, wd=0.) def lamb_opt(**kwargs): return partial(StatefulOptimizer, steppers=lamb_step, stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs) . The ratio r1/r2 is called the ‘trust ratio’ by the original authors. In most implementations this trust ratio’s upper value is clipped to make LAMB more stable. This upper bound is typically set to 10 or 100. . Data Augmentation . (Jump to lesson 11 video) . Up to this point we have created our datablocks API and optimizers and we have these running nicely together in a Learner class (which replaces the Runner class seen in prior lessons). With this we can train a reasonably good Imagenette model with a CNN (09b_learner.ipynb). But Imagenette is a bit short of data, so to make an even better model we should use data augmentation. . It’s important when doing data augmentation to look at or listen to or understand your augmented data to make sure the new data is of good enough quality and it representative of the original data. Don’t just chuck it into a model and hope for the best. Let’s look at an example where this can create problems with resizing images. . Resizing . Let’s load up some imagenette: . path = datasets.untar_data(datasets.URLs.IMAGENETTE) tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] def get_il(tfms): return ImageList.from_files(path, tfms=tfms) il = get_il(tfms) . Our transforms here are: . Convert image to RGB | Resize to 128x128 | Convert from Pillow object (bytes) to byte tensor | Convert to float tensor | Here is an image from the tench class with the ResizeFixed(128) transform: . . However, here is what the original looks like: . . Notice how the fish’s scale texture and the texture of the net is completely lost during the resizing. This resizing method may be chucking out useful textures that are key to identifying certain classes. Be careful of resampling methods, you can quickly lose some textures! . (Perhaps one could try making the resampling method used in a resizing random as a method of data augmentation?) . There are many resampling methods. Be critical about the resizing. Look at the augmented data and make sure that you aren’t losing key information like textures. Pillow has many different resizing methods. They recommend ANTIALIAS as a good default. Let’s look at the different resampling methods offered by Pillow: . ANTIALIAS BICUBIC . NEAREST | BICUBIC/NEAREST | . NEAREST is the only one that preserves the textures. There are a lot of aliasing artifacts however. The last one, BICUBLIC/NEAREST, does a resize to 256x256 with BICUBIC then another resize to 128x128 with NEAREST to achieve a pretty good compromise. . Topical: This recent tweet shows the difference between image resize methods in tensorflow and pytorch. “Something to check when porting and comparing models between frameworks” . Flipping, Rotating, Cropping . Flipping is a great data augmentation for vision. A very important point to make here is that doing image transforms on bytes is much faster than doing them on floats, because bytes are 4 times smaller than floats. If you are flipping an image, flipping bytes is identical to flipping floats in terms of the outcome. You should definitely everything you can while your image is still bytes (a Pillow object). However you should be careful when doing destructive transformations on bytes, because you can get rounding errors and saturation errors. Again - inspect the steps and take nothing for granted. . Flip: . class PilRandomFlip(PilTransform): def __init__(self, p=0.5): self.p=p def __call__(self, x): return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()&lt;self.p else x . It’s therefore really important to think when in your transformation pipeline you do certain image transforms. . We can easily extend this to doing the whole dihedral group of transformations (random horizontal flip, random vertical flip, and the four 90 degrees rotations) by passing a int between 0 and 6 to transpose: . class PilRandomDihedral(PilTransform): def __init__(self, p=0.75): self.p=p*7/8 #Little hack to get the 1/8 identity dihedral transform taken into account. def __call__(self, x): if random.random()&gt;self.p: return x return x.transpose(random.randint(0,6)) . We can also do random cropping. A great way to do data augmentation is to grab a small piece of an image and zoom into that piece. We can do this by randomly cropping and then down sizing the selection. . Naively we can do this with two steps in Pillow - crop and resize: . img.crop((60,60,320,320)).resize((128,128), resample=PIL.Image.BILINEAR) . However this degrades quality. You can instead do it all in one step with Pillow’s transform: . img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample) . This is an example of doing multiple destructive transformations when the image is still in bytes. Do them all in one go, if possible, or wait until they are floats. . RandomResizeCrop the usual data augmentation used on ImageNet (introduced here) that consists of selecting 8 to 100% of the image area and a scale between 3/4 and 4/3 as a crop, then resizing it to the desired size. It combines some zoom and a bit of squishing at a very low computational cost. . class RandomResizedCrop(GeneralCrop): def __init__(self, size, scale=(0.08,1.0), ratio=(3./4., 4./3.), resample=PIL.Image.BILINEAR): super().__init__(size, resample=resample) self.scale,self.ratio = scale,ratio def get_corners(self, w, h, wc, hc): area = w*h #Tries 10 times to get a proper crop inside the image. for attempt in range(10): area = random.uniform(*self.scale) * area ratio = math.exp(random.uniform(math.log(self.ratio[0]), math.log(self.ratio[1]))) new_w = int(round(math.sqrt(area * ratio))) new_h = int(round(math.sqrt(area / ratio))) if new_w &lt;= w and new_h &lt;= h: left = random.randint(0, w - new_w) top = random.randint(0, h - new_h) return (left, top, left + new_w, top + new_h) # Fallback to squish if w/h &lt; self.ratio[0]: size = (w, int(w/self.ratio[0])) elif w/h &gt; self.ratio[1]: size = (int(h*self.ratio[1]), h) else: size = (w, h) return ((w-size[0])//2, (h-size[1])//2, (w+size[0])//2, (h+size[1])//2) . . Jeremy says… . The most useful transformation by far shown in competition winners, is to grab a small piece of the image of the image and zoom into it. This is called a random resize crop. This is also really useful to know in any domain. For example, in NLP a really useful thing to do is a grab different sized chunks of contiguous text. With audio, if you are doing speech recognition, grab different sized pieces of the utterances. If you can find a way to get different slices of your data, it’s a fantastically useful data augmentation approach. So this is by far the most important augmentation in every imagenet winner in the last 6 years or so. . Perspective Transform . What RandomResizeCrop does, however, is it squishes the aspect ratio to some between 3:4 and 4:3. This can distort the image making objects expand outwards and inwards. Probably what they really want to do is something physically reasonable. If you are above or below something then your perspective changes. What would be even better is perspective warping. . To do perspective warping, we map the corners of the image to new points: for instance, if we want to tilt the image so that the top looks closer to us, the top/left corner needs to be shifted to the right and the top/right to the left. To avoid squishing, the bottom/left corner needs to be shifted to the left and the bottom/right corner to the right. . PIL can do this for us but it requires 8 coefficients we need to calculate. The math isn’t the most important here, as we’ve done it for you. We need to solve this system of linear equation. The equation solver is called torch.solve in PyTorch. . from torch import FloatTensor,LongTensor def find_coeffs(orig_pts, targ_pts): matrix = [] #The equations we&#39;ll need to solve. for p1, p2 in zip(targ_pts, orig_pts): matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]]) matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]]) A = FloatTensor(matrix) B = FloatTensor(orig_pts).view(8, 1) #The 8 scalars we seek are solution of AX = B return list(torch.solve(B,A)[0][:,0]) def warp(img, size, src_coords, resample=PIL.Image.BILINEAR): w,h = size targ_coords = ((0,0),(0,h),(w,h),(w,0)) c = find_coeffs(src_coords,targ_coords) res = img.transform(size, PIL.Image.PERSPECTIVE, list(c), resample=resample) return res . We can add a transform to do this perspective warping automatically with the rand resize and crop: . . Question: How do you handle tabular, text, time series etc? . Text - read it. Time series - look at the signal. Tabular you would just visualize the augmented data the same way you would. All augmentations are domain specific. You need to know your data and domain well to invent your own augmentations. Make sure it makes sense and seems reasonable. . Question: What happens if the object of interest gets cropped out by image augmentation? . These are called noisy labels. Interesingly, the ImageNet winning stratedy with crop/zooming is to randomly pick 8-100% of the pixels. They very often have no tench. Or very often they have just the fin or just the eye. If we want to used crop/zooming well, we need to be very good at handling noisy labels (more in the next lesson). Also this tells you that if you already have noisy labels - don’t worry about it. All of the research we have tells us that we can handle noisy labels as long as it’s not biased. . It will also learn to recognize all the things associated with a tench. So if there’s a middle aged many outside looking happy - it could well be a tench! :) . Batch Data Augmentation . It’s actually possible to arbitrary affine transformation of images (rotating, zooming, shifting, warping etc) on the GPU. PyTorch provides all the functionality to make this happen. All the transformations need to happen after we create a batch. The key is to do them on a whole batch at a time. Nearly all PyTorch operations can be done batch-wise. . To do this we create a mini-batch of random numbers to create a mini-batch of augmented images. . An affine transform is basically a linear transform plus a translation. They are represented by matrices and multiple affine transforms can be composed by multiplying all their matrices together. (See this Blog post.) . Let’s load an image. Its shape is torch.Size([1, 3, 128, 128]). . Once we have resized our images so that we can batch them together, we can apply more data augmentation on a batch level. For the affine/coord transforms, we proceed like this: . 1. Generate the Grid . A matrix is simply a function that takes a coordinate $(x, y)$ and maps them to some new location $(x’, y’)$. If we want to apply the same transformation to every pixel in an image, we first need to represent every pixel as a x,y coordinate. . Generate a grid map, using torch’s affine_grid, of the size of our batch (bs x height x width x 2) that contains the coordinates (-1 to 1) of a grid of size height x width (this will be the final size of the image, and doesn’t have to be the same as the current size in the batch). . def affine_grid(x, size): size = (size,size) if isinstance(size, int) else tuple(size) size = (x.size(0),x.size(1)) + size m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device) return F.affine_grid(m.expand(x.size(0), 2, 3), size, align_corners=True) grid = affine_grid(x, 128) . This has shape: torch.Size([1, 128, 128, 2]), and looks like: . tensor([[[[-1.0000, -1.0000], [-0.9843, -1.0000], [-0.9685, -1.0000], ..., [ 0.9685, -1.0000], [ 0.9843, -1.0000], [ 1.0000, -1.0000]], ..., [[-1.0000, 1.0000], [-0.9843, 1.0000], [-0.9685, 1.0000], ..., [ 0.9685, 1.0000], [ 0.9843, 1.0000], [ 1.0000, 1.0000]]]]) . Step 2: Affine Multiplication . Apply the affine transforms (which is a matrix multiplication) and the coord transforms to that grid map. . In 2D an affine transformation has the form y = Ax + b where A is a 2x2 matrix and b a vector with 2 coordinates. It’s usually represented by the 3x3 matrix . A[0,0] A[0,1] b[0] A[1,0] A[1,1] b[1] 0 0 1 . because then the composition of two affine transforms can be computed with the matrix product of their 3x3 representations. . The matrix for a anti-clockwise rotation that has an angle of theta is: . cos(theta) sin(theta) 0 -sin(theta) cos(theta) 0 0 0 1 . then we draw a different theta for each version of the image in the batch to return a batch of rotation matrices (size bsx3x3). . You then multiply all 3 channels by the rotation matrix and add the translation: . tfm_grid = (torch.bmm(grid.view(1, -1, 2), m[:, :2, :2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2) . Step 3: Interpolate . Interpolate the values of the final pixels we want from the initial images in the batch, according to the transformed grid map using Pytorch’s F.grid_sample function: . def rotate_batch(x, size, degrees): grid = affine_grid(x, size) thetas = x.new(x.size(0)).uniform_(-degrees,degrees) m = rotation_matrix(thetas) tfm_grid = (torch.bmm(grid.view(1, -1, 2), m[:, :2, :2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2) return F.grid_sample(x, tfm_grid, align_corners=True) . Here is also a faster version using F.affine_grid: . def rotate_batch(x, size, degrees): size = (size,size) if isinstance(size, int) else tuple(size) size = (x.size(0),x.size(1)) + size thetas = x.new(x.size(0)).uniform_(-degrees,degrees) m = rotation_matrix(thetas) grid = F.affine_grid(m[:,:2], size) return F.grid_sample(x.cuda(), grid, align_corners=True) . Results of this with random cropping and warping: . . We get these black borders around the image. But PyTorch grid_sample also has a padding_mode argument that lets you filling in this black space in different ways - &quot;zeros&quot;, &quot;border&quot;, or &quot;reflection&quot;. These can enrich and improve our augmented data even more. Here is reflection: . . Links and References . Lesson 11 video | Laniken’s Lesson 11 Notes | Notebooks: 07a_lsuv.ipynb | 08_data_block.ipynb | 09_optimizers.ipynb | 09c_add_progress_bar.ipynb | 10_augmentation.ipynb | . | Papers: LSUV Paper: All You Need is a Good Init | L2 Regularization versus Batch and Weight Normalization [2017] | Three mechanisms of weight decay regularization [2019] | Norm matters: Efficient and accurate normalization schemes in deep networks [2018] | Jane Street Tech Blog - L2 Regularization and Batch Norm [2019] | Adam Paper - Adam: A method for stochastic optimization [2015] | Nesterov’s accelerated gradient and momentum as approximations to regularised update descent [2017] | LAMB Paper - Large Batch Optimization for Deep Learning: Training BERT in 76 minutes [2019] | LARS Paper - Large Batch Training of Convolutional Networks [2017] (LARS also uses weight statistics, not just gradient statistics.) | Adafactor: Adaptive Learning Rates with Sublinear Memory Cost [2018] (Adafactor combines stats over multiple sets of axes) | Adaptive Gradient Methods with Dynamic Bound of Learning Rate [2019] | . | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fast.ai v3 Lesson 10 Notes: Looking inside the model",
            "content": "Overview . This lesson covers a lot of material. It starts off with a review of some important foundations such as more advanced Python programming, variance, covariance, and standard deviation. It then goes into a short discussion on situation where Softmax loss is a bad idea in image classification tasks. My notes go deeper into this part on Multilabel classification than the original lecture does. The lesson then moves onto looking inside the model using PyTorch hooks. The last part of the lesson introduces Batch Normalization and studies the pros and cons of BatchNorm and shows some alternatives normalizations that are possible. Jeremy then develops a novel kind of normalization layer to overcome BatchNorm’s main problem, and compares it to previously published approaches, with some very encouraging results. . Lesson 10 lesson video. . Foundations . Notebook: 05a_foundations.ipynb . Mean: m = t.mean() . Variance: The average of how far away each data point is from the mean. Mean squared difference from the mean. Sensitive to outliers. . var = (t-m).pow(2).mean() | Better: var = (t*t).mean() - (m*m) | $ mbox{E}[X^2] - mbox{E}[X]^2$ | . Standard Deviation: Square root of the variance. . On same scale as the original data - easier to interpret. . | std = var.sqrt() . | . Mean Absolute Deviation: Mean absolute difference from the mean. It isn’t used nearly as much as it deserves to be. Less sensitive to outliers than variance. . (t-m).abs().mean() | . Covariance: A measure of how changes in one variable are associated with changes in a second variable. How linearly associated are two variables? . cov = ((t - t.mean()) * (v - v.mean())).mean() | $ operatorname{cov}(X,Y) = operatorname{E}{ big[(X - operatorname{E}[X])(Y - operatorname{E}[Y]) big]} = operatorname{E}[XY] - operatorname{E}[X] operatorname{E}[Y]$ | cov = (t*v).mean() - t.mean()*v.mean() | . Correlation: The strength and direction of the linear relationship between two variables. . Covariance divided by the standard deviations of X and Y. | $ rho_{X,Y}= frac{ operatorname{cov}(X,Y)}{ sigma_X sigma_Y}$ | cor = cov / (t.std() * v.std()) | . See this: 3 minute video on Correlation vs Covariance. . MultiLabel Classification (When Softmax is a Bad Idea) . Jump_to lesson 10 video . A common mistake many people make is using a Softmax where it isn’t appropriate. Recall the Softmax formula: . [p(z_i) = hbox{softmax(z)}{i} = frac{e^{z{i}}}{ sum_{0 leq j leq n-1} e^{z_{j}}}] . In the Excel screenshot below, two different network outputs can produce the same Softmax output. This is weird, how does it happen? . . The sums of the exponentials for the two images (12.70 and 3.00) are dividing each of the exponentials and it happens that they come out with the exact same proportions for each category for both images. . In Image 1 there is a large activation for category “fish” (2.07), but in image 2 the activation for “fish” is only 0.63. Image 1 likely contains a fish, but it’s possible that image 2 doesn’t contain any of the categories. Softmax has to pick something however so it takes the weak fish activation and makes it big. It’s also possible that image 1 contains a cat, fish, and a building. . Put another way: many images are in fact multilabel, so Softmax is often a dumb idea, unless every one of your items has definitely at least one example of the thing you care about in it, and no items that have multiple examples in it. If an image doesn’t even have cat, dog, plane, fish, or building in it, it still has to pick something! Even if it has more than just one of the categories in it, it will have to pick one of them. . (N.B multiclass means one valid label per image, while multilabel means multiple labels per image. I always confuse these. Read this for a refresher.) . What do you do if there could there could be no things, or there could be more than one of these things? Instead you use a binary function where the output for each category in is: . [B(z_i) = frac{e^{z_i}}{1+e^{z_i}}] . This treats every category independently. The network assigns each category with a probability between 0 and 1, corresponding to how likely it thinks the category is present in the input data. (Note: the binary function is AKA the sigmoid function or logistic function). . The output of a binary function with the same example would look like: . . See how each category gets its own probability and is independent from all the others. . For image recognition, probably most of the time you don’t want Softmax. This habit comes from the fact that we all grew up with the luxury of ImageNet where the images are curated so that there is only one of each class in the image. . What if you instead added an additional category like “null”, “doesn’t exist”, “background”? This has been tried by researchers, but they found that it doesn’t work. The reason it doesn’t work is that we’d have to look for some set of features that correspond to “not cat/dog/plane/fish/building”. However this class of all things that are not something isn’t a kind of object so there isn’t some vector that can represent this. . Creating a binary has/has-not for each class is much easier for the network to learn. According to Jeremy: lots of well regarded papers make this mistake, so look out for it. If you suspect something does this, try replicating it without Softmax and you may just get a better result. . Example where Softmax is a good idea: language modelling -&gt; predict the next word. There can be only one word next. . MultiLabel Predictions . Now that we understand the concept, what would this look like in code and how would we modify the loss function with the binary output layer? . Let’s first reproduce what Jeremy did in the Excel sheet in Python: . . Where the logistic function is what Jeremy calls ‘binary’ in his lecture. . How do we interpret the outputs of softmax and logistic to get predictions? For Softmax layer the predicted label is the label with the highest output value. In code this is simply: . . For the logistic output we need to threshold the values to filter in only the largest outputs. This threshold is user defined; 0.2 is used in fastai lesson 3 so let’s just go with that. Code: . . MultiLabel Loss Function . What about the loss function for a logistic output? Recall from the last lesson that Softmax outputs a categorical probability distribution. With the numbers from the example above this is: . . All the probabilities in a categorical distribution sum to 1 (I denote this property with the blue colour). Recall also from last lesson that the loss function used for a categorical distribution is the cross-entropy. . On the other hand, when we use the Binary/Logistic function the output isn’t a categorical distribution: . . The probabilities in this vector don’t all sum to 1 (denoted with red) because they are all independent of each other. These probabilities are each the probability that the label is present in the data, independent of all the other labels. If we take 1 minus these probabilities we’d get the probability of the label not being present in the data. We can think of each of these as a 2-state system of present / not present and expand the vector out to include the not present probability: . . Now we can see that each of the rows is itself a categorical distribution with two categories (AKA Bernoulli distribution). Therefore to get the loss we can individually apply the cross-entropy loss to each of these distributions using target data (binary vector of present / not present for each label), then take the average of them all. You do that for every sample in the batch and then take the averages of all those averages to get the loss for the batch. . We don’t have to literally expand the vector out in practice, and can instead create a special case of the cross-entropy for this binary case, binary_cross_entropy: . def binary_cross_entropy(pred, targ): return -targ * pred.log() - (1 - targ) * (1 - pred).log() . The loss would be: . def multilabel_loss(out, targ): return binary_cross_entropy(logistic(out), targ).mean(1).mean(0) . Example use: . &gt;&gt;&gt;out = torch.tensor([[0.02, -2.49, -1.75, 2.07, 1.25], [-1.42, -3.93, -3.19, 0.63, -0.19]]) &gt;&gt;&gt;targ = torch.tensor([[0., 0., 0., 1., 1.], [1., 0., 0., 1., 1.]]) &gt;&gt;&gt;multilabel_loss(out, targ) tensor(0.4230) . This is a naive implementation of the loss, but it shows how it works. For a production implementation we need it to be more numerically stable (as discussed in last lesson) and do it all in log-space. We put the logistic function in log-space and then simplify things by fusing that with binary_cross_entropy. You can derive that the binary cross entropy with logistic function simplifies to: . [l(x, y) = -yx + log(1 + e^x)] . Careful with the $e^x$, however, because it will overflow when $x$ isn’t even that large. To make things more numerically stable we employ the logsumexp trick again: . [l(x, y) = m - yx + log(e^{-m} + e^{x - m})] . Where $m = max(x, 0)$. As code, this is: . def binary_cross_entropy_with_logits(out, targ): max_val = out.clamp_min(0.) return max_val - out * targ + ((-max_val).exp() + (out - max_val).exp()).log() . The loss function is modified to: . def multilabel_loss(out, targ): return binary_cross_entropy_with_logits(out, targ).mean(1).mean(0) . We’ve now recreated the loss function BCEWithLogitsLoss from PyTorch, which we can now use. Test with same example: . &gt;&gt;&gt;out = torch.tensor([[0.02, -2.49, -1.75, 2.07, 1.25], [-1.42, -3.93, -3.19, 0.63, -0.19]]) &gt;&gt;&gt;targ = torch.tensor([[0., 0., 0., 1., 1.], [1., 0., 0., 1., 1.]]) &gt;&gt;&gt;loss = torch.nn.BCEWithLogitsLoss() &gt;&gt;&gt;loss(out, targ) tensor(0.4230) . (Implementation in PyTorch (C++): binary_cross_entropy_with_logits) . Build a Learning Rate Finder . Notebook: 05b_early_stopping.ipynb, Jump to lesson 10 video . Better Callback Cancellation . In the last implementation of the Callback and Runner classes, stopping the training prematurely (e.g. for early stopping) was handled by callbacks returning booleans or by a attribute called stop getting set and checked at some point. This is a bit inflexible and also not very readable. . We can instead make use of Exceptions as a kind of control flow technique rather than just an error handling technique. You can subclass Exception to give it your own informative name without even changing its behaviour, like so: . class CancelTrainException(Exception): pass class CancelEpochException(Exception): pass class CancelBatchException(Exception): pass . Callbacks are free to raise Exceptions. The training loop can catch these and change control. This is a super neat and readable way that someone writing a callback can stop any of the three levels in the training loop from happening. . Refactoring Callback and Runner . Refactor/redesign the Callback and Runner class from last time. The Callback class now contains the ‘message passing’ (e.g. self(&#39;begin_fit&#39;) ) logic from before. This means that callback writers can now have control to override __call__ themselves in special cases, for debugging etc. . Here’s what the base class looks like now, alongside the default Training/Validation callback which holds the logic for the training or validating parts of the loop: . class Callback(): _order, run = 0, None def set_runner(self, run): self.run=run def __getattr__(self, k): ## Get attributes from Runner object return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) ## Refactored from before def __call__(self, cb_name): f = getattr(self, cb_name, None) if f and f(): return True return False ## DEFAULT Callback for Training/Validation class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False . Notice how Callback and its subclasses can access attributes in Runner (set in the set_runner method) and even the getattr in Callback is overloaded to instead look in the Runner. . The __getattr__ overloading confused me for a while, until I realised how it actually works. Quote from this Stackoverflow question: . __getattr__ is only called as a last resort i.e. if there are no attributes in the instance that match the name. For instance, if you access foo.bar, then __getattr__ will only be called if foo has no attribute called bar. If the attribute is one you don’t want to handle, raise AttributeError . Python looks for the attribute in the Callback first, if it can’t find it then it looks in the attributes of Runner. . This kind of strong coupling / encapsulation breaking made me a bit nervous initially, but after thinking about it more I think its a special design that works well in this unique setting. Runner and Callback are kind of like ‘friend classes’ from C++, where two friend classes ‘share’ their attributes with each other, but are still separate classes. By doing it this way, callback writers can gain privileged access to internals of the training loop, and so can inject code into the loop as if they were directly editing the source code of Runner. . Here is a skeleton of the code for Runner: . class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.cbs = [TrainEvalCallback()] + cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): try: ## INNER LOOP CODE except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self, dl): try: ## EPOCH CODE except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def fit(self, epochs, learn): self.epochs, self.learn, self.loss = epochs, learn, tensor(0.) try: for cb in self.cbs: cb.set_runner(self) self(&#39;begin_fit&#39;) for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): # TRAIN with torch.no_grad(): if not self(&#39;begin_validate&#39;): # VALIDATE self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.learn = None def __call__(self, cb_name): res = False for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res return res . I removed all the business code from the snippet, to save space and also so it could be implemented as an exercise. . LR_Find Callback . The learning rate finder is the work horse from part 1 of the fastai course. Let’s look at how to implement it and code that up as a callback. . . LR_Find Algorithm Outline: . Define upper and lower bounds for the learning rate and a number of steps. Lower should be small like 1e-10 and the upper should be very layer like 1e+2. Numbers of steps should be something like 100. | Start training the network with a learning rate starting at the lower bound. | After every batch update, exponentially increase the learning rate and record the loss. | If the learning rate hits the upper bound, or the loss ‘explodes’ then stop the process. | After the finder has finished, plot the loss versus learning rate so we can eyeball the best learning rate. | To exponentially increase the learning rate using the formula: . [lr_i = lr_{min} left( frac{lr_{max}}{lr_{min}} right)^{i/i_{max}}] . ‘Exploding’ loss can be defined as some factor (e.g. 10) times the lowest loss value recorded. . The code for the LR_Find callback is: . class LR_Find(Callback): _order=1 def __init__(self, max_iter=100, min_lr=1e-6, max_lr=1): self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr self.best_loss = 1e9 def begin_batch(self): if not self.in_train: return pos = self.n_iter / self.max_iter lr = self.min_lr * (self.max_lr / self.min_lr) ** pos for pg in self.opt.param_groups: pg[&#39;lr&#39;] = lr def after_step(self): if self.n_iter &gt;= self.max_iter or self.loss &gt; self.best_loss*10: raise CancelTrainException() if self.loss &lt; self.best_loss: self.best_loss = self.loss . Plot of loss versus learning rate: . . This PyImageSearch blog post is an excellent resource for learning more about LR Find and also uses exponential smoothing in the loss recordings too. . Build a CNN (with Cuda!) . Notebook: 06_cuda_cnn_hooks_init.ipynb, Jump to lesson 10 video . Let’s build a CNN for doing the MNIST problem using PyTorch and CUDA. Our simple CNN is a sequential model that contains a bunch of stride-2 convolutions, an average pooling, flatten, then a linear layer. . def get_cnn_model(data): return nn.Sequential( Lambda(mnist_resize), # ni,nf,ksize nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), # 8x14x14 nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 16x7x7 nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 32x4x4 nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 32x2x2 nn.AdaptiveAvgPool2d(1), # 32x1 Lambda(flatten), # 32 nn.Linear(32,data.c) # 10 ) . The dimensions of the data as it flows through the model are provided in the comments. AdaptiveAvgPooling downsamples the data using an average. . See: What is adapative average pooling? | Also see: How Convolutions Work: A Mini-Review | . Original data is vectors of 784 so they need to be reshaped to 28x28 to go into the convolution layers. We need to write a function mnist_resize to do this: . def mnist_resize(x): # batchsize, num_channels, height, width return x.view(-1, 1, 28, 28) . In order to turn helper functions into ‘layers’ that we can pass into nn.Sequential, we can create simple wrapper layer class Lambda(nn.Module) that takes this function and calls it in its forward method. This is used in the code above for calling mnist_resize and flatten. . Training this for one epoch on my laptop CPU took 7.14 seconds. . We need to speed this up using the GPU! To get started we need to prepare PyTorch to use the GPU. First check that Cuda is available to use with torch.cuda.is_available(), which should return True. Then set the device in PyTorch: . device = torch.device(&#39;cuda&#39;, 0) # NB assumes only 1 GPU torch.cuda.set_device(device) . To run on the GPU we need to do two things: . Put the model on the GPU, i.e. the model’s parameters. | Put the inputs and the loss function on the GPU, i.e. the things that come out of the dataloaders. | We can implement this with a callback: . class CudaCallback(Callback): def begin_fit(self): self.model.cuda() def begin_batch(self): self.run.xb, self.run.yb = self.xb.cuda(), self.yb.cuda() . At the beginning of the fit, put the model on the GPU. Before each batch starts, put the batch data on the GPU. . Adding this in training for 3 epochs took 7.12 s on my laptop - a nice 3x speedup. :) . Some Refactoring . First we can regroup all the conv/ReLU in a single function because they are always called together. . Next to refactor is the batch resizing for MNIST. This is hardcoded in the model, but we need something more general that could be used on other datasets. Of course this can be implemented as a callback! Make a callback BatchTransformXCallback for doing ‘transformations’ to the data before it goes into the model. Resize is one such possible transformation. . class BatchTransformXCallback(Callback): _order = 2 def __init__(self, tfm): self.tfm = tfm # stores a transform def begin_batch(self): self.run.xb = self.tfm(self.xb) # transform the batch . So we have a resize or view transform to perform for each batch: . def view_tfm(*size): def _inner(x): return x.view(*((-1,)+size)) return _inner mnist_view = view_tfm(1,28,28) cbfs.append(partial(BatchTransformXCallback, mnist_view)) . Discussion on CNN Kernel Sizes . (Jump_to lesson 10 video) . First conv layer on imagenet networks typically have 7x7 or 5x5 size kernels, while the rest of the conv layers use 3x3 kernels. Why is that? . If we just focus on MNIST, the first layer of the MNIST-CNN we only have a single channel image. We need to be mindful of what’s going on when we apply a kernel to this. If we have 8 3x3 filters then for a single point in the image we are converting 9 pixels (from 3x3 kernel) into a vector of 8 numbers (from 8 filters). We aren’t gaining anything from that, it’s basically shuffling the numbers around. For the first conv layer when we just have 1 or 3 channels people use a larger kernel size such as 7x7 or 5x5 in order to capture more information. . 8 3x3 filters 1 channel =&gt; 9 -&gt; 8 | 8 3x3 filters 3 channels =&gt; 27 -&gt; 8 | 8 5x5 filters 1 channel =&gt; 25 -&gt; 8 | 8 5x5 filters 3 channels =&gt; 75 -&gt; 8 | 8 7x7 filters 1 channel =&gt; 49 -&gt; 8 | 8 7X7 filters 3 channels =&gt; 147 -&gt; 8 | . Later conv layers have more ‘channels’ so that isn’t an issue anymore. The deeper layers are typically 3x3. . Here are some useful discussions on this part of the lecture that helped me grok what Jeremy meant here: fastai forum, twitter. . Looking Inside the Model . Jump_to lesson 10 video . We want to look inside of the model while it is training and see how the parameters are changing over time. Are they behaving themselves? Are they actually learning anything? Are there vanishing or exploding gradients? . PyTorch Hooks . Hooks are PyTorch’s version of callbacks, which are called inside of the model, and can be added, or registered, to any nn.Module. Hooks allow you to inject a function into the model that that is executed in either the forward pass (forward hook) or backward pass (backward hook). With hooks you can inspect / modify the output and grad of a layer. The hook can be a forward hook or a backward book. . A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list. . class Hook(): def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self)) def remove(self): self.hook.remove() def __del__(self): self.remove() def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[],[]) means,stds,hists = hook.stats means.append(outp.data.mean().cpu()) stds .append(outp.data.std().cpu()) hists.append(outp.data.cpu().histc(40,0,10)) #histc isn&#39;t implemented on the GPU . It’s very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won’t be properly released when your model is deleted. . Hooks class that contains several hooks: . class Hooks(ListContainer): def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms]) def __enter__(self, *args): return self def __exit__ (self, *args): self.remove() def __del__(self): self.remove() def __delitem__(self, i): self[i].remove() super().__delitem__(i) def remove(self): for h in self: h.remove() . Having given an __enter__ and __exit__ method to our Hooks class, we can use it as a context manager. This makes sure that onces we are out of the with block, all the hooks have been removed and aren’t there to pollute our memory. . Current State of Affairs . Use the append_stats hook to look at the mean and std of the parameters in each of the layers. . The layer means: . . This looks awful. At the beginning of the training the values increase exponentially and then suddenly crash, repeatedly. It’s not training anything when this is happening. Eventually they settle down into some range and start to train. However are we sure that all the parameters are getting back to reasonable places after these ‘crashes’? Maybe the vast majority of them have zero gradients or are zero. Likely that this awful behaviour at the start of training is leaving the model in a really sad state. . The layer standard deviations: . . Subsequent layers standard deviations get closer and closer to 0. Later layers are basically getting 0 gradient. . Better Initialization . Use Kaiming init: . for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() . Here are the layer means and stds now: . . This is looking a lot better. No longer has the repeated exponential-crash pattern anymore. The standard deviations are all much closer to 1. . However these values are just aggregates of the layer parameters, so they don’t give us the full picture about how all the parameters are behaving. Rather than look at a single number we’d like to look at the distribution. To do that we can look at how the histogram of the parameters changes over time. . Here is a histogram of the activations, binned between 0 (relu) and 10 with 40 bins: . . What we find is that even with Kaiming init, with the high learning rate we still get the same exponential-crash behaviour. The biggest concern is the amount of mass at the bottom of the histogram at 0. . Here is a plot of the percentage of activations that are 0 or nearly 0: . . This is not good. In the last layer nearly 90% of the activations are actually 0. If you were training your model like this, it could appear like it was learning something, but you could be leaving a lot of performance on the table by wasting 90% of your activations. . Generalized ReLU . Let’s try to fix this so we can train a nice high learning rate and not have this happen. The main thing we will use to fix this is a GeneralRelu layer, where you can specify: . An amount to subtract from the ReLU. (In earlier lesson it seemed that subtracting 0.5 from the ReLU might be a good idea.) | Use leaky ReLU. | Also the option of a maximum value. | . Code for that: . class GeneralRelu(nn.Module): def __init__(self, leak=None, sub=None, maxv=None): super().__init__() self.leak,self.sub,self.maxv = leak,sub,maxv def forward(self, x): x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) if self.sub is not None: x.sub_(self.sub) if self.maxv is not None: x.clamp_max_(self.maxv) return x . Retrain just like before with Kaiming init, and a GeneralRelu with parameters: . leak=0.1 | sub=0.4 | maxv=6.0 | . The layer means and standard deviations over time: . . Looking better than before - means are around 0 and the stds are around 1 and are also a lot smoother looking. . Plot the histogram of the activations again, this time from -7 to 7 (leaky relu): . . This is way better! It’s using the full richness of the possible activations. There’s not crashing of values. . How many of the activations are at or around zero: . . The majority of the activations are not zero. . If we are careful about initialization, the ReLU, use one-cycle training, and a nice high learning rate of 0.9 we can achieve 98%-99% validation set accuracy after 8 epochs. . Normalization . Notebook: 07_batchnorm.ipynb . Batch Norm . Jump_to lesson 10 video . Up to this point we have learned how to initialize the values to get better results. To get even better results we need to use normalization. The most common form of normalization is Batch Normalization. This was covered in Lesson 6, but here we implement it from scratch. . Algorithm from the BatchNorm paper: . . It normalizes the batch and scales and shifts it by $ gamma$ and $ beta$, which are learnable parameters in the model. . Here is that as code: . class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() # NB: pytorch bn mom is opposite of what you&#39;d expect self.mom, self.eps = mom, eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;vars&#39;, torch.ones(1,nf,1,1)) self.register_buffer(&#39;means&#39;, torch.zeros(1,nf,1,1)) def update_stats(self, x): # x has dims (nb, nf, h, w) m = x.mean((0,2,3), keepdim=True) v = x.var ((0,2,3), keepdim=True) self.means.lerp_(m, self.mom) self.vars.lerp_ (v, self.mom) return m,v def forward(self, x): if self.training: with torch.no_grad(): m,v = self.update_stats(x) else: m,v = self.means,self.vars x = (x-m) / (v+self.eps).sqrt() return x*self.mults + self.adds . Let’s understand what this code is doing: . Instead of $ gamma$ and $ beta$, use descriptive names - mults and adds. There is a mult and an add for each filter coming into the BatchNorm. These are initialized to 1 and 0, respectively. . | At training time, it normalizes the batch data using the mean and variance of the batch. The mean calculation is: x.mean((0,2,3), ...). The dimensions of x are (nb, nf, h, w). So (0,2,3) tells it to take the mean over the batches, heights and widths, leaving nf numbers. Same thing with the variance. . | However, at inference time every batch needs to be normalized with the same means and variances. If we didn’t do this, then if we get a totally different kind of image then it would remove all the things that are interesting about it. . | While we are training, we keep an exponentially weighted moving average of the means and variances. The lerp_ method updates the moving average. These averages are what are used at inference time. . | These averages are stored in special way using: self.register_buffer. This comes from nn.Module. It works the same as a normal PyTorch tensor, except it moves the values to the GPU when the model is moved there. Also, we need to store these values the same way we store other parameters. This will save the numbers when the model is saved. We need to do this when we have ‘helper variables’ in a layer that aren’t parameters of the model. . | Another thing to note: if you use BatchNorm then the layer before doesn’t need to have a bias because BatchNorm has a bias already. . | . Exponentially Weighted Moving Average (EWMA) . The EWMA is a moving average that gives most weighting to recent values and exponentially decaying weight to older values. It allows you to keep a running average that is robust to outliers and requires that we keep track of only one number. The formula is: . [ mu_t = alpha x_t + (1 - alpha) mu_{t-1}] . Where $ alpha$ is called the momentum, which represents the degree of weight decrease. A higher value discounts older observations faster. . In PyTorch, EWMA is called ‘linear interpolation’ and uses the function means.lerp_(m, mom). In PyTorch the momentum in both lerp and in PyTorch’s BatchNorm uses opposite convention from everyone else, so you have to subtract value from 1 before you pass it. The default momentum in our code is 0.1. . (6 minute video with more info on EWMA) . Results . Training on MNIST with CNN, Kaiming init, BatchNorm, 1 epoch: . . Working well. Means are all around 0 and the variances are all around 1. . BatchNorm Deficiencies . BatchNorm works great in most places, but it can’t be applied to online learning tasks, where we learn after every item. The problem is that the variance of one data point is infinite. You could also get the same problem if a single batch of any size contained all the same values. BatchNorm doesn’t work well for small batch sizes (like 2). This prohibits people from exploring higher-capacity models that would be limited by memory. It also can’t be used with RNNs. . tl;dr We can’t use BatchNorm with small batchsizes or with RNNs. . Layer Norm . Jump_to lesson 10 video . LayerNorm is just like BatchNorm except instead of averaging over (0,2,3) we average over (1,2,3), and this doesn’t use the running average. Used in RNNs. It is not even nearly as good as BatchNorm, but for RNNs it is something we want to use because we can’t use BatchNorm. . From the LayerNorm paper: “batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small”. . The difference with BatchNorm is: . It doesn’t keep a moving average. | It doesn’t average over the batches dimension, but over the hidden/channel dimension, so it’s independent of the batch size. | Code: . class LayerNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, eps=1e-5): super().__init__() self.eps = eps self.mult = nn.Parameter(tensor(1.)) self.add = nn.Parameter(tensor(0.)) def forward(self, x): m = x.mean((1,2,3), keepdim=True) v = x.var ((1,2,3), keepdim=True) x = (x-m) / ((v+self.eps).sqrt()) return x*self.mult + self.add . Thought experiment: can this distinguish foggy days from sunny days (assuming you’re using it before the first conv)? . Foggy days are less bright and have less contrast (lower variance). | LayerNorm would normalize the foggy and sunny days to have the same mean and variance. | Answer: no you couldn’t. | . Instance Norm . Jump_to lesson 10 video . Instance Norm paper . The problem with LayerNorm is that it combines all channels into one. Instance Norm is a better version of LayerNorm where channels aren’t combined together. The key difference between instance and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones. . Code: . class InstanceNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, nf, eps=1e-0): super().__init__() self.eps = eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) def forward(self, x): m = x.mean((2,3), keepdim=True) v = x.var ((2,3), keepdim=True) res = (x-m) / ((v+self.eps).sqrt()) return res*self.mults + self.adds . Used for Style transfer, not for classification. It’s included here as another example of normalization. You need to understand what it is doing in available to understand is it something that might work. . Group Norm . Jump_to lesson 10 video . The Group Norm paper proposes a layer that divides channels into groups and normalizes the features within each group. GroupNorm is independent of batch sizes and it does not exploit the batch dimension, like how BatchNorm does. GroupNorm stays stable over a wide range of batch sizes. GroupNorm is supposed to solve the problem of BatchNorm with small batches. . It gets close to BatchNorm performance for ‘normal’ batch sizes in image classification, and beats BatchNorm with smaller batch sizes. GroupNorm works very well in large memory tasks such as: object detection, segmentation, and high resolution images. . It isn’t implemented in the lecture, but PyTorch has it already: . GroupNorm(num_groups, num_channels, eps=1e-5, affine=True) &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10) &gt;&gt;&gt; # Separate 6 channels into 3 groups &gt;&gt;&gt; m = nn.GroupNorm(3, 6) &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm) &gt;&gt;&gt; m = nn.GroupNorm(6, 6) &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm) &gt;&gt;&gt; m = nn.GroupNorm(1, 6) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input) . (See this blog post for more details. This blog post covers even more kinds of initialization.) . Summary of the Norms with One Picture . . (Source) . In this diagram the height and width dimensions are flattened to 1D, so a single image is a ‘column’ in this diagram. . Running Batch Norm: Fixing Small Batch Size Problem . Jump_to lesson 10 video . The normalizations above are attempts to work around the problem that you can’t use small batch sizes or RNNs with BatchNorm. But none of them are as good as BatchNorm. . Here Jeremy proposes a novel solution to solve the batch size problem, but not the RNN problem. This algorithm is called Running BatchNorm. . Algorithm idea: . In the forward function, don’t divide by the batch standard deviation or subtract the batch mean, but instead use the moving average statistics at training time as well, not just at inference time. | Why does this help? Let’s say you have batch size of 2. Then from time to time you may get a batch where the items are very similar and the variance is very close to 0. But that’s fine, because you are only taking 0.1 of that, and 0.9 of what you had before. | Code: . class RunningBatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;sums&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;sqrs&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;batch&#39;, tensor(0.)) self.register_buffer(&#39;count&#39;, tensor(0.)) self.register_buffer(&#39;step&#39;, tensor(0.)) self.register_buffer(&#39;dbias&#39;, tensor(0.)) def update_stats(self, x): bs,nc,*_ = x.shape self.sums.detach_() self.sqrs.detach_() dims = (0,2,3) s = x.sum(dims, keepdim=True) ss = (x*x).sum(dims, keepdim=True) c = self.count.new_tensor(x.numel()/nc) mom1 = 1 - (1-self.mom)/math.sqrt(bs-1) self.mom1 = self.dbias.new_tensor(mom1) self.sums.lerp_(s, self.mom1) self.sqrs.lerp_(ss, self.mom1) self.count.lerp_(c, self.mom1) self.dbias = self.dbias*(1-self.mom1) + self.mom1 self.batch += bs self.step += 1 def forward(self, x): if self.training: self.update_stats(x) sums = self.sums sqrs = self.sqrs c = self.count if self.step&lt;100: sums = sums / self.dbias sqrs = sqrs / self.dbias c = c / self.dbias means = sums/c vars = (sqrs/c).sub_(means*means) if bool(self.batch &lt; 20): vars.clamp_min_(0.01) x = (x-means).div_((vars.add_(self.eps)).sqrt()) return x.mul_(self.mults).add_(self.adds) . Let’s work through this code. . In normal BatchNorm we take the running average of the variance, but this doesn’t make sense - you can’t just average a bunch of variances. Particularly if the batch size isn’t constant. The way we want to calculate the variance is like this: | . [ mbox{E}[X^2] - mbox{E}[X]^2] . Let’s instead keep track of the sums sums and the sums of the squares sqrs, that store the EWMA of them. From the above formula - to get the means and variances we need to divide them by the count (running average of H*W*BS), which we also store as an EWMA. This accounts for the possibility of different batch sizes. . | We need to do something called Debiasing (aka bias correction). We want to make sure that no observation is weighted too highly. Normal way of doing EWMA gives the first point far too much weight. These first points are all zero, so the running averages are all biased low. Add a correction factor dbias: $x_i = x_i/(1 - alpha^i)$. When $i$ is large this correction factor tends to 1 - it only pushes up the initial values. (See this post). . | Lastly, to avoid the unlucky case of having a first mini-batch where the variance is close to zero, we clamp the variance to 0.01 for the first 20 batches. . | . Results . With a batchsize of 2 and learning rate of 0.4, it totally nails it with just 1 epoch: . . Links and References . Lesson 10 lesson video. | Lesson 10 notebooks: 05a_foundations.ipynb, 05b_early_stopping.ipynb, 06_cuda_cnn_hooks_init.ipynb, 07_batchnorm.ipynb. . | Laniken Lesson 10 notes: https://medium.com/@lankinen/fast-ai-lesson-10-notes-part-2-v3-aa733216b70d | Interpreting the colorful histograms used in this lesson | Lecture on Bag-of-tricks for CNNs. Loads of state-of-the-art tricks for training CNNs for image problems, which would be a great exercise to reimplement as callbacks. | Papers to read: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | Layer Normalization | Instance Normalization: The Missing Ingredient for Fast Stylization | Group Normalization | Revisiting Small Batch Training for Deep Neural Networks | . | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/03/24/fast-ai-lesson-10-notes-looking-inside-the-model.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/03/24/fast-ai-lesson-10-notes-looking-inside-the-model.html",
            "date": " • Mar 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jimypbr.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fast.ai v3 Lesson 9 Notes: Training in Depth",
            "content": "Overview . This lesson continues with the development of the MNIST model from the last lesson. It introduces and implements a Cross-entropy loss for MNIST, then takes a deep dive refactoring the model and the training loop, where it builds the equivalent classes from PyTorch from scratch, which provides a great foundation for understanding the main PyTorch classes. In the second half, the lesson moves onto the implementation of Callbacks and how they are integrated into the training loop in the FastAI library. Then it shows how to implement one-cycle training using the callback infrastructure that was built. . Lesson 9 lecture video . I found the second half of this lesson hard to make notes for because it is so code heavy. I didn’t want to just reproduce the jupyter notebooks here. I instead opted to provide a companion to the notebooks, providing extra explanation and also motivation for the design decisions. I tried to write it such that they could be used as guide for implementing the main parts yourself from scratch, which is how I practice this course. Enjoy! . Classification Loss Function . From the last lesson the model so far is: . class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)] def __call__(self, x): for l in self.layers: x = l(x) return x . Recall we were using the MSE as the loss function, which doesn’t make sense for a multi-classification problem, but was convenient as a teaching tool. Let’s continue with this and use an appropriate loss function. . This follows the notebook: 03_minibatch_training.ipynb . Cross-Entropy Loss . We need a proper loss function for MNIST. This is a multi-class classification problem so we use Cross-entropy loss. Cross-entropy loss is calculated using a function called the Softmax function: . [p(z_i) = hbox{softmax(z)}{i} = frac{e^{z{i}}}{ sum_{0 leq j leq n-1} e^{z_{j}}}] . Where $z_i$ are the real-valued outputs of the model. Softmax takes in a vector of $K$ real numbers, and normalizes it into a probability distribution of $K$ probabilities proportional to the exponentials of the input numbers. These collectively sum to 1, and each have values between 0 and 1 (this is also called a Categorical distribution). . We now have a probability vector (length 10), $p(z_i)$, that the model thinks that a given input has label $i$ (i.e. 0-9). This could look like: . pz = [0.05, 0.05, 0.05, 0.05, 0.1, 0.6, 0.025, 0.025, 0.025, 0.025] . When training know what the target value is. If this is represented as a categorical distribution like $z$, we would get the vector $x$: . x = [0., 0., 0., 0., 0., 1.0, 0., 0., 0., 0.] . We know for certain what the target value is, so the probability for that label is 1 and the rest are 0. So we could think of this as a distribution, or just as a one-hot encoded vector for the target label. . Cross-entropy is a function commonly used to quantify the difference between two probability distributions, this is why we can use it as our loss function. If we have the ‘true’ distribution, $x_i$, and the estimated distribution, $p(z_i)$, the cross-entropy loss is defined as: . [L =- sum_i x_i log p(z_i)] . This has a minimal value when the estimated distribution matches the true distribution. You can see this in the plot of the cross entropy with varying $p(z)$: . . Another name for cross entropy is the negative log likelihood. . Since $x$ is a one-hot encoded vector, all the 0 entries will be masked out leaving the cross entropy as just: . [L = - log p(z_i) = - log ( mbox{softmax}( mathbf{z})_i)] . Where $i$ is the index of the target label. We can therefore code the cross-entropy loss for multi-class as an array lookup. The code for the cross-entropy, or negative log likelihood, is therefore: . def nll(input, target): # input is log(softmax(z)) # x is 1-hot encoded target, so this simplifies to array lookup. return -input[range(target.shape[0]), target].mean() . The total loss is just the average of the negative log likelihood’s of all the training examples (in a batch). Next we need to implement a log-Softmax function to calculate the input to nll. . Log-Softmax Layer: Naive Implementation . First implementation: let’s code up the formula for Softmax then take the log of it: . def log_softmax(x): # naive implementation return (x.exp() / x.exp().sum(-1, keepdim=True)).log() . On paper, the maths works out and we can just convert the formula to code like above. However, this implementation has several big problems that mean this code will not work in practice. . Exponentials, Logs, and Floating Point Hell… . Working with exponentials on a computer requires care - these numbers can get very big or very small, fast. Floating point numbers are finite approximation of real numbers; for most of the time we can pretend that they behave like real numbers, but when we start to get into extreme values this thinking breaks down and we are confronted with the limitations of floats. . If a float gets too big it will overflow, that is it will go to INF: . np.exp(1) -&gt; 2.718281828459045 np.exp(10) -&gt; 22026.465794806718 np.exp(100) -&gt; 2.6881171418161356e+43 np.exp(500) -&gt; 1.4035922178528375e+217 np.exp(1000) -&gt; inf # oops... . On the other hand, if a float gets too small it will underflow, that is it will go to zero: . np.exp(-1) -&gt; 0.36787944117144233 np.exp(-10) -&gt; 4.5399929762484854e-05 np.exp(-100) -&gt; 3.720075976020836e-44 np.exp(-500) -&gt; 7.124576406741286e-218 np.exp(-1000) -&gt; 0.0 # oops... . The input to exponential doesn’t even have to that big to get under/overflow. Therefore we can’t really trust the naive softmax not to break because of this. . Another less obvious issue is that when doing operations on floats with extreme values, arithmetic can stop working: . np.exp(-10) + np.exp(-100) == np.exp(-10) # wut np.exp(10) + np.exp(100) == np.exp(100) # wut? . Operations between floats are performed and then rounded. The difference in value between the numbers here is so massive that the smaller one gets rounded away and disappears - loss of precision. This is a big problem for the sum of exponentials in the denominator of the softmax formula. . The solution to dealing with extreme numbers is to transform everything into log space, where things are more stable. A lot of numerical code is implemented in log space and there are many formulae/tricks for transforming operations into log space. The easy ones are: . [ begin{align} log e^x &amp;= x log b^a &amp;= a log b log (ab) &amp;= log a + log b log left ( frac{a}{b} right ) &amp;= log(a) - log(b) end{align}] . How to transform the sum of exponentials in softmax? There is no nice formula for the log of a sum, so we’d have to leave log space, compute the sum, and then take the log of it. Leaving log space would give us all the headaches described above. However there is trick to computing the log of a sum stably called the LogSumExp trick. The idea is to use the following formula: . [ log left ( sum_{j=1}^{n} e^{x_{j}} right ) = log left ( e^{m} sum_{j=1}^{n} e^{x_{j}-m} right ) = m + log left ( sum_{j=1}^{n} e^{x_{j}-m} right )] . Where $m$ is the maximum of the $x_{j}$. The subtraction of $m$ is to bring the numbers down to a size that’s safe to leave log land to perform the sum. . (Nerdy extras: even if a float isn’t so small that it underflows, if it gets small enough it becomes ‘denormalized’. Denormal numbers extend floats to get some extra values very close to zero. They are handled differently from normal floats by the CPU and their performance is terrible, slowing your code right down. See this classic stackoverflow question for more on this). . Log-Softmax Layer: Better Implementation . Implement LogSumExp in Python: . def logsumexp(x): m = x.max(dim=-1)[0] return m + (x - m.unsqueeze(-1)).exp().sum(dim=-1).log() . PyTorch already has this: x.logsumexp(). . We can now implement log_softmax and cross_entropy_loss: . def log_softmax(x): # return x - x.logsumexp(-1,keepdim=True) # pytorch version return x - logsumexp(x).unsqueeze(-1) def cross_entropy_loss(output): return nll(log_softmax(output), target) . Now we’ve implemented cross entropy from scratch we may use PyTorch’s versions of the functions: . import torch.nn.functional as F test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss) test_near(F.cross_entropy(pred, y_train), loss) . Mini-Batch Training . Basic Training Loop . Now we have the loss function done, next we need a performance metric. For a classification problem we can use accuracy: . def accuracy(out, targ): return (torch.argmax(out, dim=1) == targ).float().mean() . Now we built a training loop. (Recall the training loop from Fast.ai part 1). . The basic training loop repeats over the following: . Get the output of model on a batch of inputs | Compare the output with the target and compute the loss | Calculate the gradients of the loss wrt every parameter of the model | Update the parameters using those gradients to make them a little bit better | In Python with our current model this is: . for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad(): for l in model.layers: if hasattr(l, &#39;weight&#39;): l.weight -= l.weight.grad * lr l.bias -= l.bias.grad * lr l.weight.grad.zero_() l.bias .grad.zero_() . What it does: . loss.backward() computes the gradient of the loss wrt the parameters of the model using Pytorch’s autograd. | The updating of the parameters is done inside of torch.no_grad() because this is not part of the gradient calculation, it’s the result of it. | It loops through the layers and checks if they have attribute ‘weight’. | After updating the parameters it zeros the gradients so that the old values don’t persist into the next iteration. | . The next part of the lesson works on refactoring this loop until we end up with an implementation equivalent to the one in PyTorch. I think it’s a good exercise to try and reproduce this yourself after watching this part of the lecture. Rather than just copy the notebook, I will structure this section as hints/descriptions of what you need to do, followed by the solution code from the notebook. . Refactoring 1 . Currently when we update the parameters we have to loop through the layers and then check to see if they have parameter ‘weight’ and then update the weight and bias of that layer. This is long winded and it exposes the implementation too much. . We want instead to be able to loop through all the parameters in the model in a cleaner way: . ... loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . Hint: Our model already is a subclass of nn.Module, which has a special way of handling its attributes (__setattr__) that we can take advantage of if we change the way the layers are declared. Doing it this way will enable the use of nn.Module methods .parameters and .zero_grad… . . Solution: . Click to reveal code… class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.l1 = nn.Linear(n_in,nh) self.l2 = nn.ReLU() self.l3 = nn.Linear(nh,n_out) def __call__(self, x): return self.l3(self.l2(self.l1(x))) . Set the layers as attributes rather than storing a list of them. Doing things this way enables nn.Module to do some magic in the background. Look at the string representation of our model now: . &gt;&gt;&gt; model Model( (l1): Linear(in_features=784, out_features=50, bias=True) (l2): ReLU() (l3): Linear(in_features=50, out_features=10, bias=True) ) . It somehow knows about the layers we set as attributes. Looping through .parameters now returns the weight and bias matrices of the layers too. . What’s actually going on is nn.Module class overrides __setattr__, so every time we set an attribute that’s a PyTorch layer it registers that to an internal list. Methods like .parameters and .zero_grad then iterate through that list. . This internal list is stored as self._modules, we can take a peek at it: . &gt;&gt;&gt; model._modules OrderedDict([(&#39;l1&#39;, Linear(in_features=784, out_features=50, bias=True)), (&#39;l2&#39;, ReLU()), (&#39;l3&#39;, Linear(in_features=50, out_features=10, bias=True))]) . Refactoring 2 . It’s more convenient now, but it’s not convenient enough. It’s not nice having to write attributes for every layer - what if we had 50 layers? The forward pass is also inconvenient to write, it was better when we could just loop through the layers. . It would be nice if we could make the old implementation that had a list of layers work while getting the __setattr__ goodness too. . Hint: checkout nn.ModuleList . . Solution: . Click to reveal code… class SequentialModel(nn.Module): def __init__(self, layers): super().__init__() self.layers = nn.ModuleList(layers) def __call__(self, x): for l in self.layers: x = l(x) return x . ``nn.ModuleList` gives us the list model, but also registers the layers in the list so we retain the nice features from before: . &gt;&gt;&gt; model SequentialModel( (layers): ModuleList( (0): Linear(in_features=784, out_features=50, bias=True) (1): ReLU() (2): Linear(in_features=50, out_features=10, bias=True) ) ) . We have implemented the equivalent to nn.Sequential, which we now may use. . model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, nout)) . Checkout the source code for this and see how similar the code is to our version: nn.Sequential??. . Refactoring 3 . That’s the model refactored. What about the optimization step? Let’s replace our previous manually coded optimization step: . with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . and instead use just: . opt.step() opt.zero_grad() . This abstracts away the optimization algorithm and implementation and lets us swap things out in future. . Hint: Let’s create a class Optimizer to do this. It should take the parameters and the learning rate and implement the step and zero_grad methods. . . Solution: . Click to reveal code… class Optimizer(): def __init__(self, params, lr=0.05): self.params = list(params) self.lr = lr def step(self): with torch.no_grad(): for p in self.params: p -= p.grad * self.lr def zero_grad(self): for p in self.params: p.grad.zero_() . Training loop is now: . opt = Optimizer(model.parameters()) for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . We now implemented an equivalent of PyTorch’s optim.SGD, which we may now use: . from torch import optim opt = optim.SGD(model.parameters(), lr=0.05) . Refactoring 4 - Dataset . Let’s refactor how the data is retrieved and grouped into batches. . It’s clunky to iterate through minibatches of x and y values separately: . xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] . Instead, let’s do these two steps together, by introducing a Dataset class: . xb, yb = train_ds[i*bs : i*bs+bs] . Hint: your class needs to override __getitem__. . . Solution: . Click to reveal code… class Dataset(): def __init__(self, x, y): self.x, self.y = x, y def __len__(self): return len(x) def __getitem__(self): return self.x[i], self.y[i] . Use: . train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) . Refactoring 5 - DataLoader . Previously, our loop iterated over batches (xb, yb) like this: . for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] ... . Let’s make our loop much cleaner, using a data loader: . for xb,yb in train_dl: ... . Hint: you need to override __iter__ and use yield. . . Solution: . Click to reveal code… class DataLoader(): def __init__(self, ds, bs): self.ds,self.bs = ds,bs def __iter__(self): for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] . Use, training and validation data loaders: . train_dl = DataLoader(train_ds, bs) valid_dl = DataLoader(valid_ds, bs) . After all this refactoring the training loop now looks like: . def fit(): for epoch in range(epochs): for xb,yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . Much smaller and very readable. . Random Sampling . We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized. . As we did with DataLoader we can implement this as a class that takes a Dataset and batch size, then overrides _iter__ so that it yields the indices of the dataset in a random order. . class Sampler(): def __init__(self, ds, bs, shuffle=False): self.n,self.bs,self.shuffle = len(ds),bs,shuffle def __iter__(self): self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n) for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] . Use: . s = Sampler(small_ds,3,True) [o for o in s] . We can then update our DataLoader class so that it takes a Sampler and can return items in a random order. . def collate(b): xs,ys = zip(*b) return torch.stack(xs),torch.stack(ys) class DataLoader(): def __init__(self, ds, sampler, collate_fn=collate): self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn def __iter__(self): for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s]) . The collate function is for gathering up the data in the batch. In this case [self.ds[i] for i in s] returns a list of (x,y) tuples. We want these to be instead be two tensors xs and ys, which is what the function collate does. . Use: . train_samp = Sampler(train_ds, bs, shuffle=True) valid_samp = Sampler(valid_ds, bs, shuffle=False) train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate) valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate) . Training Loop Implemented with PyTorch Classes . At this point we have implemented the equivalents of the PyTorch classes: DataLoader, SequentialSampler, and RandomSampler, so we may use them from now on. . The PyTorch code that does everything we have implemented so far would be: . from torch.utils.data import DataLoader import torch.functional as F from torch import optim train_ds = Dataset(x_train, y_train) valid_ds = Dataset(x_valid, y_valid) train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True) valid_dl = DataLoader(valid_ds, bs, shuffle=False) loss_func = F.cross_entropy def get_model(): model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)) opt = optim.SGD(model.parameters(), lr=0.05) return model, opt def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): # train model.train() for xb, yb in train_dl: loss = loss_func(model(xb), yb) loss.backward() opt.step() opt.zero_grad() # validate model.eval() with torch.no_grad(): total_loss, total_acc = 0.0, 0.0 for xb, yb in valid_dl: pred = model(xb) total_loss += loss_func(pred, yb) total_acc += accuracy(pred, yb) nv = len(valid_dl) # NB these averages are incorrect if the # batch size varies... print(epoch, total_loss/nv, total_acc/nv) fit(3, model, loss_func, opt, train_dl, valid_dl) . This training loop also includes validation. We calculate and print the validation loss at the end of each epoch. . Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases. . Infinitely Customizable Training Loop . (Time in Lesson 9) . Our train loop so far is in the function fit above. We need a code design where users can infinitely customize this loop to add whatever they want, like fancy progress bars, different optimizers, tensorboard integration, regularization etc. The library design would need to be open and flexible enough to handle any unforeseen extensions. There is a good way to build something that can handle this - Callbacks. . . FastAI’s callbacks not only let you look at, but fully customize every single part of the training loop. The training loop contains all the parts of the code we wrote above, but in between these parts are slots for callbacks. Like on_epoch_begin, on_batch_begin, on_batch_end, on_loss_begin… and so on. Screen grab from lecture: . . These updates can be new values, or flags that skip steps or stop the training. . With this we can create all kinds of useful stuff in FastAI like learning rate schedulers, early stopping, parallelism, or gradient clipping. You can also mix them all together. . This next part of the lesson builds the framework for handling callbacks. It’s hard to write as notes because it is very code heavy. I will make some general descriptions of the design decisions. Then I will move onto the implementations of Callbacks used within this framework. I recommend just watching the lesson and working through the notebook. . Training Loop Landmarks . The training loop has several key points or landmarks just before or just after important parts of the training loop and we may want to inject some functionality/code into those points. In running order these are: . The start of the training: begin_fit | The end of the training: after_fit | The start of each epoch: begin_epoch | The start of a batch: begin_batch | After the loss is calculated: after_loss | After the backward pass is performed: after_backward | After the optimizer has performed a step: after_step | After all the batches and before validation: begin_validate | The end of each epoch: after_epoch | The end of the training: after_fit | Also after every batch or epoch we may want to halt everything: do_stop | Callback Class + Callback Handler (Version 1) . A sensible design option when faced with this would be to define an abstract base class that has methods corresponding to all the landmarks (+ method names) above. Every one of these methods should return True or False to indicate success/failure or some other stopping condition. At each of the landmarks in the training loop these booleans will be checked to see if the training loop should continue or not. . What the Callback base class could look like: . . We want to be able to pass multiple callbacks to the training loop so we’d need an addition class to handle collections of callbacks called CallbackHandler. It would have a collection of Callback objects and the same methods as Callback except it loops through all of its callback objects and return a boolean indicated if all the callbacks were successful or if any failed. . Here is a snippet of a potential CallbackHandler class: . . Alternative Design: Runner Class . The last design could lead to some code smell as seen here: . . Callbacks cb are passed as the argument of every function in the training loop. This suggests that these functions should be part of a class and cb should be an instance attribute in that class. . We create a new class Runner (I won’t list here), which contains one_batch, all_batches, and fit methods from the training loop, takes a list of Callback objects in the constructor, while also integrating the logic of the the previous CallbackHandler class. . It has some clever refactoring so that the looping through the callbacks is handled by overriding __call__, finding all the callbacks in its collection that have the required method name (e.g. ‘begin_epoch’) and calling them. The boolean logic of starting and stopping is handled by this method too, which means the Callback subclasses no longer need to return booleans - they can just do their job without needing to know the context within which they are used. Here is an example of a Callback in this implementation: . class ChattyCallback(Callback): def begin_epoch(self): print(&#39;begin_epoch...&#39;) def after_epoch(self): print(&#39;after epoch...&#39;) def begin_fit(self): print(&#39;begin_fit...&#39;) def begin_validate(self): print(&#39;begin_validate...&#39;) . &gt;&gt;&gt; run = Runner(cbs=[ChattyCallback()]) &gt;&gt;&gt; run.fit(2, learn) begin_fit... begin_epoch... begin_validate... after epoch... begin_epoch... begin_validate... after epoch... . The Runner design decouples the training loop from the callbacks such that even the different logic required for training and validation parts of the training loop can be implemented as a Callback which is hard coded into the Runner class: . class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False . (IMHO: The Runner code is quite hard to understand, but it’s not important in the rest of the course. This is an experimental class and it doesn’t end up even in the FastAI2 library. Looking at the state of the library (2/2020), ideas from this class do appear in the new Learner class. It’s better just to know what you need to write callbacks). . Things to note for all the Callbacks implemented in the next section: . They assume the existence of self.in_train, denoting if we are in training or validation. This variable is set by TrainEvalCallback. | They also have access to variables in the Runner class such as: self.opt, self.model, self.loss_func, self.data, self.n_epochs, and self.epochs. | . Callbacks Applied: Annealing . (Time in lesson 9 video) . Rather than spend too much time on understanding Runner, let’s move onto doing something useful - implementing some callbacks. . Let’s implement callbacks to do one-cycle training. If you can train the first batches well, then the whole training will be better, and you can get super-convergence. Good annealing is critical to doing the first few batches well. . First let’s make a callback Recorder that records the learning rate and loss after every batch. This calls will need two lists for the learning rates and the losses that are initialized at the being of the training loop, and it will need to append to these lists after every batch. . Recorder: . class Recorder(Callback): def begin_fit(self): self.lrs,self.losses = [],[] def after_batch(self): if not self.in_train: return self.lrs.append(self.opt.param_groups[-1][&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) # methods for plotting results def plot_lr (self): plt.plot(self.lrs) def plot_loss(self): plt.plot(self.losses) . Next we need a callback class that can update the parameters of the optimizer opt according to some schedule function based on how many epochs have elapsed. . ParamScheduler: . class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_func): self.pname, self.sched_func = pname, sched_func def set_param(self): for pg in self.opt.param_groups: pg[self.pname] = self.sched_func(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() . Next we want to define some annealing functions for raising and lowering the learning rate as shown in these plots: . .   |   | . These annealers should take a start and end value and a position between 0 and 1 denoting the relative position in the schedule. Rather than writing a function that takes all 3 of these arguments, when 2 of them are constant, we could either implement the annealing functions as an abstract base class or just use partial functions. Here partial functions are used: . def annealer(f): def _inner(start, end): return partial(f, start, end) return _inner @annealer def sched_lin(start, end, pos): return start + pos*(end-start) @annealer def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2 @annealer def sched_no(start, end, pos): return start @annealer def sched_exp(start, end, pos): return start * (end/start) ** pos def cos_1cycle_anneal(start, high, end): return [sched_cos(start, high), sched_cos(high, end)] . annearler is a decorator function. Decorators take a function and return another function and have the fancy @decorator syntax in Python. . We want to combine raising and lowering schedules in a single function alongside a list of positions for when the different schedules start. This is the combine_scheds function: . def combine_scheds(pcts, scheds): assert sum(pcts) == 1. pcts = tensor([0] + listify(pcts)) assert torch.all(pcts &gt;= 0) pcts = torch.cumsum(pcts, 0) def _inner(pos): idx = (pos &gt;= pcts).nonzero().max() actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx]) return scheds[idx](actual_pos) return _inner sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) . Which gives the following schedule: . . Now we can make our list of callbacks and run the training loop: . cbs = [Recorder(), AvgStatsCallback(accuracy), ParamScheduler(&#39;lr&#39;, sched)] learn = create_learner(get_model_func(0.3), loss_func, data) run = Runner(cbs=cbs) run.fit(3, learn) . We can then check the Recorder plots to see if it worked: . .   |   | . Super! . Q &amp; A . Why do we have to zero out our gradients in PyTorch? . In models, Parameters often have lots of sources of gradients. The grad stored by the parameters in PyTorch is a running sum - it is updated with +=, not =. If we didn’t zero the gradients after every update then these old values from previous batches would accumulate. . | Why does the optimizer separate step and zero_grad? . If we merged the two, we remove the ability to not zero the gradients here. There are cases where we may want that control. For example, what if we are dealing with super resolution 4K images and we can only fit a batch size of 2 into RAM. The stability you get from this batch size is poor and you need a larger batch size. We could instead not zero the grads every time, rather do it ever other batch. Our effective batch size would have then doubled. That’s called gradient accumulation. . | What’s the difference between FastAI callbacks and PyTorch Hooks? . PyTorch hooks allow you to hook into the internals of your model. So if you want to look at the forward pass of layer 2 of you model, FastAI callbacks couldn’t do that because they are operating at a higher level. All FastAI sees is the forward and backward passes of your model. What goes on within them is PyTorch’s domain. . | . Links and References . Lecture video: Lesson9 | Course notebooks: 04_callbacks.ipynb, 05_anneal.ipynb | Lesson notes by @Lankinen are great transcriptions of the lecture. | An even deeper dive into PyTorch’s classes, written by the FastAI team: What is torch.nn really? | Sylvain’s talk, An Infinitely Customizable Training Loop (from the NYC PyTorch meetup) and the slides that go with it . | Softmax vs Sigmoid? tl;dr sigmoid is a special case of softmax. | Some other cool Log tricks: Exp-normalize trick, Gumbel-max trick | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/16/fast-ai-lesson-9-notes-training-in-depth.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/16/fast-ai-lesson-9-notes-training-in-depth.html",
            "date": " • Feb 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fast.ai v3 Lesson 8 Notes: Backprop from the Foundations",
            "content": "Overview . Part 2 of FastAI 2019 is ‘bottom-up’ - building the core of the FastAI library from scratch using PyTorch. . This lesson implements matrix multiplication in pure Python, then refactors and optimizes it using broadcasting and einstein summation. Then this lesson starts to look at the initialization of neural networks. Finally the lesson covers handcoding the forward and backwards passes of a simple model with linear layers and ReLU, before refactoring the code to be more flexible and concise so that you can understand how PyTorch’s work. . Lesson 8 lecture video. . Different Matrix Multiplication Implementations . Naive Matmul . A baseline naive implementation in pure python code: . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . Time: 3.26s . Doing loops in pure python and updating array elements one at a time is the bane of performance in python. There is almost always another way that gives better performance. (Though admittedly in some cases the faster way isn’t always obvious or more readable IMHO). . Elementwise Matmul . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): # Any trailing &quot;,:&quot; can be removed c[i,j] = (a[i,:] * b[:,j]).sum() return c . Time: 4.84ms . The loop over k is replaced with a sum() over the elements of row slice in a times the column slice in b. This operation is outsourced to library calls in numpy which are likely compiled code written in C or Fortran, which gives the near 1000x speed-up. . Broadcasting matmul . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) # c[i] = (a[i, :, None] * b).sum(dim=0) alternative return c . Time: 1.11ms . WTH is this? As is almost always the case, optimizing code comes at the expense of code readability. Let’s work through this to convince ourselves that this is indeed doing a matmul. . Aside: Proof of Broadcasting Matmul . Matmul is just a bunch of dot products between the rows of one matrix and the columns of another: i.e. c[i,j] is the dot product of row a[i, :] and column b[:, j]. . Let’s consider the case of 3x3 matrices. a is: . tensor([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.]], dtype=torch.float64) . b is: . tensor([[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]], dtype=torch.float64) . Let’s derive the code above looking purely through modifying the shape of a. . a has shape (3,3) | a[0], first row of a, has shape (3,) and val [1, 1, 1] | a[i, :, None] (or a[i].unsqueeze(-1)) has shape (3,1) and val [[1], [1], [1]] | Now multiplying the result of 3 by the matrix b is represented by the expression (I have put brackets in to denote the array dimensions): . [ left( begin{matrix}(1) (1) (1) end{matrix} right) times left( begin{matrix}(0&amp;1&amp;2) (3&amp;4&amp;5) (6&amp;7&amp;8) end{matrix} right)] . From the rules of broadcasting, the $(1)$s on the left array are expanded to match the size of the rows on the right array (size 3). As such, the full expression computed effectively becomes: . [ left( begin{matrix}(1&amp;1&amp;1) (1&amp;1&amp;1) (1&amp;1&amp;1) end{matrix} right) times left( begin{matrix}(0&amp;1&amp;2) (3&amp;4&amp;5) (6&amp;7&amp;8) end{matrix} right) = left( begin{matrix}(0&amp;1&amp;2) (3&amp;4&amp;5) (6&amp;7&amp;8) end{matrix} right)] . The final step is to sum(dim=0), which sums up all the rows leaving a vector of shape (3,), value: [ 9., 12., 15.] . That completes the dot product and forms the first row of matrix c. Simply repeat that for the remaining 2 rows of a and you get the final result of the matmul: . tensor([[ 9., 12., 15.], [18., 24., 30.], [27., 36., 45.]], dtype=torch.float64) . Einstein Summation Matmul . This will be familiar to anyone who studied Physics, like me! Einstein summation (einsum) is a compact representation for combining products and sums in a general way. From the numpy docs: . “The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so np.einsum(&#39;i,i&#39;, a, b) is equivalent to np.inner(a,b). If a label appears only once, it is not summed, so np.einsum(&#39;i&#39;, a) produces a view of a with no changes.” . def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . Time: 172µs . This is super concise with great performance, but also kind of gross. It’s a bit weird that einsum is a mini-language that we pass as a Python string. We get no linting or tab completion benefits that you would get if it were somehow a first class citizen in the language. I think einsum could certainly be great and quite readable in cases where you are doing summations on tensors with lots of dimensions. . PyTorch Matmul Intrinsic . Matmul is already provided by PyTorch (or Numpy) using the @ operator: . def matmul(a, b): return a@b . Time: 31.2µs . The best performance is, unsuprisingly, provided by the library implementation. This operation will drop down to an ultra optimized library like BLAS or cuBLAS, written by low-level coding warrior-monks working at Intel or Nvidia who have have spent years hand optimizing linear algebra code in C and assembly. (The matrix multiply algorithm is actually a very complicated topic, and no one knows what the fastest possible algorithm for it is. See this wikipedia page for more). So basically in the real world, you should probably avoid writing your own matmal! . Single Layer Network: Forward Pass . Work through the Jupyter notebook: 02_fully_connected.ipynb . Create simple network for MNIST. One hidden layer and one output layer, parameters: . n = 50000 m = 784 nout = 1 # just for teaching purposes here, should be 10 nh = 50 . The model will look like this: . [X rightarrow mbox{Lin}(W_1, b_1) rightarrow mbox{ReLU} rightarrow mbox{Lin2}(W_2, b_2) rightarrow mbox{MSE} rightarrow L] . Linear activation function: . def lin(x, w, b): return x@w + b . ReLU activation function: . def relu(x): return x.clamp_min(0.) . Loss function we’ll use here is the Mean Squared Error (MSE). This doesn’t quite fit for a classification task, but it’s used as a pedgogical tool for teaching the concepts of loss and backpropagation. . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . Forward Pass of model: . def model(xb): l1 = lin(xb, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 preds = model(x_train) loss = mse(preds, y_train) . Let’s go over the tensor dimensions to review how the forward pass works: . Input $X$ is a batch of vectors of size 784, shape=[N, 784] | Hidden layer is of size 50 and has an input of shape=[N, 784] =&gt; $W_1$: shape=[784, 50], $b_1$: shape=[50], output: shape=[N, 50] | Output layer has size 1 and input of shape=[N, 50] =&gt; $W_2$: shape=[50, 1], $b_2$: shape=[1], output: shape=[N, 1] | . Initialisation . Recent research shows that weight initialisation in NNs is actually super important. If the network isn’t initialised well, then after one pass through the network the output can sometimes become vanishingly small or even explode, which doesn’t bode well for when we do backpropagation. . A rule of thumb to prevent this is: . The mean of the activations should be zero | The variance of the activations should stay close to 1 across every layer. | Let’s try Normal initialisation with a linear layer: . w1 = torch.randn(m,nh) b1 = torch.zeros(nh) w2 = torch.randn(nh,1) b2 = torch.zeros(1) . x_valid.mean(),x_valid.std() &gt;&gt;&gt; (tensor(-0.0059), tensor(0.9924)) . t = lin(x_valid, w1, b1) t.mean(),t.std() &gt;&gt;&gt; (tensor(-1.7731), tensor(27.4169)) . After one layer, it’s already in the rough. . A better initialisation is Kaiming/He initialisation (paper). For a linear activation you simply divide by the square root of the number of inputs to the layer.: . w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . Test: . t = lin(x_valid, w1, b1) t.mean(),t.std() &gt;&gt;&gt; (tensor(-0.0589), tensor(1.0277)) . The initialisation used depends on the activation function used. If we instead use a ReLU layer then we have to do something different from the linear. . If you have a normal distribution with mean 0 with std 1, but then clamp it at 0, then obviously the resulting distribution will no longer have mean 0 and std 1. . From pytorch docs: a: the negative slope of the rectifier used after this layer (0 for ReLU by default) . [ text{std} = sqrt{ frac{2}{(1 + a^2) times text{fan_in}}} ] This was introduced in the paper that described the Imagenet-winning approach from He et al: Delving Deep into Rectifiers, which was also the first paper that claimed “super-human performance” on Imagenet (and, most importantly, it introduced resnets!) . w1 = torch.randn(m,nh)*math.sqrt(2/m) . Test: . t = relu(lin(x_valid, w1, b1)) t.mean(),t.std() &gt;&gt;&gt; (tensor(0.5854), tensor(0.8706)) . The function that does this in the Pytorch library is: . from torch.nn import init w1 = torch.zeros(m,nh) init.kaiming_normal_(w1, mode=&#39;fan_out&#39;) . &#39;fan_out&#39; means that we divide by m, while &#39;fan_in&#39; would mean we divide by nh. This bit here is confusing because we are using the opposite convention to PyTorch has. PyTorch Linear layer stores the matrix as (nh, m), where our implementation is (m, nh). Looking inside the forward pass of linear in PyTorch the weight matrix is transposed before being multiplied. This means that for this special case here we swap ‘fan_out’ and ‘fan_in’. If we were using PyTorch’s linear layer we’d initialize with ‘fan_in’. . Let’s get a better view of the means and standard deviations of the model with Kaiming initialization by running the forward pass a few thousand times and looking at the distributions. . (Update, 8/2/20: Old plots were buggy. Fixed plots, added code, and added plots with Linear-ReLU model). . Linear-ReLU Model, Kaiming Init . def model_dist(x): w1 = torch.randn(m, nh) * math.sqrt(2/m) b1 = torch.zeros(nh) l1 = lin(x, w1, b1) l2 = relu(l1) l2 = l2.detach().numpy() return l2.mean(), l2.std() data = np.array([model_dist(x_train) for _ in range(3000)]) means, stds = data[:, 0], data[:, 1] . Mean and standard deviations of the outputs with Kaiming Initialization: . . . The means and standard deviations of the output have Gaussian distributions. The mean of the means is 0.55 and the mean of the standard deviations is 0.82. The mean is shifted to be positive because the ReLU has set all the negative values to 0. The typical standard deviation we get with Kaiming initialization is quite close to 1, which is what we want. . Full Model, Kaiming Init . def model_dist(x): w1 = torch.randn(m, nh) * math.sqrt(2/m) b1 = torch.zeros(nh) w2 = torch.randn(nh, nout) / math.sqrt(nh) b2 = torch.zeros(nout) l1 = lin(x, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) l3 = l3.detach().numpy() return l3.mean(), l3.std() data = np.array([model_dist(x_train) for _ in range(3000)]) means, stds = data[:, 0], data[:, 1] . . . The means have a clearly Gaussian distribution with mean value 0.01. The standard deviations have a slightly skewed distribution, but the mean value is 0.71. We see empirically that the expected output values of the model after Kaiming initialisation are approximately mean 0, standard deviation near to 1, so it seems to be working well. . Aside: Init in Pytorch - sqrt(5)?? . In torch.nn.modules.conv._ConvNd.reset_parameters: . def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) . A few differences here: . Uses Uniform distribution instead of a Normal distribution. This just seems to be convention the PyTorch authors have chosen to use. Not an issue and it is centred around zero anyway. | The sqrt(5) is probably a bug, according to Jeremy. | The initialization for the linear layer is similar. . From the documentation on parameter a: . a: the negative slope of the rectifier used after this layer (0 for ReLU by default) . For ReLU it should be 0, but here it is hard-coded to sqrt(5). So for ReLU activations in Conv layers, the initialization of some layers in PyTorch is suboptimal by default. . (Update 8/2/20). We can look at the distribution of the outputs of our model using PyTorch’s default init: . def model_dist(x, n_in, n_out): layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)] for l in layers: x = l(x) x = x.detach().numpy() return x.mean(), x.std() . . . Mean value is approximately 0.0 and the standard deviation is 0.16. This isn’t great - we have lost so much variation after just two layers. The course investigates this more in the notebook: 02a_why_sqrt5.ipynb. . (Update: Here is a link to the issue in PyTorch, still open (2020-2-13), https://github.com/pytorch/pytorch/issues/18182) . Gradients and Backpropagation . To understand backpropagation we need to first understand the chain rule from calculus. The model looks like this: . [x rightarrow mbox{Lin1} rightarrow mbox{ReLU} rightarrow mbox{Lin2} rightarrow mbox{MSE} rightarrow L] . Where $L$ denotes the loss. We can also write this as: . [L = mbox{MSE}( mbox{Lin2}( mbox{ReLU}( mbox{Lin1(x)})), y)] . Or fully expanded: . [ begin{align} L &amp;= frac{1}{N} sum_n^N left(( mbox{max}(0, X_nW^{(1)} + b^{(1)})W^{(2)} + b^{(2)}) - y_n right)^2 end{align}] . In order to update the parameters of the model, we need to know what is the gradient of $L$ with respect to (wrt) the parameters of the model. What are the parameters of this model? They are: $W_{ij}^{(1)}$, $W^{(2)}_{ij}$, $b^{(1)}_i$, $b^{(2)}_i$ (including indices to remind you of the tensor rank of the parameters). The partial derivatives of the parameters we want to calculate are: . [ frac{ partial L}{ partial W^{(1)}{ij}}, frac{ partial L}{ partial W^{(2)}{ij}}, frac{ partial L}{ partial b^{(1)}{i}}, ; mbox{and} ; frac{ partial L}{ partial b^{(2)}{i}}] . On first sight, looking at the highly nested function of $L$ finding the derivative of it wrt to matrices and vectors looks like a brutal task. However the cognitive burden is greatly decreased thanks to the chain rule. . When you have a nested function, such as: . [f(x,y,z) = q(x, y)z q(x,y) = x+y] . The chain rule tells you that the derivative of $f$ wrt to $x$ is: . [ frac{ partial f}{ partial x} = frac{ partial f}{ partial q} frac{ partial q}{ partial x} = (z) cdot(1) = z] . A helpful mnemonic is to picture the $ partial q$’s ‘cancelling out’. . Backpropagation: Graph Model . How does this fit into backpropagation? Things become clearer when the model is represented as a computational graph, instead of as equations. . Imagine some neuron $f$ in the middle of a bigger network. In the forward pass, data $x$ and $y$ flows from left to right through the neuron $f$, outputting $z$, then calculating the loss $L$. Then we want the gradients of all the variables wrt the loss. Here is a diagram taken from CS231 course : . . (Source: brilliant CS231 course from Stanford. This lecture made backpropagation ‘click’ for me: video, notes). . Calculate the gradients of the variables backwards from right to left. We have the gradient $ frac{ partial L}{ partial z}$ coming from ‘upstream’. To calculate $ frac{ partial L}{ partial x}$, we use the chain rule: . [ frac{ partial L}{ partial x} = frac{ partial L}{ partial z} frac{ partial z}{ partial x}] . The gradient = upstream gradient $ times$ local gradient. This relation recurses back through the rest of the network, so a neuron directly before $x$ would receive the upstream gradient $ frac{ partial L}{ partial x}$. The beauty of the chain rule is that it enables us to break up the model into its constituent operations/layers, compute their local gradients, then multiply by the gradient coming from upstream, then propagate the gradient backwards, repeating the process. . Coming back to our model - $ mbox{MSE}( mbox{Lin2}( mbox{ReLU}( mbox{Lin1(x)})), y)$ - to compute the backward pass we just need to compute the expressions for the derivatives of MSE, Linear layer, and ReLU layer. . Gradients of Vectors or Matrices . What happens when $z$, $x$, and $y$ aren’t scalar, but are vectors or matrices? Nothing changes with how backpropagation works - just the maths for computing the local gradients gets a bit hairier. . If the loss $L$ is a scalar and $ mathbf{z}$ is a vector then the derivative would be vector: . [ frac{ partial L}{ partial mathbf{z}} = left( frac{ partial L}{ partial z_1}, frac{ partial L}{ partial z_2}, …, frac{ partial L}{ partial z_n}, right)] . Think: “For each element of $ mathbf{z}$, if it changes by a small amount how much will $L$ change?” . If $ mathbf{x}$ and $ mathbf{z}$ are both vectors then the derivative would be a Jacobian matrix: . [ mathbf{ frac{ partial mathbf{z}}{ partial mathbf{x}}} = left[ begin{array}{ccc} frac{ partial z_1}{ partial x_1} &amp; frac{ partial z_1}{ partial x_2} &amp; … &amp; frac{ partial z_1}{ partial x_m} frac{ partial z_2}{ partial x_1} &amp; frac{ partial z_2}{ partial x_2} &amp; … &amp; frac{ partial z_2}{ partial x_m} … &amp; … &amp; … &amp; … frac{ partial z_n}{ partial x_1} &amp; frac{ partial z_n}{ partial x_2} &amp; … &amp; frac{ partial z_n}{ partial x_m} end{array} right]] . Think: “For each element of $ mathbf{x}$”, if it changes by a small amount then how much will each element of $ mathbf{y}$ change? . Summary, again taken from CS231n: . . More info: a full tutorial on matrix calculus is provided here: Matrix Calculus You Need For Deep Learning. . Gradient of MSE . The mean squared error: . [L = frac{1}{N} sum_i^N (z_i - y_i)^2] . Where $N$ is the batch size, $z_i$ is the output of the model for data point $i$, and $y_i$ is the target value of $i$. The loss is the average of the squared error in a batch. $ mathbf{z}$ is a vector here. The derivative of scalar $L$ wrt a vector will be vector. . [ begin{align} frac{ partial L}{ partial z_i} &amp;= frac{ partial}{ partial z_i} left( frac{1}{N} sum_j^N (z_j - y_j)^2 right) &amp;= frac{ partial}{ partial z_i} frac{1}{N} (z_i - y_i)^2 &amp;= frac{2}{N}(z_i - y_i) end{align}] . All the other terms in the sum go to zero because they don’t depend on $z_i$. Notice also how $L$ doesn’t appear in the gradient - we don’t actually need the value of the loss in the backwards step! . In Python code: . def mse_grad(inp, targ): # inp from last layer of model, shape=(N,1) # targ targets, shape=(N) # want: grad of MSE wrt inp, shape=(N, 1) grad = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / inp.shape[0] inp.g = grad . Gradient of Linear Layer . Linear layer: . [Y = XW + b] . Need to know: . [ frac{ partial L}{ partial X}, frac{ partial L}{ partial W}, frac{ partial L}{ partial b}] . Where $X$, and $W$ are matrices and $b$ is a vector. We already know $ frac{ partial L}{ partial Y}$ - it’s the upstream gradient (remember it’s a tensor, not necessarily a single number). . Here is where the maths gets a bit hairier. It’s not worth redoing the derivations of the gradients here, which can be found in these two sources: matrix calculus for deep learning, linear backpropagation. . The results: . [ frac{ partial L}{ partial X} = frac{ partial L}{ partial Y}W^T frac{ partial L}{ partial W} = X^T frac{ partial L}{ partial Y} frac{ partial L}{ partial b_i} = sum_j^M frac{ partial L}{ partial y_{ij}}] . In Python: . def lin_grad(inp, out, w, b): # inp - incoming data (x) # out - upstream data # w - weight matrix # b - bias inp.g = out.g @ w.t() w.g = inp.t() @ out.g b.g = out.g.sum(dim=0) . Gradient of ReLU . Gradient of ReLU is easy. For the local gradient - if the input is less than 0, gradient is 0, otherwise it’s 1. In Python . def relu_grad(inp, out): # inp - input (x) # out - upstream data inp.g = (inp&gt;0).float() * out.g . Putting it together: forwards and backwards . def forwards_and_backwards(inp, targ): # forward pass l1 = lin(inp, w1, b1) l2 = relu(l1) out = lin(l2, w2, b2) loss = mse(out, targ) # backward pass mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . Check the Dimensions. How does batchsize affect things? . (Added 17-03-2020) . What do the dimensions of the gradients look like? The loss $L$ is a scalar and the parameters are tensors so remembering the rules above the derivative of $L$ wrt any parameter will have the same dimensionality as that parameter. The gradients of the parameters have the same shape as the parameters, which makes intuitive sense. . w1.g.shape =&gt; [784, 50] | b1.g.shape =&gt; [50] | w2.g.shape =&gt; [50, 1] | b2.g.shape =&gt; [1] | loss.shape =&gt; [] (scalar) | . Notice how the batch size doesn’t appear in the gradients. That’s not to say it doesn’t matter - the batch size is there behind the scenes in the gradient calculation: the loss is an average of the individual losses in a batch, and also as a dimension multiplied out in the matrix multiplies of the gradient calculations. . To be even more explicit with the dimensions: . inp.g = out.g @ self.w.t() # [N, 784] = [N, 50] @ [50, 784] self.w.g = inp.t() @ out.g # [784, 50] = [784, N] @ [N, 50] self.b.g = out.g.sum(0) # [50] = [N, 50].sum(0) inp.g = out.g @ self.w.t() # [N, 50] = [N, 1] @ [1, 50] self.w.g = inp.t() @ out.g # [50, 1] = [50, N] @ [N, 1] self.b.g = out.g.sum(0) # [1] = [N, 1].sum(0) . With bigger batch size you are accumulating more gradients because it is basically doing more dot products. If you could hack the loss so its gradient is constant and increase the batch size then these gradients would get correspondingly larger (in absolute size). . In reality this is cancelled out because the larger the batch size the smaller the gradient. You can see this by look at the gradient calculation for MSE: it is divided by the batch size. . Let’s vary the batchsize and plot the average gradients of the parameters W1 and W2, alongside the loss and loss gradient: . . The average gradient of the loss gets smaller with increasing batchsize, while the other gradients and the loss pretty much settle towards some value. . Refactoring . (Updated 17-03-2020) . The rest of the notebook - 02_fully_connected.ipynb - is spent refactoring this code using classes so we understand how PyTorch’s classes are constructed. I won’t reproduce it all here. If you want to reproduce it yourself you need to create a base Module that all your layer inherit from, which remembers the inputs it was called with (so it can do gradient calculations): . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . The different layers (linear, ReLU, MSE) need to subclass Module and implement forward and bwd methods. . The end result of this gives an equivalent implementation of PyTorch’s nn.Module. The equivalent with PyTorch classes, which we can now use, is: . from torch import nn class Model(nn.Module): def __init__(self, n_in, n_out): super().__init__() self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)] self.loss = mse def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x.squeeze(), targ) . Now that we understand how backprop works, we luckily don’t have to derive anymore derivatives of tensors, we can instead from now on harness PyTorch’s autograd to do all the work for us! . model = Model(m, nh, 1) loss = model(x_train, y_train) loss.backward() # do the backward pass! . Links and References . Lesson 8 lecture video. | Lesson notes from Laniken provide a transcription of the lesson. | Broadcasting tutorial from Jake Vanderplas: Computation on Arrays: Broadcasting. | Deeplearning.ai notes on initialisation with nice demos of different initialisations and their effects: deeplearning.ai | Kaiming He paper on initialization with ReLu activations (assignment: read section 2.2 of this paper): Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | Fixup Initialization: paper where they trained a 10,000 layer NN with no normalization layers through careful initialization. | Things that made Backprop ‘click’ for me: CS231: backpropagation explained using the a circuit model: http://cs231n.github.io/optimization-2/ | CS231: backpropagation lecture (Andrej Karpathy), slides. | Blog post with worked examples of backpropagation on simple calculations. | Calculus on Computational Graphs, Chris Olah. | . | StackExchange: Tradeoff batch size vs. number of iterations to train a neural network - worth reading about somewhat unintuitive effect batchsize has on training performance and speed. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/06/fast-ai-lesson-8-notes-backprop-from-the-foundations.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/06/fast-ai-lesson-8-notes-backprop-from-the-foundations.html",
            "date": " • Feb 6, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jimypbr.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Galaxy Zoo Kaggle Competition Redux with Fast.ai",
            "content": "Overview . . The Galaxy Zoo kaggle competition is something that I feel a special connection with. Not only because my own background is in astrophysics, but this competition first introduced me to Kaggle and data science. While I didn’t have the time or knowledge to compete at the time, I followed it and I knew even then that the winning solutions would be based on convolutional neural networks. . I’ve been learning deep learning on and off since 2017, but it wasn’t until fastai that I actually managed to get from zero to one. After finishing the first course in fastai, going back and doing the Galaxy Zoo challenge was actually pretty undaunting! It’s basically a quirky image multi-label classification classification posed as a regression problem. Once I got my head around this, I was very fast in getting a CNN running in fastai. . Problem . From the Galaxy Zoo challenge description: . Understanding how and why we are here is one of the fundamental questions for the human race. Part of the answer to this question lies in the origins of galaxies, such as our own Milky Way. Yet questions remain about how the Milky Way (or any of the other ~100 billion galaxies in our Universe) was formed and has evolved. Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Understanding the distribution, location and types of galaxies as a function of shape, size, and color are critical pieces for solving this puzzle. . … . . (Image Credit: ESA/Hubble &amp; NASA) . Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies. That’s where you come in. . This competition asks you to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class. Can you write an algorithm that behaves as well as the crowd does? . Rather than being an image classification or image multi-label classification problem, that one might initially assume, this problem is actually a regression problem. The task is actually to predict the distribution of how the users would label an image of a galaxy. The UX presented to the user is that of a descision tree of questions: . . The distribution of user’s answers to these questions is represented as a 37D vector of floats with values between 0 and 1. These values are weighted according to the description here. . The task is therefore to train an algorithm that takes an image of a galaxy as an input and outputs a 37D vector of floats; it is a multi-dimensional regression problem. . Preprocessing . Let’s look at a batch of the images: . . The target galaxy is always in the centre of the image. There is a lot of redundant space round the outside of the galaxies. Each image is 424x424. I cropped the images down to 224x224. This saved on computation without throwing out resolution. . Transforms . The transforms I used: . tfms = get_transforms(flip_vert=False, max_warp=0.0, max_rotate=360, max_lighting=0.0, max_zoom=1.05) . Here they are as a grid: . . I also tried with adjusting the brightness and contrast, but I found that that didn’t improve anything. . DataBlock . The image file names and accompanying classification vectors are stored in a CSV file training_solutions_rev1. . I modified the fastai class FloatList to GalaxyFloatList, which is the same except it uses GalaxyFloatItem instead of FloatItem. GalaxyFloat has the show method subclassed so that the 37D float vector is converted to a string using a function I wrote called vec2labels. . class GalaxyFloatItem(FloatItem): def show(self, ax:plt.Axes, **kwargs): &quot;Translate the GalaxyZoo vector into a list of features&quot; ax.set_title(vec2labels(self.data)) class GalaxyFloatList(ItemList): def __init__(self, items:Iterator, log:bool=False, classes:Collection=None, **kwargs): super().__init__(np.array(items, dtype=np.float32), **kwargs) self.log = log self.copy_new.append(&#39;log&#39;) self.c = self.items.shape[1] if len(self.items.shape) &gt; 1 else 1 self.loss_func = MSELossFlat() def get(self, i): o = super().get(i) return GalaxyFloatItem(np.log(o) if self.log else o) def reconstruct(self,t): return GalaxyFloatItem(t.numpy()) src = (ImageList.from_csv(path, &#39;training_solutions_rev1.csv&#39;, folder=&#39;images_training_rev1_cropped&#39;, suffix=&#39;.jpg&#39;, cols=0) .split_by_rand_pct(0.2) .label_from_df(cols=list(range(1, 38)), label_cls=GalaxyFloatList)) data = (src.transform(tfms, size=112, resize_method=ResizeMethod.SQUISH, padding_mode=&#39;reflection&#39;).databunch()).normalize(imagenet_stats) . Model . For my model I used a ResNet50 CNN pretrained on Imagenet. . learner = cnn_learner(data, models.resnet50, metrics=rmse, ps=0.1, wd=1e-4,) learner.model[-1] = nn.Sequential(*learner.model[-1], GalaxyOutput()) . At the end of the network I tacked on a simple layer that correctly normalises the probability vector outputed by the network so that the output obeys the rules of probability. I got this idea after looking at the winning submission. Training without normalising the output also worked quite well, I found, however it does produce ill-formed results such as small negative numbers so normalising is good idea to get a small performance boost. . class GalaxyOutput(nn.Module): def __init__(self): super().__init__() def forward(self, x): return answer_probability(x) . Here is the code that normalises the probability vectors: . task_sectors = { 1: slice(0, 3), 2: slice(3, 5), 3: slice(5, 7), 4: slice(7, 9), 5: slice(9, 13), 6: slice(13, 15), 7: slice(15, 18), 8: slice(18, 25), 9: slice(25, 28), 10: slice(28, 31), 11: slice(31, 37), } def normalize(q): return q / (q.sum(dim=1) + eps)[:, None] def answer_probability(x): # Source: http://benanne.github.io/2014/04/05/galaxy-zoo.html # clip probabilities nb = x.shape[0] x = x.clamp_min(0.) # normalize q1 = normalize(x[:, task_sectors[1]]) q2 = normalize(x[:, task_sectors[2]]) q3 = normalize(x[:, task_sectors[3]]) q4 = normalize(x[:, task_sectors[4]]) q5 = normalize(x[:, task_sectors[5]]) q6 = normalize(x[:, task_sectors[6]]) q7 = normalize(x[:, task_sectors[7]]) q8 = normalize(x[:, task_sectors[8]]) q9 = normalize(x[:, task_sectors[9]]) q10 = normalize(x[:, task_sectors[10]]) q11 = normalize(x[:, task_sectors[11]]) # reweight w1 = 1.0 w2 = q1[:, 1] * w1 w3 = q2[:, 1] * w2 w4 = w3 w5 = w4 w6 = 1.0 w7 = q1[:, 0] * w1 w8 = q6[:, 0] * w6 w9 = q2[:, 0] * w2 w10 = q4[:, 0] * w4 w11 = w10 wq1 = w1*q1 wq2 = w2[:, np.newaxis]*q2 wq3 = w3[:, np.newaxis]*q3 wq4 = w4[:, np.newaxis]*q4 wq5 = w5[:, np.newaxis]*q5 wq6 = w6*q6 wq7 = w7[:, np.newaxis]*q7 wq8 = w8[:, np.newaxis]*q8 wq9 = w9[:, np.newaxis]*q9 wq10 = w10[:, np.newaxis]*q10 wq11 = w11[:, np.newaxis]*q11 return torch.cat([wq1, wq2, wq3, wq4, wq5, wq6, wq7, wq8, wq9, wq10, wq11], dim=1) . This code is pretty yuck so I will explain. task_sectors are the slices of the probability vector corresponding to the answers of each of the questions (tasks) in the decision tree. These sectors are all normalised individually, so they are &gt;=0 and sum to 1. . Training . One of the things I stuggled a lot in remedying in this problem was underfitting. With the default settings of CNNs in fastai (ps=0.5 and wd=1e-2) the validation loss was consistently lower than the training loss even after training for many epochs. The loss was also not improving over subsequent cycles. Here is an example of the loss plot for this case: . . According to Jeremy in lesson 2 of fastai, underfitting can be remedied with reducing regularization. After many experiments I settled on the following values: . ps=0.1 | wd=1e-4 | . I then trained the network using the freeze/unfreeze protocol taught in the fastai course and used progressive resizing to get the drive down the error further. To overcome underfitting I had to run many cycles until the validation error stopped being less than the training error. . Training Programme . Train head 2 epochs lr=5e-2 . | Unfreeze all layers . | Train 10 epochs lr=1e-4. Validation error here is ~0.081. . | Resize to 224x224 . | data = (src.transform(tfms, padding_mode=&#39;reflection&#39;) .databunch().normalize(imagenet_stats)) learner.data = data data.train_ds[0][0].shape . | Freeze all layers . | Train head 2 epochs lr=1e-2 . | Unfreeze . | learner.fit_one_cycle(6, 1e-4) . | learner.fit_one_cycle(6, 1e-4) . | learner.fit_one_cycle(6, 1e-4) . | learner.fit_one_cycle(8, 1e-4/5) . | Change dropout: ps=0.25 . | learner.fit_one_cycle(6, slice(1e-6, 1e-5/2)) . | The final training epochs: . epoch train_loss valid_loss root_mean_squared_error time . 0 | 0.005536 | 0.005953 | 0.077037 | 05:08 | . 1 | 0.005683 | 0.005960 | 0.077083 | 05:09 | . 2 | 0.005581 | 0.005979 | 0.077199 | 05:11 | . 3 | 0.005662 | 0.005976 | 0.077189 | 05:09 | . 4 | 0.005648 | 0.005932 | 0.076905 | 05:10 | . 5 | 0.005611 | 0.005942 | 0.076965 | 05:11 | . 6 | 0.005511 | 0.005919 | 0.076818 | 05:09 | . 7 | 0.005534 | 0.005906 | 0.076728 | 05:10 | . | . Total training time: ~200 minutes . Final Validation RMSE: 0.076728 . Summary and Further Thoughts . This competition was required a lot more work that I thought it would be, even with all the convenience of fastai. Before I started I expect this project to be a multi-label classification problem, but it’s actually a regression problem. Writing the normalization layer was tricky to figure out and in the end I learned a lot about PyTorch and fastai by writing this and appending it onto a pretrained network. . I believe my main issue was underfitting in this problem. I remedied this by reducing the regularisation and running for more epochs. In the future I will do further experiments to see if it can be fixed in another way with a larger network or different learning rate schedules. . The final validation RMSE is about the equal to what Dielemann achieved for a single model: http://benanne.github.io/2014/04/05/galaxy-zoo.html. I’m a bit disappointed that I couldn’t do better than the result from 5 years ago, but on the other hand the amount of code required to do this today compared to what Dielemann wrote is tiny. His final score was down at 0.074 after bagging the results of many CNNs. This is a huge amount of effort and was necessary to win the Kaggle competition at the time, however I feel this isn’t worth trying to reproduce this. . My Jupyter notebooks for this are on github: https://github.com/jimypbr/galaxyzoo-fastai .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/2019/09/20/galaxy-zoo-kaggle-competition-redux-with-fast-ai.html",
            "relUrl": "/deep-learning/machine-learning/fastai/2019/09/20/galaxy-zoo-kaggle-competition-redux-with-fast-ai.html",
            "date": " • Sep 20, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Fast.ai v3 Lesson 6 Notes: CNN Deep Dive; Ethics",
            "content": "Overview of the Lesson . This lesson starts with teaching the powerful techniques to avoid overfitting and decrease training time.: . Dropout: remove activations at random during training in order to regularize the model | Data augmentation: modify model inputs during training in order to effectively increase data size | Batch normalization: adjust the parameterization of a model in order to make the loss surface smoother. | . Next the lesson teaches convolutions, which can be thought of as a variant of matrix multiplication with tied weights, and are the operation at the heart of modern computer vision models (and, increasingly, other types of models too). . This knowledge is then used to create a class activated map, which is a heat-map that shows which parts of an image were most important in making a prediction. . Finally, the lesson ends with data ethics. . Platform.ai - Assisted Image Labeling . Jeremy showed in his TED talk in 2015 a cool demo where you can ‘collaborate’ with a pretrained neural network to label an unlabeled image dataset. | Basically it is a UI where the images are projected from the network into a 2D space (via T-SNE or similar). If the model is trained well then there will be good separation between the images in this space. | It is an iterative process where the user labels a few images, the network trains with these labels, the network then guesses the labels, and the user can correct these and label more images. Repeat. | Platform.ai is a product brought out by Jeremy that lets you do with your own image dataset that you upload. | . . This is similar to active learning. | . Tabular Data: Deep Dive . We want to understand every line of the code of TabularModel: . class TabularModel(Module): &quot;Basic model for tabular data.&quot; def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False): super().__init__() ps = ifnone(ps, [0]*len(layers)) ps = listify(ps, layers) self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs]) self.emb_drop = nn.Dropout(emb_drop) self.bn_cont = nn.BatchNorm1d(n_cont) n_emb = sum(e.embedding_dim for e in self.embeds) self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range sizes = self.get_sizes(layers, out_sz) actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None] layers = [] for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)): layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act) if bn_final: layers.append(nn.BatchNorm1d(sizes[-1])) self.layers = nn.Sequential(*layers) def get_sizes(self, layers, out_sz): return [self.n_emb + self.n_cont] + layers + [out_sz] def forward(self, x_cat:Tensor, x_cont:Tensor) -&gt; Tensor: if self.n_emb != 0: x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] x = torch.cat(x, 1) x = self.emb_drop(x) if self.n_cont != 0: x_cont = self.bn_cont(x_cont) x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont x = self.layers(x) if self.y_range is not None: x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0] return x . Model . The model for a tabular learner in fastai is like this one (source): . . In this model there is a categorical variable: words in real estate ad and there are two continuous variables: latitude and longitude. . The words in a real estate ad can be represented as a sparse vector of the word counts in the text. The network learns a lower dimensional embedding for these words as shown as the green layer in the diagram. . In pink is the actual ML model: it’s a simple multi-layer perceptron. After the categorical variables have been encoded by their embedding layers, these vectors are catted together along with the continuous variables to make one big vector; this is the input to the MLP. That’s all there is to the tabular learner. . In the fastai the code to create the tabular learner is: . learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=exp_rmspe) . What do these parameters mean? . layers is a list of ints, which specify the size of each of the layers in the MLP. Here it has two layers of size 1000 and 500 respectively. . | Now the intermediate weight matrix is going to have to go from a 1000 activation input to a 500 activation output, which means it’s going to have to be 500,000 elements in that weight matrix. That’s an awful lot for a data set with only a few hundred thousand rows. So this is going to overfit, and we need to make sure it doesn’t. The way to make sure it doesn’t is to use regularization; not to reduce the number of parameters. . | One way to do that will be to use weight decay which fast.ai will use automatically, and you can vary it to something other than the default if you wish. It turns out in this case, we’re going to want more regularization. . | The parameter ps provides something called dropout. . | Also the parameter emb_drop provides dropout to just the embeddings. . | . Dropout . Dropout is a kind of regularization. This is the dropout paper. . The diagram from the paper illustrates perfectly what is going on: . . For dropout, we throw that away. At random, we throw away some percentage of the activations. . N.B. it doesn’t zero the weights/parameters. (Remember, there’s only two types of layer in a neural net - parameters and activations). . We throw each one away with a probability p. A common value of p is 0.5. . It means that no one activation can memorize some part of the input because that’s what happens if we over fit. If we over fit, some part of the model is basically learning to recognize a particular image rather than a feature in general or a particular item. This forces the network to use more neurons to determine the outcome and so the network is more likely to learn the actual patterns in the data, rather than trying to short-circuit the problem by memorizing the data. . During backpropagation, the gradients for the zeroed out neurons are also zero. . Check out this quote from one of the creators, Geoffry Hinton: . I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting. . Hinton: Reddit AMA . Dropout stops your neural network from conspiring against you! Dropout is a technique that works really well, and has become standard practice in training neural networks. . In fastai nearly every learner has a parameter ps for defining how much dropout to use (number between 0 and 1). . Dropout: Training versus test time: . There is an interesting feature of dropout regarding training time and test time (AKA inference time). Training time is when we’re actually updating the weights - doing backpropagation etc. During training time, dropout works the way we just saw. . At test time however we turn off dropout. We’re not going to do dropout anymore because we want it to be as accurate as possible. It’s not updating any weights at test time so overfitting obviously isn’t an issue. But there is a small issue here. If previously p was set to 0.5, then half the activations were being removed. Which means when we turn them all back on again, now our overall activation level is twice what it used to be. Therefore, in the paper, they suggest multiplying all of the weights affect by dropout at test time by p. . You could alternatively scale things at training time instead, except you would scale the activations and gradients of the non-zeroed neurons by $ frac{1}{1-p}$ . (Source). . Dropout in Tabular Learner . Looking again at the tabular learner: . learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=exp_rmspe) . In this case: . Dropout of 0.001 on layer 1. | Dropout of 0.01 on layer 2. | Then some dropout of 0.04 on the embedding layers. | . What is the embedding dropout actually doing? Look at the source code of the forward method again specifically at the embedding part: . def forward(self, x_cat:Tensor, x_cont:Tensor) -&gt; Tensor: if self.n_emb != 0: x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] x = torch.cat(x, 1) x = self.emb_drop(x) ... . It calls each embedding | It concatenates the embeddings in a single matrix (batch of vectors) | It calls dropout on that | . The output of an embedding layers is basically a big vector so we can think of it as just another layer in the neural network and so just call dropout on that like we normally would. . Here is the TabularModel for the Rossmann dataset: . TabularModel( (embeds): ModuleList( (0): Embedding(1116, 50) (1): Embedding(8, 5) (2): Embedding(4, 3) (3): Embedding(13, 7) (4): Embedding(32, 17) (5): Embedding(3, 2) (6): Embedding(26, 14) (7): Embedding(27, 14) (8): Embedding(5, 3) (9): Embedding(4, 3) (10): Embedding(4, 3) (11): Embedding(24, 13) (12): Embedding(9, 5) (13): Embedding(13, 7) (14): Embedding(53, 27) (15): Embedding(22, 12) (16): Embedding(7, 4) (17): Embedding(7, 4) (18): Embedding(4, 3) (19): Embedding(4, 3) (20): Embedding(9, 5) (21): Embedding(9, 5) (22): Embedding(3, 2) (23): Embedding(3, 2) ) (emb_drop): Dropout(p=0.04) (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=229, out_features=1000, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.001) (4): Linear(in_features=1000, out_features=500, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.01) (8): Linear(in_features=500, out_features=1, bias=True) ) ) . There are 24 categorical variables and so 24 embedding layers. Embedding(53, 27) means that there are 52+1 possible values (+1 is #na#) and the size of the embedding is 27D. . There are also these extra layers in there BatchNorm1d too. These are batch normalization, another standard regularization technique. . Batch Normalization . Batch norm is a very high impact training technique that was published in 2015. . . Showing the current then state of the art ImageNet model Inception. This is how long it took them to get a pretty good result, and then they tried the same thing with batch norm, and it was a lot faster. . From the abstract of the original paper: . Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates… . Batch Normalization layer adjusts the distribution of the output of a layer by controlling the the first two moments of the layer distributions (mean and standard deviation). This allows networks to be trained with a higher learning rate (so they train faster) and with more layers. . The algorithm: . . You have the activations from the layer $x$ going into the batch norm layer. . The first thing we do is we find the mean with those activations - sum divided by the count that is just the mean. | The second thing we do is we find the variance of those activations - a difference squared divided by the mean is the variance. | Then we normalize - the values minus the mean divided by the standard deviation is the normalized version. It turns out that bit is actually not that important. We used to think it was - it turns out it’s not. The really important bit is the next bit. | We take those values and we add a vector of biases (they call it beta here). We’ve seen that before. We’ve used a bias term before. So we’re just going to add a bias term as per usual. Then we’re going to use another thing that’s a lot like a bias term, but rather than adding it, we’re going to multiply by it. These are the parameters gamma $ gamma$ and beta $ beta$ which are learnable parameters. | Basically $ gamma$ and $ beta$ are biases. $ beta$ is just a normal bias layer and $ gamma$ is a multiplicative/scale bias layer. They are parameters and so they are learned with gradient descent. . Roughly speaking, this works by scaling a layer’s output to the size and location it needs to be in (like between 0 and 5 for a movie review). This is harder to do with just stacks of non-linear functions because of all the intricate interactions between them. Navigating that complex landscape is hard and there will be many bumps in the road. . There is one more aspect to batch norm - momentum: . BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) . This has nothing to do with momentum in optimization. This is momentum as in exponentially weighted moving average. Specifically this mean and standard deviation (in batch norm algorithm), we don’t actually use a different mean and standard deviation for every mini batch. If we did, it would vary so much that it be very hard to train. So instead, we take an exponentially weighted moving average of the mean and standard deviation. . Why Batch Normalization works is actually still a debated topic. . In the original paper they said it accelerates training by reducing something they call ‘internal covariate shift’. This is one of those things where researchers came up with some intuition and some idea about this thing they wanted to try and found that it worked well. They then look for an explanation after the fact. So the original explanation for why it works may well be wrong. In this paper - How Does Batch Normalization Help Optimization? - they have an alternative explanation: . . The above is from this paper. The plot represents the ‘loss landscape’ of the network during training. The red line is what happens whn you train without Batch Norm - very very bumpy. The blue line is training with batch norm - a lot smoother. If the loss landscape is very bumpy then your model can get trapped in some awful region of parameter space that it can’t escape from. If it is smoother then you can train with a higher learning rate and hence converge faster. . Other points of view: . An influential twitter thread on how Batch Norm works that vindicates the Internal Covariate Shift explanation: Twitter thread. | Blog post with analysis of the different points of view: https://arthurdouillard.com/post/normalization/ | . Why it works is still debatable and I need to read more into this, but this aside, it seems that the output distribution of the hidden layers in the network is very important for training networks more quickly and with more layers. We already know that these internal distributions are very important for training because of all the research done into the art of initializing neural networks when training from scratch. Getting this wrong can prevent the network from training at all by making gradients vanish or explode. So with this in mind, it makes sense that adjusting these distributions as data flows through the network could improve training. . Data Augmentation . One of the most effective and least studied forms of regularization is data augmentation. . Here is a link to the notebook that explores data augmentation in computer vision: lesson6-pets-more.ipynb. . I recommend reading the fastai documentation on data augmentation for computer vision: vision.transform. . In particular, read the list of transforms. . The data augmentation you pick should be realistic of what you expect in the dataset and problem domain. | . How do convolutions work? . Convolutions are like a special kind of matrix multiply. . Checkout the website: http://setosa.io/ev/image-kernels/: . . Post from Matthew Kleinsmith: CNNs from Different Viewpoints. This is a wonderfully concise explanation with great diagrams and hardly any text. The following diagrams are from that post. . Sliding window view: . . You can alternatively think of it as a a set of linear equations. . . You can also think of it as a fully connected neural network. In the following the colour of the links stand for their weight, and the gray links are 0. . . You can also interpret it as a matrix multiply: . . Banded matrix multiply where the colours again stand for the weights. $b$ is a bias term. . We have to also consider padding: . . This diagram uses zero padding, but it could be reflection padding or whatever. . So a single convolution kernel is a small matrix of weights (typical sized 3 to 7) and a bias. In a convolutional layer the same convolution is applied to every channel of input. If you take example of a colour image there the image is 3x224x224 in size. The 2D convolutional kernal will be applied to all 3 channels simultaneously and the results from all 3 is summed to produce a single number for each pixel. . . If you have multiple convolutions then you have multiple different outputs. We stack these together to make another tensor: . . This output can also be fed into another convolution layer, and so on. . In order to avoid our memory going out of control, from time to time we create a convolution where we don’t step over every single set of 3x3, but instead we skip over two at a time. We would start with a 3x3 centered at (2, 2) and then we’d jump over to (2, 4), (2, 6), (2, 8), and so forth. That’s called a stride 2 convolution. What that does is, it looks exactly the same, it’s still just a bunch of kernels, but we’re just jumping over 2 at a time. We’re skipping every alternate input pixel. So the output from that will be H/2 by W/2. When we do that, we generally create twice as many kernels, so we can now have 32 activations in each of those spots. That’s what modern convolutional neural networks tend to look like. . The learn.summary() of a resnet it looks like this: . ====================================================================== Layer (type) Output Shape Param # Trainable ====================================================================== Conv2d [64, 176, 176] 9,408 False ______________________________________________________________________ BatchNorm2d [64, 176, 176] 128 True ______________________________________________________________________ ReLU [64, 176, 176] 0 False ______________________________________________________________________ MaxPool2d [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ ReLU [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ ReLU [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ ReLU [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ Conv2d [128, 44, 44] 73,728 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 8,192 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [256, 22, 22] 294,912 False ______________________________________________________________________ BatchNorm2d [256, 22, 22] 512 True . There are stacks of convolutional layers and every so often it halves the grid size and doubles the number of channels. . Manual Convolution . Input image: . . Convolve with tensor: . k = tensor([ [0. ,-5/3,1], [-5/3,-5/3,1], [1. ,1 ,1], ]).expand(1,3,3,3)/6 . The images has 3 channels so we need to expand the tensor to replicate the kernel 3 times. We also add in an additional unit dimension because PyTorch expects to work with mini-batches always so this way it has the right tensor rank. . You then take that image t and convolve it using PyTorch: . edge = F.conv2d(t[None], k) . . CNN Heatmap Example . . This is covered in the notebook: lesson6-pets-more.ipynb. . Ethics and Data Science . I won’t cover this here. Instead just watch the video, it’s great: ethics and data science (YouTube) Also here is the transcription of this section by @hiromi . Jeremy Says… . Not an explicit “do this” but it feels like it fits here. “One of the big opportunities for research is to figure out how to do data augmentation for different domains. Almost nobody is looking at that and to me it is one of the biggest opportunities that could let you decrease data requirements by 5-10x.” Lesson 6: Data augmentation on inputs that aren’t images 26 | In context of data augmentation: reflection mode padding almost always works best. | If you take your time going through the convolution kernel section and the heatmap section of this notebook, running those lines of code and changing them around a bit. The most important thing to remember is shape (rank and dimensions of tensor). Try to think “why?”. Try going back to the printout of the summary, the list of the actual layers, the picture we drew and think about what’s going on. Lesson 6: Go through the convolution kernel and heatmap notebook 11 | (Source: Robert Bracco) . Q &amp; A . In what proportion would you use dropout vs. other regularization errors, like, weight decay, L2 norms, etc.? [54:49]: . So remember that L2 regularization and weight decay are kind of two ways of doing the same thing? We should always use the weight decay version, not the L2 regularization version. So there’s weight decay. There’s batch norm which kind of has a regularizing effect. There’s data augmentation which we’ll see soon, and there’s dropout. So batch norm, we pretty much always want. So that’s easy. Data augmentation, we’ll see in a moment. So then it’s really between dropout versus weight decay. I have no idea. I don’t think I’ve seen anybody to provide a compelling study of how to combine those two things. Can you always use one instead of the other? Why? Why not? I don’t think anybody has figured that out. I think in practice, it seems that you generally want a bit of both. You pretty much always want some weight decay, but you often also want a bit of dropout. But honestly, I don’t know why. I’ve not seen anybody really explain why or how to decide. So this is one of these things you have to try out and kind of get a feel for what tends to work for your kinds of problems. I think the defaults that we provide in most of our learners should work pretty well in most situations. But yeah, definitely play around with it. . | . Links and References . Link to Lesson 6 lecture. | Parts of my notes were copied from the excellent lecture transcriptions made by @hiromi: Detailed lesson notes. | Homework notebooks: lesson6-rossmann.ipynb | rossman_data_clean.ipynb | lesson6-pets-more.ipynb | . | CNNs from Different Viewpoints | Blog post about different kinds of Normalization. | Cross-entropy loss | Lecture on BackProp going deeper into how it works from A. Karpathy: https://www.youtube.com/watch?v=i94OvYb6noo | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/09/01/fast-ai-lesson-6-notes-cnn-deep-dive-ethics.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/09/01/fast-ai-lesson-6-notes-cnn-deep-dive-ethics.html",
            "date": " • Sep 1, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Fast.ai v3 Lesson 5 Notes: Backpropagation; Neural Network From Scratch",
            "content": "Overview of the Lesson . This lesson looks at the fundament components of deep learning - parameters, activations, backpropagation, transfer learning, and discriminative learning rates. . Components of Deep Learning . Roughly speaking, this is the bunch of concepts that we need to learn about - . Inputs | Weights/parameters | Random | Activations | Activation functions / nonlinearities | Output | Loss | Metric | Cross-entropy | Softmax | Fine tuning | Layer deletion and random weights | Freezing &amp; unfreezing | . Diagram of a neural network: . . There are three types of layer in a NN: . Input. . | Weights/Parameters: These are layers that contain parameters or weights. These are things like matrices or convolutions. Parameters are used by multiplying them by input activations doing a matrix product. The yellow things in the above diagram are our weight matrices / weight tensors. Parameters are the things that your model learns in train via gradient descent: . weights = weights - learning_rate * weights.grad . | Activations: These are layers that contain activations, also called as non-linear layers which are stateless. For example, ReLu, softmax, or sigmoid. . | Here is the process of input, weight multiplication, and activation up close (image credit): . . The parameters/weights are the matrix $ mathbf{W}$, the input is the vector $x$, and there is also the bias vector $b$. This can be expressed mathematically as: ( mathbf{a} = g( mathbf{W^T} mathbf{x} + mathbf{b}) ) Let’s get an intuition for how the dimension of the data changes as it flows through the network. In the diagram above there is an input vector of size 4 and a weight matrix of size 4x3. The matrix vector product in terms of just the dimensions is: $(4, 3)^T cdot (4) = (3, 4) cdot (4) = (3)$. . In summation notation this is $(W_{ji})^Tx_j = W_{ij} x_j = a_i$. The $j$ terms are summed out and we are left with $i$ dimension only. . The activation function is an element-wise function. It’s a function that is applied to each element of the input, activations in turn and creates one activation for each input element. If it starts with a twenty long vector it creates a twenty long vector by looking at each one of those, in turn, doing one thing to it and spitting out the answer, so an element-wise function. These days the activation function most often used is ReLu. . Backpropagation . After the loss has been calculated from the different between the output and the ground truth, how are millions of parameters in the network then updated? This is done by a clever algorithm called backpropogation. This algorithm calculates the partial derivatives of the loss with respect to every parameter in the network. It does this using the chain-rule from calculus. The best explanation of this I’ve seen is from Chris Olah’s blog. . In PyTorch, these derivatives are calculated automatically for you (aka autograd) and the gradient of any PyTorch variable is stored in its .grad attribute. . Mathematically the weights are updated like so: (w_t = w_{t-1} - lr times frac{ partial L}{ partial w_{t-1}} ) . How is Transfer Learning Done? . What happens when we take a resnet34 trained on ImageNet and we do transfer learning? How can a network that is trained to identify 1000 different everyday objects be repurposed for, say, identifying galaxies? . Let’s look at some examples we’ve seen already. Here are is the last layer group of the resnet34 used in the dog/cat breed example: . (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5) (8): Linear(in_features=512, out_features=37, bias=True) ) . And here is the same layer group from the head pose example: . (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5) (8): Linear(in_features=512, out_features=2, bias=True) ) . The two layers that change here are (4) and (8). . Layer (4) is a matrix with some default size that is the same in both cases. It anyway needs to be relearned for a new problem. | Layer (8) is different in out_features. This is the dimensionality of the output. There are 37 cat/dog breeds in the first example, and there are x and y coordinates in the second. Layer (8) is a Linear layer – i.e. it’s a matrix! | . So to do transfer learning we just need to swap out the last matrix with a new one. . . For an imagenet network this matrix would have shape (512, 1000), for the cat/dog (512, 37), and for the head pose (512, 2). Every row in this matrix represents a category you are predicting. . Fastai library figures out the right size for this matrix by looking at the data bunch you passed to it. . Freezing Layers . . The new head of the network has two randomly initialized matrices. They need to be trained because they are random, however the rest of the network is quite well trained on imagenet already - they are vastly better than random even though they weren’t trained on the task at hand. . So we freeze all the layers before the head, which means we don’t update their parameters during training. It’ll be a little bit faster because there are fewer calculations to do, and also it will save some memory because there are fewer gradients to store. . After training a few epochs with just the head unfrozen, we are ready to train the whole network. So we unfreeze everything. . Discriminative Learning Rates . Now we’re going to train the whole thing but we still have a pretty good sense that these new layers we added to the end probably need more training and these ones right at the start that might just be like diagonal edges probably don’t need much training at all. . Let’s review what the different layers in a CNN learn to do first. The first layer visualized looks like this: . . This layer is good at finding diagonal lines in different directions. . . In layer 2 some of the filters were good at spotting corners in the bottom right corner. . . In layer 3, one of the filters found repeating patterns or round orange things or fluffy or floral textures. . . As we go deeper, they’re becoming more sophisticated but also more specific. By layer 5, It could find bird eyeballs or dog faces. . If you’re wanting to transfer and learn to something for galaxy morphology there’s probably going to be no eyeballs in that dataset. So the later layers are no good to you but there will certainly be some repeating patterns or some diagonal edges. The earlier a layers is in the pretrained model the more likely it is that you want those weights to stay as they are. . We can implement this by splitting the model into a few sections and giving the earlier sections slower learning rates. Earlier layers of the model we might give a learning rate of 1e - 5 and newly added layers of the model we might give a learning rate of 1e - 3. What’s gonna happen now is that we can keep training the entire network. But because the learning rate for the early layers is smaller it’s going to move them around less because we think they’re already pretty good. If it’s already pretty good to the optimal value if you used a higher learning rate it could kick it out. It could actually make it worse which we really don’t want to happen. . In fastai this is done with any of the following lines of code: . learn.fit_one_cycle(5, 1e-3) learn.fit_one_cycle(5, slice(1e-3)) learn.fit_one_cycle(5, slice(1e-5, 1e-3)) . These mean: . A single number like 1e-3: Just using a single number means every layer gets the same learning rate so you’re not using discriminative learning rates. | A slice with a single number slice(1e-3):If you pass a single number to slice it means the final layers get a learning rate of 1e-3 and then all the other layers get the same learning rate which is that divided by 3. All of the other layers will be (1e-3)/3 and the last layers will be 1e-3. | A slice with two numbers, slice(1e-5, 1e-3) In the last case, the final layers the these randomly hidden added layers will still be again 1e-3. The first layers will get 1e-5. The other layers will get learning rates that are equally spread between those two. Multiplicatively equal. If there were three layers there would be 1e-5, 1e-4 and 1e-3. Multiplied by the same factor between layers each time. | This divided by 3 thing that is a little weird and we won’t talk about why that is until part two of the course. it is specific quirk around batch normalization. . Collaborative Filtering: Deep Dive . Recall from the previous lesson the movie recommendation problem. . . For the movielens dataset the model looks like: . $&gt; learn.model EmbeddingDotBias( (u_weight): Embedding(944, 40) (i_weight): Embedding(1654, 40) (u_bias): Embedding(944, 1) (i_bias): Embedding(1654, 1) ) . We already know what u_weight and i_weight are: the user and movie embedding matrices, respectively. What are the additional components of the model: u_bias and i_bias? . Interpreting Bias . These are the 1D bias terms for the movies and films. In the model diagram above these are the two edge nodes on the left and right that connect to the ADD node. . Every user and every movie has its own bias term. . You can interpret user bias as how much that user likes movies. . | You can interpret movie bias as how well liked a movie is in general. (Ironically, the bias is the unbiased movie score). . | . Here are the movies with the highest bias alongside their actual movie rating: . movie_bias = learn.bias(top_movies, is_item=True) . Top 10: . [(tensor(0.6105), &quot;Schindler&#39;s List (1993)&quot;, 4.466442953020135), (tensor(0.5817), &#39;Titanic (1997)&#39;, 4.2457142857142856), (tensor(0.5685), &#39;Shawshank Redemption, The (1994)&#39;, 4.445229681978798), (tensor(0.5451), &#39;L.A. Confidential (1997)&#39;, 4.161616161616162), (tensor(0.5350), &#39;Rear Window (1954)&#39;, 4.3875598086124405), (tensor(0.5341), &#39;Silence of the Lambs, The (1991)&#39;, 4.28974358974359), (tensor(0.5330), &#39;Star Wars (1977)&#39;, 4.3584905660377355), (tensor(0.5227), &#39;Good Will Hunting (1997)&#39;, 4.262626262626263), (tensor(0.5114), &#39;As Good As It Gets (1997)&#39;, 4.196428571428571), (tensor(0.4800), &#39;Casablanca (1942)&#39;, 4.45679012345679), (tensor(0.4698), &#39;Boot, Das (1981)&#39;, 4.203980099502488), (tensor(0.4589), &#39;Close Shave, A (1995)&#39;, 4.491071428571429), (tensor(0.4567), &#39;Apt Pupil (1998)&#39;, 4.1), (tensor(0.4566), &#39;Vertigo (1958)&#39;, 4.251396648044692), (tensor(0.4542), &#39;Godfather, The (1972)&#39;, 4.283292978208232)] . Bottom 10: . [(tensor(-0.4076), &#39;Children of the Corn: The Gathering (1996)&#39;, 1.3157894736842106), (tensor(-0.3053), &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, 1.7142857142857142), (tensor(-0.2892), &#39;Cable Guy, The (1996)&#39;, 2.339622641509434), (tensor(-0.2856), &#39;Mortal Kombat: Annihilation (1997)&#39;, 1.9534883720930232), (tensor(-0.2530), &#39;Striptease (1996)&#39;, 2.2388059701492535), (tensor(-0.2405), &#39;Free Willy 3: The Rescue (1997)&#39;, 1.7407407407407407), (tensor(-0.2361), &#39;Showgirls (1995)&#39;, 1.9565217391304348), (tensor(-0.2332), &#39;Bio-Dome (1996)&#39;, 1.903225806451613), (tensor(-0.2279), &#39;Crow: City of Angels, The (1996)&#39;, 1.9487179487179487), (tensor(-0.2273), &#39;Barb Wire (1996)&#39;, 1.9333333333333333), (tensor(-0.2246), &quot;McHale&#39;s Navy (1997)&quot;, 2.1884057971014492), (tensor(-0.2197), &#39;Beverly Hills Ninja (1997)&#39;, 2.3125), (tensor(-0.2178), &quot;Joe&#39;s Apartment (1996)&quot;, 2.2444444444444445), (tensor(-0.2080), &#39;Island of Dr. Moreau, The (1996)&#39;, 2.1578947368421053), (tensor(-0.2064), &#39;Tales from the Hood (1995)&#39;, 2.037037037037037)] . Having seen many of the films in both these lists, I’m not not surprise by what I see here! . Interpreting Weights (the Embeddings) . Grab the weights: . $&gt; movie_w = learn.weight(top_movies, is_item=True) $&gt; movie_w.shape torch.Size([1000, 40]) . There are 40 factors in the model. Looking at the 40 latent factors isn’t intuitive or necessarily meaningful so it’s better to squish the 40 factors down to 3 - using Principle Component Analysis (PCA). . $&gt; movie_pca = movie_w.pca(3) $&gt; movie_pca.shape torch.Size([1000, 3]) . We can now rank each of the films along these 3 dimensions, knowing film, interpret what these dimensions mean. . Factor 0 . fac0,fac1,fac2 = movie_pca.t() movie_comp = [(f, i) for f,i in zip(fac0, top_movies)] . Top 10: . [(tensor(1.1000), &#39;Wrong Trousers, The (1993)&#39;), (tensor(1.0800), &#39;Close Shave, A (1995)&#39;), (tensor(1.0705), &#39;Casablanca (1942)&#39;), (tensor(1.0304), &#39;Lawrence of Arabia (1962)&#39;), (tensor(0.9957), &#39;Citizen Kane (1941)&#39;), (tensor(0.9792), &#39;Some Folks Call It a Sling Blade (1993)&#39;), (tensor(0.9778), &#39;Persuasion (1995)&#39;), (tensor(0.9752), &#39;North by Northwest (1959)&#39;), (tensor(0.9706), &#39;Wallace &amp; Gromit: The Best of Aardman Animation (1996)&#39;), (tensor(0.9703), &#39;Chinatown (1974)&#39;)] . Bottom 10: . [(tensor(-1.2963), &#39;Home Alone 3 (1997)&#39;), (tensor(-1.2210), &quot;McHale&#39;s Navy (1997)&quot;), (tensor(-1.2199), &#39;Leave It to Beaver (1997)&#39;), (tensor(-1.1918), &#39;Jungle2Jungle (1997)&#39;), (tensor(-1.1209), &#39;D3: The Mighty Ducks (1996)&#39;), (tensor(-1.0980), &#39;Free Willy 3: The Rescue (1997)&#39;), (tensor(-1.0890), &#39;Children of the Corn: The Gathering (1996)&#39;), (tensor(-1.0873), &#39;Bio-Dome (1996)&#39;), (tensor(-1.0436), &#39;Mortal Kombat: Annihilation (1997)&#39;), (tensor(-1.0409), &#39;Grease 2 (1982)&#39;)] . Interpretation: This dimension is best described as ‘connoisseur movies’. . Factor 1 . Top 10: . [(tensor(1.1052), &#39;Braveheart (1995)&#39;), (tensor(1.0759), &#39;Titanic (1997)&#39;), (tensor(1.0202), &#39;Raiders of the Lost Ark (1981)&#39;), (tensor(0.9324), &#39;Forrest Gump (1994)&#39;), (tensor(0.8627), &#39;Lion King, The (1994)&#39;), (tensor(0.8600), &quot;It&#39;s a Wonderful Life (1946)&quot;), (tensor(0.8306), &#39;Pretty Woman (1990)&#39;), (tensor(0.8271), &#39;Return of the Jedi (1983)&#39;), (tensor(0.8211), &quot;Mr. Holland&#39;s Opus (1995)&quot;), (tensor(0.8205), &#39;Field of Dreams (1989)&#39;)] . Bottom 10: . [(tensor(-0.8085), &#39;Nosferatu (Nosferatu, eine Symphonie des Grauens) (1922)&#39;), (tensor(-0.8007), &#39;Brazil (1985)&#39;), (tensor(-0.7866), &#39;Trainspotting (1996)&#39;), (tensor(-0.7703), &#39;Ready to Wear (Pret-A-Porter) (1994)&#39;), (tensor(-0.7173), &#39;Beavis and Butt-head Do America (1996)&#39;), (tensor(-0.7150), &#39;Serial Mom (1994)&#39;), (tensor(-0.7144), &#39;Exotica (1994)&#39;), (tensor(-0.7129), &#39;Lost Highway (1997)&#39;), (tensor(-0.7094), &#39;Keys to Tulsa (1997)&#39;), (tensor(-0.7083), &#39;Jude (1996)&#39;)] . Interpretation: This dimension I think can be best described as ‘blockbuster’, or ‘family’. . Factor 2 . Top 10: . [(tensor(1.0152), &#39;Beavis and Butt-head Do America (1996)&#39;), (tensor(0.8703), &#39;Reservoir Dogs (1992)&#39;), (tensor(0.8640), &#39;Pulp Fiction (1994)&#39;), (tensor(0.8582), &#39;Terminator, The (1984)&#39;), (tensor(0.8572), &#39;Scream (1996)&#39;), (tensor(0.8058), &#39;Terminator 2: Judgment Day (1991)&#39;), (tensor(0.7728), &#39;Seven (Se7en) (1995)&#39;), (tensor(0.7457), &#39;Starship Troopers (1997)&#39;), (tensor(0.7243), &#39;Clerks (1994)&#39;), (tensor(0.7227), &#39;Die Hard (1988)&#39;)] . Bottom 10: . [(tensor(-0.6912), &#39;Lone Star (1996)&#39;), (tensor(-0.6720), &#39;Jane Eyre (1996)&#39;), (tensor(-0.6613), &#39;Steel (1997)&#39;), (tensor(-0.6227), &#39;Piano, The (1993)&#39;), (tensor(-0.6183), &#39;My Fair Lady (1964)&#39;), (tensor(-0.5946), &#39;Evita (1996)&#39;), (tensor(-0.5827), &#39;Home for the Holidays (1995)&#39;), (tensor(-0.5669), &#39;Cinema Paradiso (1988)&#39;), (tensor(-0.5668), &#39;All About Eve (1950)&#39;), (tensor(-0.5586), &#39;Sound of Music, The (1965)&#39;)] . Regularization: Weight Decay . . On the left the model isn’t complex enough. One the right the model has more parameters and is too complex. The middle model has just the right level of complexity. . In Jeremy’s opinion, to reduce model complexity: . In statistics: they reduce the number of parameters. Jeremy thinks this is wrong. | In machine learning: they increase the number of parameters, but increase the amount of regularization. | . The regularization used in fastai is weight decay (aka L2 regularization). This is where the loss function is modified with an extra component that penalizes extreme values of the weights. . Unregularized loss: | . [ begin{align} L(x, w) &amp;= mse( hat{y}, y) nonumber &amp;= mse(m(x, w), y) . end{align}] . L2 regularized loss: | . [L(x, w) = mse(m(x,w), y) + lambda sum w^2] . The weight decay parameter $ lambda$ corresponds to the fastai parameter to the Learner model: wd . This is set by default to 0.01. . Higher values of wd give more regularlization. Looking at the diagram of line fits above, more regularlization makes the line ‘stiffer’ - harder to force through every point. . | Lower values of wd, conversely, make the line ‘slacker’ - it’s easier to force it through every point. . | It’s worth experimenting with wd if your model is overfitting or underfitting. . | . With weight decay the update formula for the weights in gradient descent becomes: . [w_t = w_{t-1} - lr times frac{ partial L}{ partial w_{t-1}} - lr times lambda w_{t-1}] . (Source) . Jeremy Says… . The answer to the question “Should I try blah?” is to try blah and see, that’s how you become a good practitioner. Lesson 5: Should I try blah? | If you want to play around, try to create your own nn.linear class. You could create something called My_Linear and it will take you, depending on your PyTorch experience, an hour or two. We don’t want any of this to be magic and you know everything necessary to create this now. These are the things you should be doing for assignments this week, not so much new applications but trying to write more of these things from scratch and get them to work. Learn how to debug them and check them to see what’s going in and coming out. Lesson 5 Assignment: Create your own version of nn.linear | A great assignment would be to take Lesson 2 SGD and try to add momentum to it. Or even the new notebook we have for MNIST, get rid of the Optim.SGD and write your own update function with momentum Lesson 5: Another suggested assignment | It’s definitely worth knowing that taking layers of neural nets and chucking them through PCA is very often a good idea. Because very often you have way more activations than you want in a layer, and there’s all kinds of reasons you would might want to play with it. For example, Francisco who’s sitting next to me today has been working on something to do with image similarity. And for image similarity, a nice way to do that is to compare activations from a model, but often those activations will be huge and therefore your thing could be really slow and unwieldy. So people often, for something like image similarity, will chuck it through a PCA first and that’s kind of cool. | (Source - Robert Bracco) . Q &amp; A . When we load a pre-trained model, can we explore the activation grids to see what they might be good at recognizing? [36:11] . Yes, you can. And we will learn how to (should be) in the next lesson. . | Can we have an explanation of what the first argument in fit_one_cycle actually represents? Is it equivalent to an epoch? . Yes, the first argument to fit_one_cycle or fit is number of epochs. In other words, an epoch is looking at every input once. If you do 10 epochs, you’re looking at every input ten times. So there’s a chance you might start overfitting if you’ve got lots of lots of parameters and a high learning rate. If you only do one epoch, it’s impossible to overfit, and so that’s why it’s kind of useful to remember how many epochs you’re doing. . | What is an affine function? . An affine function is a linear function. I don’t know if we need much more detail than that. If you’re multiplying things together and adding them up, it’s an affine function. I’m not going to bother with the exact mathematical definition, partly because I’m a terrible mathematician and partly because it doesn’t matter. But if you just remember that you’re multiplying things together and then adding them up, that’s the most important thing. It’s linear. And therefore if you put an affine function on top of an affine function, that’s just another affine function. You haven’t won anything at all. That’s a total waste of time. So you need to sandwich it with any kind of non-linearity pretty much works - including replacing the negatives with zeros which we call ReLU. So if you do affine, ReLU, affine, ReLU, affine, ReLU, you have a deep neural network. . | Why am I sometimes getting negative loss when training? [59:49] . You shouldn’t be. So you’re doing something wrong. Particularly since people are uploading this, I guess other people have seen it too, so put it on the forum. We’re going to be learning about cross entropy and negative log likelihood after the break today. They are loss functions that have very specific expectations about what your input looks like. And if your input doesn’t look like that, then they’re going to give very weird answers, so probably you press the wrong buttons. So don’t do that. . | . Links and References . Link to Lesson 5 lecture | Parts of my notes were copied from the excellent lecture transcriptions made by @PoonamV: Lecture notes | Homework notebooks: Notebook 1: SGD and MNIST | Notebook 2: Rossmann (tabular data) | . | Netflix and Chill: Building a Recommendation System in Excel - Latent Factor Visualization in Excel blog post | An overview of gradient descent optimization algorithms - Sebastian Ruder | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/29/fast-ai-lesson-5-notes-backpropagation-neural-network-from-scratch.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/29/fast-ai-lesson-5-notes-backpropagation-neural-network-from-scratch.html",
            "date": " • Aug 29, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Fast.ai v3 Lesson 4 Notes: NLP; Tabular Data; Recommenders",
            "content": "Overview of the Lesson . The first part of this lesson dives into natural language processing (NLP), using the IMDB movie review dataset. We train a classifier that categorises if a review is negative or positive. This is called sentiment analysis. This is done via a state-of-the-art NLP algorithm called ULMFiT. . Next the lesson shows how to use deep learning with tabular data using fastai. . Lastly the lesson shows how collaborative filtering models (aka recommender systems) can be built using similar ideas to those for tabular data, but with some special tricks to get both higher accuracy and more informative model interpretation. . Natural Language Processing (NLP) . We want to build a NLP classifier. | Task: IMDB movie reviews - postive or negative? | Using neural networks for NLP classification hasn’t been successful until a break through made in 2018 – ULMFit. This is what FastAI is using now. | . . Just as we have seen already in imaging problems, we can get good performance by using transfer learning. | In NLP transfer learning means taking a language model which has be pretrained on some large corpus of text and then fine tuning that for our current problem using its own text corpus. | . Language Model . The language model in this case is a special type of neural network called an RNN (recurrent neural network) and what it does is predict the next word given a sequence of prior words. So in the diagram above you have the sentences: “I’d like to eat a hot [ ]” : the language model should predict “dog” | “It was a hot [ ]” : the language model should predict “day” | . | This takes 2-3 days to train on a decent GPU, so not much point in you doing it. You may as well start with ours. Even if you’ve got a big corpus of like medical documents or legal documents, you should still start with Wikitext 103. There’s just no reason to start with random weights. It’s always good to use transfer learning if you can. | Once you have trained your language model you can stick it on the internet (e.g. github) for others to download and use for their own NLP problems. fastai provides a pretrain language model trained on text from Wikipedia. | This kind of learning is what Yann Lecun calls “Self-supervised Learning”. You don’t give the dataset labels, rather the labels are built into the data itself. | . Fine Tuning the Language Model . Starting from the pretrained Wikitext language model you can fine tune the language model with your own target corpus. Every domain that you work in will have its own domain specific language that it uses. . | For the case of movie reviews it may learn about actor’s names or certain vocabulary will be more important. For example: “My favourite actor is Tom ___ (Cruise)” | “I thought the photography was fantastic but I wasn’t really so happy about the _____ (director).” | . | Fine tuning your language model will take a long time. However this is basically a one-time cost. You only have to train the language model once and then you can use that model for training classifiers or whatever, which won’t take a long time to train. | This transfer learning approach works very well and gives state of the art performance on the IMDB dataset. | . IMDB Sentiment Classification . The data loading process for text was covered in the previous lesson. Here is a short review: . Load data using a data bunch or the data block API | The data is tokenized: this means that text is split into raw words or ‘tokens’. Special tokens denote puncuation, unknown words etc. | The tokenized data is then numericalized: every token is assigned its own unique number. A text document becomes a list of numbers, which can be processed by a neural network. | This data loading and transforming is achieved in fastai with the data block API: . data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) .split_from_df(col=2) .label_from_df(cols=0) .databunch()) . Training the Language Model . No point training the Wikitext 103 model from scratch just download the pretrained one from fastai. Instead we want to start with that a fine tune it with the IMDB corpus. First we load the IMDB data for language model learning: . bs=48 data_lm = (TextList.from_folder(path) #Inputs: all the text files in path .filter_by_folder(include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) #We may have other temp folders that contain text files so we only keep what&#39;s in train and test .split_by_rand_pct(0.1) #We randomly split and keep 10% (10,000 reviews) for validation .label_for_lm() #We want to do a language model so we label accordingly .databunch(bs=bs)) data_lm.save(&#39;data_lm.pkl&#39;) . We can say: . It’s a list of text files﹣the full IMDB actually is not in a CSV. Each document is a separate text file. | Say where it is﹣in this case we have to make sure we just to include the train and test folders. | We randomly split it by 0.1. | . This data looks like: . . You then train the language model, not using a CNN rather a Recursive Neural Network (RNN). In fastai the code is: . learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3) . The pretrained language model that comes from fastai is AWD_LSTM: link. . You then do usual routine for training: . Run LRFind | Train the network head (1-2 epochs) | Unfreeze | Run LRFind again | Train the whole network (5+ epochs). | Save the encoder: learn.save_encoder(&#39;fine_tuned_enc&#39;) | . Predicting Text with the Language Model . With the trained language model we can have some fun by making it finish sentences. . TEXT = &quot;I liked this movie because&quot; N_WORDS = 40 N_SENTENCES = 2 print(&quot; n&quot;.join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES))) . The output of this: . I liked this movie because of the cool scenery and the high level of xxmaj british hunting . xxmaj the only thing this movie has going for it is the horrible acting and no script . xxmaj the movie was a big disappointment . xxmaj I liked this movie because it was one of the few movies that made me laugh so hard i did n’t like it . xxmaj it was a hilarious film and it was very entertaining . xxmaj the acting was great , i ‘m . Text Classifier . Load the data: . data_clas = (TextList.from_folder(path, vocab=data_lm.vocab) #grab all the text files in path .split_by_folder(valid=&#39;test&#39;) #split by train and valid folder (that only keeps &#39;train&#39; and &#39;test&#39; so no need to filter) .label_from_folder(classes=[&#39;neg&#39;, &#39;pos&#39;]) #label them all with their folders .databunch(bs=bs)) . Create a text classifer and give it the language model we trained: . learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) learn.load_encoder(&#39;fine_tuned_enc&#39;) # load language model . Tabular Data . Tabular data is one of the most common problems that data scientists work on day-to-day. This are things like spreadsheets, relational databases, or financial reports. People used to be sceptical about using neural networks for tabular data - everybody knows you should be using XGBoost! However not only does it work well, it can do things that even XGBoost can’t do. . fastai has created the module fastai.tabular for using NNs with tabular data. . Loading the Data . Import the fastai modules: . from fastai import * from fastai.tabular import * . The data input is assumed to be a pandas dataframe. Here is the Adult dataset, which is a classic dataset where you have to predict somebody’s salary given a number of variables like age, education, occupation etc: . path = untar_data(URLs.ADULT_SAMPLE) df = pd.read_csv(path/&#39;adult.csv&#39;) . For fastai’s tabular models you need to tell it about your columns: . Which column is the target variable? | Which columns have continuous variables? | Which columns have categorical variables? | What preprocessing do you want to do to the columns? | In code these variables look like: . dep_var = &#39;salary&#39; cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] procs = [FillMissing, Categorify, Normalize] . Using these we can then load the data using data block API: . data = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs) .split_by_idx(list(range(800,1000))) .label_from_df(cols=dep_var) .add_test(test, label=0) .databunch()) . There are a number of processors in the fastai library. The ones we’re going to use this time are: . FillMissing: Look for missing values and deal with them some way (e.g. mean, median…). | Categorify: Find categorical variables and turn them into Pandas categories | Normalize : Do a normalization ahead of time which is to take continuous variables and subtract their mean and divide by their standard deviation so they are zero-one variables. | . For the full list of transforms available see the documentation. . Training the Model . learn = tabular_learner(data, layers=[200,100], metrics=accuracy) . learn.fit(1, 1e-2) Total time: 00:03 epoch train_loss valid_loss accuracy 1 0.362837 0.413169 0.785000 (00:03) . This creates a tabular_learner network with the parameter layers=[200, 100]. What is this exactly? If you look at model in pytorch, learn.model: . TabularModel( (embeds): ModuleList( (0): Embedding(10, 6) (1): Embedding(17, 8) (2): Embedding(8, 5) (3): Embedding(16, 8) (4): Embedding(7, 5) (5): Embedding(6, 4) (6): Embedding(3, 3) ) (emb_drop): Dropout(p=0.0) (bn_cont): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=42, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Linear(in_features=200, out_features=100, bias=True) (4): ReLU(inplace) (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (6): Linear(in_features=100, out_features=2, bias=True) ) ) . The tabular learner is a just a multi-layer perceptron (MLP) (the layers group) with some funny input bolted onto the front of it. In the layers group you can see the first two Linear layers have an output size of 200 and 100, respectively. These are the sizes we put into the layers parameter in the model. So it’s a two layer MLP with layer sizes of 200 and 100. . The input layer consists of a bunch of Embedding layers. We’ll explain these later, but basically for each of the categorical features in the data (there are 6 here) there is an embedding layer. An embedding maps the total number of unique values of a categorical variable to a lower dimensional continuous vector space. If you take the zeroth embedding layer as an example: . (0): Embedding(10, 6) . This variable has 9 unique values + 1 null value added by the fastai processors. Its output is 6 dimensional. . All of the outputs of the embedding layers are concatenated together along with the 3 continuous features to create a 42 dimensional vector that is the input to the MLP part of the network. . Collaborative Filtering . Collaborative filtering is where you have many users and many items and you want to predict how much a certain user is going to like a certain item. You have historical information about who bought what, who liked which item etc. You then want to predict what a particular user would like that they haven’t seen before. . The most basic version would be a table with userId, movieId, and rating: .   userId movieId rating timestamp . 0 | 73 | 1097 | 4.0 | 1255504951 | . 1 | 561 | 924 | 3.5 | 1172695223 | . 2 | 157 | 260 | 3.5 | 1291598691 | . 3 | 358 | 1210 | 5.0 | 957481884 | . 4 | 130 | 316 | 2.0 | 1138999234 | . The data is sparse - no single user has rated even a decent fraction of the films and many films haven’t been rated. . To achieve these aims, the problem is posed as a Matrix Factorisation problem. That is you suppose that there is some matrix that describes all the users $U$, and a matrix that describes all the movies $M$, and that the ratings of all the movies by all the users is the matrix product of these two matrices: (UM = R ) The matrices $U$ and $M$ are called the Embedding matrices. The idea is that every row of the matrix $U$ is some $D_u$ dimensional vector that represents a single user, and likewise every row of the matrix $M$ is some $D_m$ dimensional vector that represents a single movie. These vectors are such that, if I take the dot product of a user vector and movie vector it will predict the rating the user would assign that movie. . The embeddings here are the same as what we saw earlier in the categorical variables for tabular data. It’s worth taking a deeper dive into what these are. . Embeddings . Given that users and movies are only categorical variables, how do we determine how ‘far apart’ they are from each other. How similar is one film to another. How similar is one user to another? . | Often in machine learning categorical variables are represented using One-hot Encoding. . | This is where categorical variables are represented as a sparse vector, with a dimension for every unique value. For example, consider three items: . &#39;Twix&#39;: [1, 0, 0] &#39;Kit-kat&#39;: [0, 1, 0] &#39;Vodka&#39;: [0, 0, 1] . | This is often sufficient for categorical variables in machine learning algorithms. However there is a lack of meaning in these vectors. For example, all the vectors are equidistant, but I know that ‘Twix’ and ‘Kit-kat’ are both chocolate bars and so are ‘nearer’ to each other than they are to ‘Vodka’. One-hot encoding does not encode these semantics. . | This is what embeddings can do for us. An embedding is a matrix of weights. They map these one-hot vectors to a continuous vector space that encodes some meaning about the categories. In the example above this could be some 2D space of ‘foody’ things and ‘drinky’ things: . &#39;Twix&#39;: [0.98, 0] &#39;Kit-kat&#39;: [0.97, 0] &#39;Vodka&#39;: [0.1, 0.95] . | But the meaning is context dependent. The embedded space dimensions could represent anything like whether the item is expensive or whether it is more likely to be consumed at night. . | Embeddings have to be trained with supervised learning. They are initialized with random weights and then learned in collaborative filtering and in the tabular network with gradient-descent. . | . Embedding Layer as a Look-up Table . What does an embedding layer look like under the hood? text mining - How does Keras ‘Embedding’ layer work? - Cross Validated . It’s pretty much a lookup table of vectors. You have an input size of 5000 and an embedding size of 100 then you will have a list of 5000 100d vectors. You could represent this as a spare-vector dense matrix multiply, but that would be inefficient. | . Embedding Layer as matrix multiplication . The lookup, multiplication, and addition procedure we’ve just described is equivalent to matrix multiplication. Given a $1 times N$ sparse representation $S$ and an $N times M$ embedding table $E$, the matrix multiplication $S times E$ gives you the $1 times M$ dense vector. . . Different Uses of Embeddings . Embeddings map items (e.g. movies, text…) to a low dimensional dense eal vectors such that similar items are close to each other. | Embeddings can also be applied to dense data (e.g. audio) to create a meaningful similarity metric. | Jointly embedding diverse data types (e.g. text, images, audio…) can define a similarity metric between them. | . Example: Movie Lens Dataset . Link to notebook here. . First you choose some number of factors $N_f$. This is the size of the embedding. There are $N_u$ users and $N_m$ movies. You then create emedding matrices for users and movies: . User embedding matrix of size $(N_u, N_f)$ | Movie embedding matrix of size $(N_m, N_f)$ | . Note that the sizes of the embeddings for the users and movies, $N_f$, have to be the same because we are taking a dot product of them. . You can also add biases. Maybe some users just really like movies a lot more than other users. Maybe there are certain movies that everybody just likes. So in addition to the matrices you can add a single movie for how much a user likes movies, and a single number for how popular a movie is. . So the prediction of how a user would rate a movie would be the dot product of the vector from the user embedding matrix with the vector from the movie embedding matrix, plus the bias for the user and the bias for the movie. This intuitively makes sense - you have the embedded model of how users like different movies (embedding model), and then the individual characteristics of that particular user and that particular film (bias). . The rating of the movie is then calculated using a sigmoid function with a range of 0 to 5 stars. . . In fastai the code to do this is: . ratings = pd.read_csv(path/&#39;ratings.csv&#39;) data = CollabDataBunch.from_df(ratings, seed=42) y_range = [0,5.5] learn = collab_learner(data, n_factors=50, y_range=y_range) learn.fit_one_cycle(3, 5e-3) . What does the model look like? . $&gt; learn.model EmbeddingDotBias( (u_weight): Embedding(101, 50) (i_weight): Embedding(101, 50) (u_bias): Embedding(101, 1) (i_bias): Embedding(101, 1) ) . In this dataset there are 100 movies and 100 users. The inputs to the embedding layers are of size 101. This is because fastai adds in a ‘null’ category - #na#. You can see this in the CollabDataBunch object: . $&gt; data.train_ds.x.classes OrderedDict([(&#39;userId&#39;, array([&#39;#na#&#39;, &#39;15&#39;, &#39;17&#39;, &#39;19&#39;, ..., &#39;652&#39;, &#39;654&#39;, &#39;664&#39;, &#39;665&#39;], dtype=&#39;&lt;U21&#39;)), (&#39;movieId&#39;, array([&#39;#na#&#39;, &#39;1&#39;, &#39;10&#39;, &#39;32&#39;, ..., &#39;6539&#39;, &#39;7153&#39;, &#39;8961&#39;, &#39;58559&#39;], dtype=&#39;&lt;U21&#39;))]) . Cold Start Problem . If you don’t have any data on your user’s preferences then you can’t recommend them anything. There isn’t an easy solution to this; likely the only way is to have a second model which is not a collaborative filtering model but a metadata driven model for new users or new movies. A few possible approaches to tackle this problem: . Ask the user in the UX. For example Netflix proposes films and tv series to a user and asks them which ones they like so that it can bootstrap collaborative filtering. | You could use metadata about the user and the products and handcraft a crude recommendation system that way. | Jeremy Says… . If you’re doing NLP stuff, make sure you use all of the text you have (including unlabeled validation set) to train your language model, because there’s no reason not to. In Kaggle competions they don’t give you the labels for the test set, but you can still use the test data for self-supervised learning. Lesson 4: A little NLP trick | Jeremy used to use random forests / xgboost with tabular data 99% of the time. Today he uses neural networks 90% of the time. It’s his goto method he tries first. | (Source: Robert Bracco) . Q &amp; A . Does the language model approach works for text in forums that are informal English, misspelled words or slangs or shortforms like s6 instead of Samsung S 6? [12:47] . Yes, absolutely it does. Particularly if you start with your wikitext model and then fine-tune it with your “target” corpus. Corpus is just a bunch of documents (emails, tweets, medical reports, or whatever). You could fine-tune it so it can learn a bit about the specifics of the slang , abbreviations, or whatever that didn’t appear in the full corpus. So interestingly, this is one of the big things that people were surprised about when we did this research last year. People thought that learning from something like Wikipedia wouldn’t be that helpful because it’s not that representative of how people tend to write. But it turns out it’s extremely helpful because there’s a much a difference between Wikipedia and random words than there is between like Wikipedia and reddit. So it kind of gets you 99% of the way there. . So language models themselves can be quite powerful. For example there was a blog post from SwiftKey (the folks that do the mobile-phone predictive text keyboard) and they describe how they kind of rewrote their underlying model to use neural nets. This was a year or two ago. Now most phone keyboards seem to do this. You’ll be typing away on your mobile phone, and in the prediction there will be something telling you what word you might want next. So that’s a language model in your phone. . Another example was the researcher Andrej Karpathy who now runs all this stuff at Tesla, back when he was a PhD student, he created a language model of text in LaTeX documents and created these automatic generation of LaTeX documents that then became these automatically generated papers. That’s pretty cute. . We’re not really that interested in the output of the language model ourselves. We’re just interested in it because it’s helpful with this process. . | How to combine NLP (tokenized) data with meta data (tabular data) with Fastai? For instance, for IMBb classification, how to use information like who the actors are, year made, genre, etc. [49:14] . Yeah, we’re not quite up to that yet. So we need to learn a little bit more about how neural net architectures work as well. But conceptually, it’s kind of the same as the way we combine categorical variables and continuous variables. Basically in the neural network, you can have two different sets of inputs merging together into some layer. It could go into an early layer or into a later layer, it kind of depends. If it’s like text and an image and some metadata, you probably want the text going into an RNN, the image going into a CNN, the metadata going into some kind of tabular model like this. And then you’d have them basically all concatenated together, and then go through some fully connected layers and train them end to end. We will probably largely get into that in part two. In fact we might entirely get into that in part two. I’m not sure if we have time to cover it in part one. But conceptually, it’s a fairly simple extension of what we’ll be learning in the next three weeks. . | Where does the magic number of in the learning rate come from? [33:38] . learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7)) . Good question. So the learning rate is various things divided by 2.6 to the fourth. The reason it’s to the fourth, you will learn about at the end of today. So let’s focus on the 2.6. Why 2.6? Basically, as we’re going to see in more detail later today, this number, the difference between the bottom of the slice and the top of the slice is basically what’s the difference between how quickly the lowest layer of the model learns versus the highest layer of the model learns. So this is called discriminative learning rates. So really the question is as you go from layer to layer, how much do I decrease the learning rate by? And we found out that for NLP RNNs, the answer is 2.6. . How do we find out that it’s 2.6? I ran lots and lots of different models using lots of different sets of hyper parameters of various types (dropout, learning rates, and discriminative learning rate and so forth), and then I created something called a random forest which is a kind of model where I attempted to predict how accurate my NLP classifier would be based on the hyper parameters. And then I used random forest interpretation methods to basically figure out what the optimal parameter settings were, and I found out that the answer for this number was 2.6. So that’s actually not something I’ve published or I don’t think I’ve even talked about it before, so there’s a new piece of information. Actually, a few months after I did this, Stephen Merity and somebody else did publish a paper describing a similar approach, so the basic idea may be out there already. . Some of that idea comes from a researcher named Frank Hutter and one of his collaborators. They did some interesting work showing how you can use random forests to actually find optimal hyperparameters. . | How does the language model trained in this manner perform on code switched data (Hindi written in English words), or text with a lot of emojis?: . Text with emojis, it’ll be fine. There’s not many emojis in Wikipedia and where they are at Wikipedia it’s more like a Wikipedia page about the emoji rather than the emoji being used in a sensible place. But you can (and should) do this language model fine-tuning where you take a corpus of text where people are using emojis in usual ways, and so you fine-tune the Wikitext language model to your reddit or Twitter or whatever language model. And there aren’t that many emojis if you think about it. There are hundreds of thousands of possible words that people can be using, but a small number of possible emojis. So it’ll very quickly learn how those emojis are being used. So that’s a piece of cake. . I’m not really familiar with Hindi, but I’ll take an example I’m very familiar with which is Mandarin. In Mandarin, you could have a model that’s trained with Chinese characters. There are about five or six thousand Chinese characters in common use, but there’s also a romanization of those characters called pinyin. It’s a bit tricky because although there’s a nearly direct mapping from the character to the pinyin (I mean there is a direct mapping but that pronunciations are not exactly direct), there isn’t direct mapping from the pinyin to the character because one pinyin corresponds to multiple characters. . So the first thing to note is that if you’re going to use this approach for Chinese, you would need to start with a Chinese language model. . Actually fastai has something called Language Model Zoo where we’re adding more and more language models for different languages, and also increasingly for different domain areas like English medical texts or even language models for things other than NLP like genome sequences, molecular data, musical MIDI notes, and so forth. So you would you obviously start there. . To then convert that (in either simplified or traditional Chinese) into pinyin, you could either map the vocab directly, or as you’ll learn, these multi-layer models﹣it’s only the first layer that basically converts the tokens into a set of vectors, you can actually throw that away and fine-tune just the first layer of the model. So that second part is going to require a few more weeks of learning before you exactly understand how to do that and so forth, but if this is something you’re interested in doing, we can talk about it on the forum because it’s a nice test of understanding. . | Regarding using NN for Tabular data: What are the 10% of cases where you would not default to neural nets? [40:41]: . Good question. I guess I still tend to give them a try. But yeah, I don’t know. It’s kind of like as you do things for a while, you start to get a sense of the areas where things don’t quite work as well. I have to think about that during the week. I don’t think I have a rule of thumb. But I would say, you may as well try both. I would say try a random forest and try a neural net. They’re both pretty quick and easy to run, and see how it looks. If they’re roughly similar, I might dig into each and see if I can make them better. But if the random forest is doing way better, I’d probably just stick with that. Use whatever works. . | Do you think that things like scikit-learn and xgboost will eventually become outdated? Will everyone will use deep learning tools in the future? Except for maybe small datasets?[50:36] . I have no idea. I’m not good at making predictions. I’m not a machine learning model. I mean xgboost is a really nice piece of software. There’s quite a few really nice pieces of software for gradient boosting in particular. Actually, random forests in particular has some really nice features for interpretation which I’m sure we’ll find similar versions for neural nets, but they don’t necessarily exist yet. So I don’t know. For now, they’re both useful tools. scikit-learn is a library that’s often used for pre-processing and running models. Again, it’s hard to predict where things will end up. In some ways, it’s more focused on some older approaches to modeling, but I don’t know. They keep on adding new things, so we’ll see. I keep trying to incorporate more scikit-learn stuff into fastai and then I keep finding ways I think I can do it better and I throw it away again, so that’s why there’s still no scikit-learn dependencies in fastai. I keep finding other ways to do stuff. . | What about time series on tabular data? is there any RNN model involved in tabular.models? [1:05:09]: . We’re going to look at time series tabular data next week, but the short answer is generally speaking you don’t use a RNN for time series tabular data but instead, you extract a bunch of columns for things like day of week, is it a weekend, is it a holiday, was the store open, stuff like that. It turns out that adding those extra columns which you can do somewhat automatically basically gives you state-of-the-art results. There are some good uses of RNNs for time series, but not really for these kind of tabular style time series (like retail store logistics databases, etc). . | . Links and References . Link to Lesson 4 lecture | Homework notebooks: Notebook 1: lesson4-collab.ipynb | Notebook 2: lesson4-tabular.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson4 Detailed Notes. | Link to ULMFiT paper: https://arxiv.org/abs/1801.06146 | Fastai blog post on tabular data An Introduction to Deep Learning for Tabular Data · fast.ai | Medium post on recommenders with NN: Collaborative Embeddings for Lipstick Recommendations | Lecture on Embeddings: Embeddings, Machine Learning Crash Course, Google Developers | Word2Vec: Vector Representations of Words | Paper Review: Neural Collaborative Filtering Explanation &amp; Implementation | Blog post from Twitter on Embeddings: Embeddings@Twitter | Video: Embeddings for Everything: Search in the Neural Network Era | Applying the four step “Embed, Encode, Attend, Predict” framework to predict document similarity | Mini-course on Recommendation Systems, Google: Introduction to Recommendation Systems, Google Developers | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/18/fast-ai-lesson-4-notes-nlp-tabular-data-recommenders.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/18/fast-ai-lesson-4-notes-nlp-tabular-data-recommenders.html",
            "date": " • Aug 18, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Fast.ai v3 Lesson 3 Notes: Multi-label, Segmentation",
            "content": "Overview of Lesson . This lesson firstly dives deeper in to fastai’s approach to loading data for deep learning: the data block API; and secondly goes into more advanced problems beyond classification that you can solve with deep learning and the fastai library. Namely: . Multi-label classification (Planet Amazon dataset) | Regression problems (Head Orientation dataset) | Image Segmentation (Camvid dataset) | Text Classification (IMDB dataset) | . The lesson ends with a brief look at the fundamentals of deep learning: non-linearity and the Universal Approximation theorem. . DataBlock API . The trickiest step previously in deep learning has often been getting the data into a form that you can get it into a model. So far we’ve been showing you how to do that using various “factory methods” which are methods where you say, “I want to create this kind of data from this kind of source with these kinds of options.” That works fine, sometimes, and we showed you a few ways of doing it over the last couple of weeks. But sometimes you want more flexibility, because there’s so many choices that you have to make about: . Where do the files live | What’s the structure they’re in | How do the labels appear | How do you spit out the validation set | How do you transform it | . In fastai there is this unique API called the data block API. The data block API makes each one of those decisions a separate decision that you make. There are separate methods with their own parameters for every choice that you make around how to create / set up the data. . To give you a sense of what that looks like, the first thing I’m going to do is go back and explain what are all of the PyTorch and fastai classes you need to know about that are going to appear in this process. Because you’re going to see them all the time in the fastai docs and PyTorch docs. . We will now explain the different PyTorch and fastai classes that appear in the data block API. . Dataset (PyTorch) . The first class you need to know is the Dataset class, which is part of PyTorch. It is very simple, here is the source code: . . It actually does nothing at all. It is an abstract class, defining that subclasses of Dataset must implement __getitem__ and __len__ methods. The first means you can use the python array indexing notation with a Dataset object: d[12]; and the second means that you can get the length of the Dataset object: len(d). . DataLoader (PyTorch) . A Dataset is not enough to train a model. For SGD we need to be able to produce mini-batches of data for training. To create mini-batches we use another PyTorch class called a DataLoader. Here is the documentation for that: . . This takes a Dataset as a parameter. | It will create batches of size batch_size by grabbing items at random from the dataset. | The dataloader then sends the batch over to the GPU to your model. | . DataBunch (fastai) . The DataLoader is still not enough to train a model. To train a model we need to split the data into training, validation, and testing. So for that fastai has its own class called DataBunch. . . The DataBunch combines together a training DataLoader, a validation DataLoader, and optionally a test DataLoader. . You can read from the documentation that DataBunch also handles on-the-fly data transformations with tfms and allows you to create a custom function for building the mini-batches with collate_fn. . Learn to use the data block API by example . I won’t reproduce the examples here because fastai’s documentation already has a fantastic page full of data block API examples here. I recommend you read the whole thing or download and run it because the documentation pages are all Jupyter notebooks! . Image Transforms (docs) . fastai provides a complete image transformation library written from scratch in PyTorch. Although the main purpose of the library is data augmentation for use when training computer vision models, you can also use it for more general image transformation purposes. | Data augmentation is perhaps the most important regularization technique when training a model for Computer Vision: instead of feeding the model with the same pictures every time, we do small random transformations (a bit of rotation, zoom, translation, etc…) that don’t change what’s inside the image (to the human eye) but do change its pixel values. Models trained with data augmentation will then generalize better. | get_transforms creates a list of a image transformations. | Which image transformations are appropriate to use depends on your problem and what would likely appear in the real data. Flipping images of cats/dogs vertical isn’t useful because they wouldn’t appear upside-down. While for satellite images it makes no sense to zoom, but flipping them vertically and horizontally would make sense. | fastai is unique in that it provides a fast implementation of perspective warping. This is the max_wap option in get_transforms. This is like the kind of warping that occurs if you take a picture of a cat from above versus from below. This kinds of transformation wouldn’t make sense for satellite images, but would for cats and dogs. | . Planet Amazon: Multi-label Classification . (Link to Notebook) . The Planet Amazon dataset is an example of a multi-label classification problem. The dataset consists of satellite images taken of the Amazon rainforest, each of which has a list of labels describing what’s in the image, for example: weather, trees, river, agriculture. . Here is a sample of the images: . Here is what some of the training labels look like: . df = pd.read_csv(path/&#39;train_v2.csv&#39;) df.head() .   image_name tags . 0 | train_0 | haze primary | . 1 | train_1 | agriculture clear primary water | . 2 | train_2 | clear primary | . 3 | train_3 | clear primary | . 4 | train_4 | agriculture clear habitation primary road | . There are many different labels that an image can have and so there are a huge number of combinations. This makes treating it as a single label classification impractical. If there were 20 different labels then there could be as many as $2^{20}$ possible combinations. Better to have the model output a 20d vector than have it try to learn $2^{20}$ individual labels! . Loading the Data . Code for loading data with comments: . tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.) src = (ImageList.from_csv(path, &#39;train_v2.csv&#39;, folder=&#39;train-jpg&#39;, suffix=&#39;.jpg&#39;) # image names are listed in a csv file (names found by default in first column) # path to the data is `path` # the folder containing the data is `folder` # the file ext is missing from the names in the csv so add `suffix` to them. .split_by_rand_pct(0.2) # split train/val randomly with 80/20 split .label_from_df(label_delim=&#39; &#39;) # label from dataframe (the output of `from_csv`). Default is second column. # split the tags by &#39; &#39; ) data = (src.transform(tfms, size=128) .databunch().normalize(imagenet_stats)) . Create the Model . To create a Learner for multi-label classification you don’t need to do anything different from before. fastai create_cnn takes the DataBunch object and see that the type of the target variable and takes care of creating the output layers etc for you behind the scenes. . For this particular problem the only thing we do different is to pass a few different metrics to the Learner. . acc_02 = partial(accuracy_thresh, thresh=0.2) f_score = partial(fbeta, thresh=0.2) learn = cnn_learner(data, models.resnet50, metrics=[acc_02, f_score]) . Note: metrics change nothing about how the model learns. They are only an output for you to see how well it is learning. They are not to be confused with the model’s loss function. . What are these metrics about here? Well the network will output some M-dimensional vector with numbers between 0 and 1. Each of the the elements in this vector indicate the presence of one of the labels, but we need to decide a threshold value, above which we will say that this or that label is ‘on’. We set this threshold to 0.2. acc_02 and f_score here are the accuracy and f-score after applying 0.2 thresholding to the model output. . Train the Model . The model is trained in the basically same way as in the previous lessons: . Freeze all the layers except the head. | Run lr_find(). | Train the head for a few cycles. | Unfreeze the rest of the network and run lr_find() again. | Train the whole model for some more cycles with a differential learning rate. | After this however, Jeremy shows a cool new trick however called progressive resizing. Here you train your network on images that are smaller, then continue training the network on larger images. So start $128^2$ for a few cycles, then $256^2$, $512^2$ etc. You can save time on training for higher resolution images by effectively using smaller resolution models as pretrained models for larger resolutions. . To do this you simply have to tweak the DataBunch from before and give it a new size then give that to the Learner: . data = (src.transform(tfms, size=256) .databunch().normalize(imagenet_stats)) learn.data = data data.train_ds[0][0].shape . I won’t reproduce anymore here, but the full example is covered in this Lesson 3 homework notebook. . Camvid: Image Segmentation . (Link to Notebook) . The next example we’re going to look at is this dataset called CamVid. It’s going to be doing something called segmentation. We’re going to start with a picture like the left and produce a colour-coded picture on the right: . Input Image Segmented Output . | | . All the bicycle pixels are the same colour, all the car pixels are the same color etc. . Segmentation is an image classification problem where you need to label every single pixel in the image. . | In order to build a segmentation model, you actually need to download or create a dataset where someone has actually labeled every pixel. As you can imagine, that’s a lot of work, so you’re probably not going to create your own segmentation datasets but you’re probably going to download or find them from somewhere else. . | We use Camvid dataset. fastai comes with many datasets available for download through the fastai library. They are listed here. . | . Loading the Data . Segmentation problems come with sets of images: the input image and a segmentation mask. | The segmentation mask is a 2D array of integers. | fastai has a special function for opening image masks called open_mask. (For ordinary images use open_image). | . Create a databunch: . codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) get_y_fn = lambda x: path_lbl/f&#39;{x.stem}_P{x.suffix}&#39; src = (SegmentationItemList.from_folder(path_img) #Where to find the data? -&gt; in path_img and its subfolders .split_by_fname_file(&#39;../valid.txt&#39;) #How to split in train/valid? -&gt; using predefined val set in valid.txt .label_from_func(get_y_fn, classes=codes) #How to label? -&gt; use the label function on the file name of the data ) data = (src.transform(get_transforms(), tfm_y=True, size=128) #Data augmentation? -&gt; use tfms with a size of 128, # also transform the label images (tfm_y) .databunch(bs=8) #Finally -&gt; use the defaults for conversion to databunch ).normalize(imagenet_stats) . fastai shows the images with the masks superimposed for you with show_batch: . . Create the Model . For segmentation an architecture called UNET turns out to be better than using a CNN. Here’s what it looks like:. | Here is a link to the University website where they talk about the U-Net. But basically this bit down on the left hand side is what a normal convolutional neural network looks like. It’s something which starts with a big image and gradually makes it smaller and smaller until eventually you just have one prediction. What a U-Net does is it then takes that and makes it bigger and bigger and bigger again, and then it takes every stage of the downward path and copies it across, and it creates this U shape. | It is a bit like a convolutional autoencoder except there are these data sharing links that cross horizontal across the network in the diagram. | In fastai you create a UNET with unet_learner(data, models.resnet34, metrics=metrics, wd=wd) and you pass in all the same stuff as with cnn_learner. | . Results . With UNET and the default fastai results Jeremy managed to achieve a SOTA result of 0.92 accuracy. . BIWI Head Pose: Regression . (Link to Notebook) . . In this problem we want to locate the center point of the face of a person in an image. . So far, everything we’ve done has been a classification model﹣something that created labels or classes. This, for the first time, is what we call a regression model. A lot of people think regression means linear regression, it doesn’t. Regression just means any kind of model where your output is some continuous number or set of numbers. So we need to create an image regression model (i.e. something that can predict these two numbers). How do you do that? Same way as always - data bunch api then CNN model. . Loading the Data . tfms = get_transforms(max_rotate=20, max_zoom=1.5, max_lighting=0.5, max_warp=0.4, p_affine=1., p_lighting=1.) data = (PointsItemList.from_folder(path) .split_by_valid_func(lambda o: o.parent.name==&#39;13&#39;) .label_from_func(get_ctr) .transform(tfms, tfm_y=True, size=(120,160)) .databunch().normalize(imagenet_stats) ) . This is exactly the same as we have already seen except the target variable is represented as a different data type - the ImagePoints class. An ImagePoints object represents a ‘flow’ (it’s just a list) of 2D points on an image. The points have the convention of (y, x) and are scaled to be between -1 and 1. . An example flow looks like: . . In the case of BIWI, there is only one point in the flow however, but you get the idea. For facial keypoints type problems ImagePoints is what you want to use. . When it comes to training the model you use the same cnn_learner as in the other examples, the only difference being the loss function. For problems where you are predicting a continuous value like here you typically use the Mean Squared Error (MSELossFlat() in fastai) as the loss function. In fastai you don’t need to specify this, cnn_learner will select it for you. . IMDB: Text Classification . (Link to the Notebook) . This is a short section on a Natural Language Processing (NLP) problem. Instead of classifying images we will look classifying documents. We will use the IMDB data set and classify if a movie review is negative or positive. . Use different fastai module for text: . from fastai.text import * . Loading the Data . The data is a csv and looks like: . . Use the datablock/databunch API to load the csv for text data types: . data_lm = TextDataBunch.from_csv(path, &#39;texts.csv&#39;) . From this you can create a learner and train on this. There are two steps not shown here that transform the text from text to something that you can give to a neural network to train on (i.e. numbers). These steps are Tokenization and Numericalization. . Tokenization . Split raw sentences into words, or ‘tokens’. It does this by: . Splitting the string into just the words | Takes care of punctuation | Separates contractions from words: “didn’t” -&gt; “did” + “n’t” | Replacing unknown words with a single token “xxunk”. | Cleaning out HTML from the text. | . data = TextClasDataBunch.from_csv(path, &#39;texts.csv&#39;, valid_pct=0.01) data.show_batch() . Example: . “Raising Victor Vargas: A ReviewYou know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It’s warm and gooey, but you’re not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn’t quite feel right. … . =&gt; . xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review n n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it ‘s warm and gooey , but you ‘re not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj … . Anything starting with ‘xx’ is some special token. . Numericalization . Once we have extracted the tokens from the text, we can convert them to integers by create a big list of all the tokens used: vocabulary. This list only includes words that are used at least twice and is truncated with a maximum size of 60,000 (by default). Words that don’t make the cut are replaced with ‘XXUNK’. . From the notebook: . . With the data block API . Here are the previous steps done this time with the data block API: . data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) .split_from_df(col=2) .label_from_df(cols=0) .databunch()) . Training a Classifier Preview . Lesson 4 covers the training of the text classifier in detail. Here are the steps covered as a preview. . You need to first create a language model trained on your text corpus. fastai has language_model_learner for this. This training is quite time/compute intensive. | Then you create a text classifier - text_classifier_model - and use the language model trained in 1 as the feature encoder. | What is deep learning fundamentally? . Up to this point we’ve seen loads of different problems that deep learning helps us tackle. Deep learning is buzzword for algorithms that use these things called neural networks, which sound like something complicated that may have something to do with how the human brain works. If you remove all the mystique from deep learning you see that it is basically a model with parameters that are updated using Stochastic Gradient Descent. These parameters are parameters to matrix multiplications (convolutions also a tweaked kind of matrix multiplication). . A matrix multiply is a linear function and any stacking of matrix multiplies is a also a linear function because of linearity. Telling the difference between cats and dogs is far more than a linear function can do. So after the matrix multiplications we have something called a non-linearity of activation function. This takes the result of the matrix multiplication and sticks it through some non-linear function. . In the old days the most common function used was the sigmoid, e.g. tanh: . . These days the workhorse is the rectified linear unit (ReLU): . . Sounds fancy, but in reality it’s this: . def relu(x): max(x, 0) . So how can a stack of matrix multiplications and relu’s result in a model that can classify IMDB reviews or galaxies? Because of a thing called the Universal Approximation Theorem. What it says is that if you have stacks of linear functions and nonlinearities, the thing you end up with can approximate any function arbitrarily closely. So you just need to make sure that you have a big enough matrix to multiply by, or enough of them. If you have this function which is just a sequence of matrix multiplies and nonlinearities where the nonlinearities can be basically any of these activation functions, if that can approximate anything, then all you need is some way to find the particular values of the weight matrices in your matrix multiplies that solve the problem you want to solve. We already know how to find the values of parameters. We can use gradient descent. So that’s actually it. . There is a nice website that has interactive javascript demos that demonstrate this: http://neuralnetworksanddeeplearning.com. . Jeremy Says… . If you use a dataset, it would be very nice of you to cite the creator and thank them for their dataset. | This week, see if you can come up with a problem that you would like to solve that is either multi-label classification or image regression or image segmentation or something like that and see if you can solve that problem. Context: Fast.ai Lesson 3 Homework 36 | Always use the same stats that the model was trained with (e.g. imagenet). (See relevant question in Q &amp; A section). Context: Lesson 3: Normalized data and ImageNet 7 | (Source: Robert Bracco) . Q &amp; A . When your model makes an incorrect prediction in a deployed app, is there a good way to “record” that error and use that learning to improve the model in a more targeted way? [42:01] If you had, for example, an image classifier online you could have a user tell you if the classifier got it wrong and what the right answer is. | You could then store that image that was incorrectly classified. | Every so often you could go and fine-tune your network on a new data bunch of just the misclassified images. | You do this by taking your existing network, unfreezing the layers, and then run some epochs on the misclassified images. You may want to run with a slightly higher learning rate or for more epochs because these images are more interesting/suprising to the model. | . | What resources do you recommend for getting started with video? For example, being able to pull frames and submit them to your model. [47:39] . The answer is it depends. If you’re using the web which I guess probably most of you will be then there’s web API’s that basically do that for you. So you can grab the frames with the web API and then they’re just images which you can pass along. If you’re doing a client side, I guess most people would tend to use OpenCV for that. But maybe during the week, people who are doing these video apps can tell us what have you used and found useful, and we can start to prepare something in the lesson wiki with a list of video resources since it sounds like some people are interested. . | Is there a way to use learn.lr_find() and have it return a suggested number directly rather than having to plot it as a graph and then pick a learning rate by visually inspecting that graph? (And there are a few other questions around more guidance on reading the learning rate finder graph) [1:00:26] . The short answer is no and the reason the answer is no is because this is still a bit more artisanal than I would like. As you can see, I’ve been saying how I read this learning rate graph depends a bit on what stage I’m at and what the shape of it is. I guess when you’re just training the head (so before you unfreeze), it pretty much always looks like this: . . And you could certainly create something that creates a smooth version of this, finds the sharpest negative slope and picked that. You would probably be fine nearly all the time. . But then for you know these kinds of ones, it requires a certain amount of experimentation: . . But the good news is you can experiment. Obviously if the lines going up, you don’t want it. Almost certainly at the very bottom point, you don’t want it right there because you needed to be going downwards. But if you kind of start with somewhere around 10x smaller than that, and then also you could try another 10x smaller than that. Try a few numbers and find out which ones work best. . And within a small number of weeks, you will find that you’re picking the best learning rate most of the time. So at this stage, it still requires a bit of playing around to get a sense of the different kinds of shapes that you see and how to respond to them. Maybe by the time this video comes out, someone will have a pretty reliable auto learning rate finder. We’re not there yet. It’s probably not a massively difficult job to do. It would be an interesting project﹣collect a whole bunch of different datasets, maybe grab all the datasets from our datasets page, try and come up with some simple heuristic, compare it to all the different lessons I’ve shown. It would be a really fun project to do. But at the moment, we don’t have that. I’m sure it’s possible but we haven’t got them. . | Could you use unsupervised learning here (pixel classification with the bike example) to avoid needing a human to label a heap of images[1:10:03] . Not exactly unsupervised learning, but you can certainly get a sense of where things are without needing these kind of labels. Time permitting, we’ll try and see some examples of how to do that. You’re certainly not going to get as such a quality and such a specific output as what you see here though. If you want to get this level of segmentation mask, you need a pretty good segmentation mask ground truth to work with. . | Is there a reason we shouldn’t deliberately make a lot of smaller datasets to step up from in tuning? let’s say 64x64, 128x128, 256x256, etc… [1:10:51] . Yes, you should totally do that. It works great. This idea, it’s something that I first came up with in the course a couple of years ago and I thought it seemed obvious and just presented it as a good idea, then I later discovered that nobody had really published this before. And then we started experimenting with it. And it was basically the main tricks that we use to win the DAWNBench ImageNet training competition. . Not only was this not standard, but nobody had heard of it before. There’s been now a few papers that use this trick for various specific purposes but it’s still largely unknown. It means that you can train much faster, it generalizes better. There’s still a lot of unknowns about exactly how small, how big, and how much at each level and so forth. We call it “progressive resizing”. I found that going much under 64 by 64 tends not to help very much. But yeah, it’s a great technique and I definitely try a few different sizes. . | What does accuracy mean for pixel wise segmentation? Is it#correctly classified pixels / #total number of pixels? [1:12:35] . Yep, that’s it. So if you imagined each pixel was a separate object you’re classifying, it’s exactly the same accuracy. So you actually can just pass in accuracy as your metric, but in this case, we actually don’t. We’ve created a new metric called acc_camvid and the reason for that is that when they labeled the images, sometimes they labeled a pixel as Void. I’m not quite sure why but some of the pixels are Void. And in the CamVid paper, they say when you’re reporting accuracy, you should remove the void pixels. So we’ve created accuracy CamVid. So all metrics take the actual output of the neural net (i.e. that’s the input to the metric) and the target (i.e. the labels we are trying to predict). . . We then basically create a mask (we look for the places where the target is not equal to Void) and then we just take the input, do the argmax as per usual, but then we just grab those that are not equal to the void code. We do the same for the target and we take the mean, so it’s just a standard accuracy. . It’s almost exactly the same as the accuracy source code we saw before with the addition of this mask. This quite often happens. The particular Kaggle competition metric you’re using or the particular way your organization scores things, there’s often little tweaks you have to do. And this is how easy it is. As you’ll see, to do this stuff, the main thing you need to know pretty well is how to do basic mathematical operations in PyTorch so that’s just something you kind of need to practice. . | I’ve noticed that most of the examples and most of my models result in a training loss greater than the validation loss. What are the best ways to correct that? I should add that this still happens after trying many variations on number of epochs and learning rate. [1:15:03] . Remember from last week, if your training loss is higher than your validation loss then you’re underfitting. It definitely means that you’re underfitting. You want your training loss to be lower than your validation loss. If you’re underfitting, you can: . Train for longer. | Train the last bit at a lower learning rate. | . But if you’re still under fitting, then you’re going to have to decrease regularization. We haven’t talked about that yet. In the second half of this part of the course, we’re going to be talking quite a lot about regularization and specifically how to avoid overfitting or underfitting by using regularization. If you want to skip ahead, we’re going to be learning about: . weight decay | dropout | data augmentation | . They will be the key things that are we talking about. . | For a dataset very different than ImageNet like the satellite images or genomic images shown in lesson 2, we should use our own stats. Jeremy once said: “If you’re using a pretrained model you need to use the same stats it was trained with.” Why it is that? Isn’t it that, normalized dataset with its own stats will have roughly the same distribution like ImageNet? The only thing I can think of, which may differ is skewness. Is it the possibility of skewness or something else the reason of your statement? And does that mean you don’t recommend using pre-trained model with very different dataset like the one-point mutation that you showed us in lesson 2? [1:46:53] . Nope. As you can see, I’ve used pre-trained models for all of those things. Every time I’ve used an ImageNet pre-trained model, I’ve used ImageNet stats. Why is that? Because that model was trained with those stats. For example, imagine you’re trying to classify different types of green frogs. If you were to use your own per-channel means from your dataset, you would end up converting them to a mean of zero, a standard deviation of one for each of your red, green, and blue channels. Which means they don’t look like green frogs anymore. They now look like grey frogs. But ImageNet expects frogs to be green. So you need to normalize with the same stats that the ImageNet training people normalized with. Otherwise the unique characteristics of your dataset won’t appear anymore﹣you’ve actually normalized them out in terms of the per-channel statistics. So you should always use the same stats that the model was trained with. . | There’s a question about tokenization. I’m curious about how tokenizing words works when they depend on each other such as San Francisco. [1:56:45] . How do you tokenize something like San Francisco. San Francisco contains two tokens San Francisco. That’s it. That’s how you tokenize San Francisco. The question may be coming from people who have done traditional NLP which often need to use these things called n-grams. N-rams are this idea of a lot of NLP in the old days was all built on top of linear models where you basically counted how many times particular strings of text appeared like the phrase San Francisco. That would be a bi-gram for an n-gram with an n of 2. The cool thing is that with deep learning, we don’t have to worry about that. Like with many things, a lot of the complex feature engineering disappears when you do deep learning. So with deep learning, each token is literally just a word (or in the case that the word really consists of two words like you&#39;re you split it into two words) and then what we’re going to do is we’re going to then let the deep learning model figure out how best to combine words together. Now when we see like let the deep learning model figure it out, of course all we really mean is find the weight matrices using gradient descent that gives the right answer. There’s not really much more to it than that. . Again, there’s some minor tweaks. In the second half of the course, we’re going to be learning about the particular tweak for image models which is using a convolution that’ll be a CNN, for language there’s a particular tweak we do called using recurrent models or an RNN, but they’re very minor tweaks on what we’ve just described. So basically it turns out with an RNN, that it can learn that San plus Francisco has a different meaning when those two things are together. . | Some satellite images have 4 channels. How can we deal with data that has 4 channels or 2 channels when using pre-trained models? [1:59:09] . I think that’s something that we’re going to try and incorporate into fast AI. So hopefully, by the time you watch this video, there’ll be easier ways to do this. But the basic idea is a pre-trained ImageNet model expects a red green and blue pixels. So if you’ve only got two channels, there’s a few things you can do but basically you’ll want to create a third channel. You can create the third channel as either being all zeros, or it could be the average of the other two channels. So you can just use you know normal PyTorch arithmetic to create that third channel. You could either do that ahead of time in a little loop and save your three channel versions, or you could create a custom dataset class that does that on demand. . For 4 channel, you probably don’t want to get rid of the 4th channel. So instead, what you’d have to do is to actually modify the model itself. So to know how to do that, we’ll only know how to do in a couple more lessons time. But basically the idea is that the initial weight matrix (weight matrix is really the wrong term, they’re not weight matrices; their weight tensors so they can have more than just two dimensions), so that initial weight tensor in the neural net, one of its axes is going to have three slices in it. So you would just have to change that to add an extra slice, which I would generally just initialize to zero or to some random numbers. So that’s the short version. But really to understand exactly what I meant by that, we’re going to need a couple more lessons to get there. . | . Links and References . Link to Lesson 3 lecture | Homework notebooks: Notebook 1: lesson3-planet.ipynb | Notebook 2: lesson3-camvid.ipynb | Notebook 3: lesson3-imdb.ipynb | Notebook 4: lesson3-head-pose.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson3 Detailed Notes. | Universal approximation theorem + more: http://neuralnetworksanddeeplearning.com | Source for Jeremy says: https://forums.fast.ai/t/things-jeremy-says-to-do/36682 | Cyclical Learning Rates for Training Neural Networks paper by Leslie Smith | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/23/fast-ai-lesson-3-notes-multi-label-segmentation.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/23/fast-ai-lesson-3-notes-multi-label-segmentation.html",
            "date": " • Jul 23, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "SGD From Scratch in PyTorch",
            "content": "Overview . In this post I explore Stochastic Gradient Descent (SGD) which is an optimization method commonly used in neural networks. This continues Lesson 2 of fast.ai on Stochastic Gradient Descent (SGD). I will copy from the fast.ai notebook on SGD and dig deeper into the what’s going on there. . Linear Regression . We will start with the simplest model - the Linear model. Mathematically this is represented as: . [ vec{y} = X vec{a} + vec{b}] . Where $X$ is a matrix where each of the rows is a data point, $ vec{a}$ is the vector of model weights, and $ vec{b}$ is a bias vector. In the 1D case, these would correspond to the familiar ‘slope’ and ‘intercept’ of a line. We can make this more compact by combining the bias inside of the model weights and adding an extra column to $X$ with all values set to one. These are represented in Pytorch as tensors. . In Pytorch, a tensor is a data structure that encompasses arrays of any dimension. A vector is a tensor of rank 1, while a matrix is a tensor of rank 2. For simplicity we will stick to the case of a 1D linear model. In PyTorch $X$ would then be: . n=100 x = torch.ones(n,2) x[:,0].uniform_(-1.,1) . The model has two parameters and there are n=100 datapoints. x therefore has shape (100, 2). The .uniform_(-1., 1) generates floating point numbers between -1 and 1. The trailing _ is PyTorch convention that the function operates inplace. . Let’s look at the first 5 values of x: . &gt; x[:5] tensor([[ 0.7893, 1.0000], [-0.7556, 1.0000], [-0.0055, 1.0000], [-0.2465, 1.0000], [ 0.0080, 1.0000]]) . Notice how the second column is all 1s - this is the bias. . We’ll now set the true values for the model weights, $a$, to slope=3 and intersection=10: . a = tensor(3.,10) a_true = a . With x and a set we can now generate some fake data with some small normally distributed random noise: . y = x@a + torch.randn(n) * 0.6 . . Loss Function . We want to find parameters (weights) a such that they minimize the error between the points and the line x@a. Note that here a is unknown. For a regression problem the most common error function or loss function is the mean squared error. In python this function is: . def mse(y_hat, y): return ((y_hat-y)**2).mean() . Where y is the true value and y_hat is the predicted value. . We start with guess at the value of the weights a: . a = tensor(-1, 1) . We can make prediction for y, y_hat, and compute the error against the known values: . &gt; y_hat = x@a &gt; mse(y_hat, y) tensor(92.9139) . So far we have specified the model (linear regression) and the evaluation criteria (or loss function). Now we need to handle optimization; that is, how do we find the best values for a? How do we find the best fitting linear regression. . Gradient Descent . We would like to find the values of a that minimize mse_loss. Gradient descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient. Here is gradient descent implemented in PyTorch: . a = nn.Parameter(a) lr = 1e-1 def update(): y_hat = x@a loss = mse(y, y_hat) loss.backward() with torch.no_grad(): # don&#39;t compute the gradient in here a.sub_(lr * a.grad) a.grad.zero_() for t in range(100): update() . We are going to create a loop. We’re going to loop through 100 times, and we’re going to call a function called update. That function is going to: . Calculate y_hat (i.e. our prediction) | Calculate loss (i.e. our mean squared error) | Calculate the gradient. In PyTorch, calculating the gradient is done by using a method called backward. Mean squared error was just a simple standard mathematical function. PyTorch keeps track of how it was calculated and lets us automatically calculate the derivative. So if you do a mathematical operation on a tensor in PyTorch, you can call backward to calculate the derivative and the derivative gets stuck inside an attribute called .grad. | Then take the weights a and subtract the gradient from them (sub_). There is an underscore there because that’s going to do it in-place. It’s going to actually update those coefficients a to subtract the gradients from them. Why do we subtract? Because the gradient tells us if the whole thing moves downwards, the loss goes up. If the whole thing moves upwards, the loss goes down. So we want to do the opposite of the thing that makes it go up. We want our loss to be small. That’s why we subtract. | lr is our learning rate. All it is is the thing that we multiply by the gradient. | . Animate it! . Here is an animation of the training gradient descent with learning rate LR=0.1 . . Notice how it seems to spring up to find the intercept first then adjusts to get the slope right. The starting guess at the intercept is 1, while the real value is 10. At the start this would cause the biggest loss so the we would expect the gradient on the intercept parameter to be higher than the gradient on the slope parameter. . It sucessfully recovers, more or less, the weights that we generated the data with: . &gt; a tensor([3.0332, 9.9738] . Stochastic Gradient Descent . The gradient descent algorithm calculates the loss across the entire dataset every iteration. For this problem this works great, but it won’t scale. If we were training on imagenet then we’d have to compute the loss on 1.5 million images just to do a single update of the parameters. This would be both incredibly slow and also impossible to fit into computer memory. Instead we grab random mini-batches of 64, or so, data points and compute the loss and gradient with those and then update the weights. As code this looks almost identical to before, but with some random indexes added to x and y: . def update_mini(rand_idx): y_hat = x[rand_idx]@a loss = mse(y[rand_idx], y_hat) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . Using mini-batches approximates the gradient, but also adds random noise to the optimiser causing the parameters to ‘jump around’ a little. This can make it require more iterations to converge. We will see this visually in the next section. On the other hand, some random noise is a good thing in training neural networks because it allows the optimiser to better explore the high dimensional parameter space and potentially find a solution with a lower loss. . Animate it! . Here is an animation of the training with batch size of 16: . . It converges on the same answer as gradient descent, but it is a little slower and has a bit of jitter that isn’t in the gradient descent animation. . Experiments with the Learning Rate and Batch Size . We can gain a better understanding of how SGD works by playing with the parameters, learning rate and batch size, and visualising the learning process. . Learning Rate . Here the learning rate in SGD is varied, keeping the batch size fixed at 16. . Parameters Animation . SGD LR=1e-2bs=16 | | . SGD LR=1e-1bs=16 | | . SGD LR=1.0 bs=16 | | . With the learning rate of 0.01 it too small and it takes an age, but it does eventually converge on the right answer. With a learning rate of 1.0 the whole thing goes off the rails and it can’t get anywhere near the right answer. . Batch Size . Here the batch size in SGD is varied, holding the learning rate fixed at LR=0.1: . Parameters Animation . SGD bs=1 | | . SGD bs=2 | | . SGD bs=4 | | . SGD bs=8 | | . SGD bs=16 | | . SGD bs=32 | | . SGD bs=64 | | . All of the instances do converge to the right answer in this case (though in general that wouldn’t be the case for all problems). For bs=1 it jumps around a lot even after it gets into the right place. This is because the weights are updated using only one data point every iteration. So it jitters around the right solution and will never stop jittering with more iterations. . However with increasing batch size the jitter gets less and less. At batch size of 64 the animation is almost identical to the gradient descent animation. This makes sense since n=100 so with bs=64 we have almost gone back to the full gradient descent algorithm (which would be bs=n ). . References . Link to Lesson 2 lecture | SGD From Scratch Notebook | Lesson2 Detailed Notes by @hiromi. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html",
            "date": " • Jul 13, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Fast.ai v3 Lesson 2 Notes: Data Cleaning and Production",
            "content": "Overview of Lesson . This lesson has two parts. The first part is about constructing a image classifier from your own data. It details data collection from Google images, creating a validation set, and cleaning the data using the model. . In the second part, we construct a simple linear model from scratch in PyTorch and train it using gradient descent and stochastic gradient descent. That part got quite lengthy so I made it its own blog post here. . Download Your Own Image Data . There is a trick to downloading data from google images. You can do the search manually for the images, then run some javascript magic to get the URLs for the images. You can then save these in a file and then download them from the command line. . Go to Google images and search for your desired images. . | Open the browser javascript console: (⌘+⎇+J on Mac, Crtl+Shift+J on Windows/Linux). . | Run the following the console: . urls = Array.from(document.querySelectorAll(&#39;.rg_di.rg_meta&#39;)).map(el=&gt;JSON.parse(el.textContent).ou); window.open(&#39;data:text/csv;charset=utf-8,&#39; + escape(urls.join(&#39; n&#39;))); . | This initiates a download of a CSV that contains all the urls to the images shown on Google images. . | Use fastai’s download_images function and pass it the path to the CSV file as the argument. . | Remove images that aren’t valid. Use fastai’s verify_images to delete these. . | Then Train With A CNN . Following the steps from Lesson 1: . Load data using the DataBunch API: . np.random.seed(42) # fix seed for to get same validation set data = ImageDataBunch.from_folder(path, train=&#39;.&#39;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats) . | Create the CNN learner and specify the architecture: . learn = create_cnn(data, models.resnet34, metrics=error_rate) . | First fit the head of the pretrained CNN with a few cycles: . learn.fit_one_cycle(4) ... . | Then unfreeze the body of the pretrained CNN: . learn.unfreeze() . | Run the learning rate finder: . learn.lr_find() . | Inspect the learning rate graph and find the strongest downward slope whose negative trend persists for while with the increasing learning rate. Try to pick a learning rate corresponding to the steepest part of this slope. . . | Train the whole network again for a few cycles using a range of learning rates for each layer group, with the learning rate you picked being the highest. This is called Discriminative Layer Training in fastai. . learn.fit_one_cycle(2, max_lr=slice(3e-5, 3)) . | In the Bear example Jeremy does this produces an error rate of 1.4% with a few hundred images and a few minutes of training time on a GPU. . Intepretation . For a classification task such as the Bear example in the lecture, you want to look at the confusion matrix to see where it is failing (well, except where you have loads of classes). FastAI has a handy class for interpreting classification results: . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . . Pretty good - only one mistake! . Cleaning Up Your Dataset . Maybe there is noise or mistakes in your dataset. If we download images from google then perhaps there are images that are of the wrong thing. We want to clean it up. Here is where human intelligence and a computer learner can be combined! It’s very unlikely that a mislabeled data is going to be predicted correctly and with high confidence. We can look at the instances that the computer learner is least confident about - i.e. the instances with the highest loss. There is a nice widget for Jupyter notebook for inspecting and deleting things called FileDeleter: . from fastai.widgets import * losses,idxs = interp.top_losses() top_loss_paths = data.valid_ds.x[idxs] . After cycling through FileDeleter and deleting the bad data you should eventually see fewer and fewer bad data points. At this point you are done and should retrain your model on the cleaned dataset. . Generally these CNN models are pretty good at handling small amounts of noise so this data cleaning will normally give you a small improvement. . Putting your Model into Production . You probably want to use CPU for inference, except for massive scale (and you almost certainly don’t need to train in real time). GPU is only effective if you can get things into neat batches with sizes like 64, which exploits the GPU parallelism. In PyTorch you can specify CPU via: . fastai.defaults.device = torch.device(&#39;cpu&#39;) . Let’s use the trained model for inference. We upload an image of a bear and store that in a variable img: . img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) . . And as per usual, we created a data bunch, but this time, we’re not going to create a data bunch from a folder full of images, we’re going to create a special kind of data bunch which is one that’s going to grab one single image at a time. So we’re not actually passing it any data. The only reason we pass it a path is so that it knows where to load our model from. That’s just the path that’s the folder that the model is going to be in. . You also need to pass it the same transformations , size, and normalizations that you used when training the CNN. You then create_cnn with this fake dataset and then load the weights that were saved in the training phase: . classes = [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddys&#39;] data2 = ImageDataBunch.single_from_classes(path, classes, tfms=get_transforms(), size=224).normalize(imagenet_stats) learn = create_cnn(data2, models.resnet34) learn.load(&#39;stage-2&#39;) . Then prediction is done using the predict method and passing in the real single image data: . pred_class,pred_idx,outputs = learn.predict(img) pred_class &gt; &#39;black&#39; . This is the engine of an web-app. The rest of the app can be coded up in a framework like Flask or Starlette. Here is a great example that uses Starlette: cougar-or-not. . There are services for hosting, such as: https://www.pythonanywhere.com/ . Things That Can Go Wrong . The problems will basically be either: . The learning rate is too high or too low | The number of epochs is too many or too few | . Learning rate too high: basically ruins everything and results in a super high validation loss . Learning rate too low: the error rate goes down really slowly. The other thing you see if your learning rate is too small is that your training loss will be higher than your validation loss. You never want a model where your training loss is higher than your validation loss. That always means you are under-fitting which means either your learning rate is too low or your number of epochs is too low. So if you have a model like that, train it some more or train it with a higher learning rate. . Number of epochs too few: training loss much higher than validation loss, which is a symptom of under-fitting. It needs to learn more. . Number of epochs too many: Too many epochs create something called “overfitting”. If you train for too long as we’re going to learn about it, it will learn to recognize your particular teddy bears but not teddy bears in general. . This is a good post about diagnosing your fit in machine learning: machine learning mastery. . The Truth About Overfitting . The only thing that tells you you are overfitting is that the error rate improves for a while and then starts getting worse again. . Myth: If the training loss is less than the validation loss then the model is overfitting. Absolutely not true. . Any model that is trained correctly will always have a lower training loss than validation loss . You want your model to have a low error. So as long as you’re training and your model error is improving, you’re not overfitting. . In Jeremy’s option, despite what you hear, it’s actually very hard to overtrain in deep learning. . Underfitting . How can the training loss be higher than the validation loss? This doesn’t really seem like it could happen except if you had some contrived validation set. It can however happen quite easily with training neural networks because of dropout. . Dropout is turned on while training and turned off in the validation. If the result is made much worse by dropout then it means that the network has not learned sufficiently well and it is therefore underfitting. Ways to fix this are: train with more epochs, use higher learning rate, use less dropout, or adjust weight decay parameters. . Batch Norm also operates differently at training and test time. . SGD From Scratch . This part kind of outgrew this blog post so I have spun this out into its own blog post here. . Jeremy Says… . If forum posts are overwhelming, click “summarize this topic” at the bottom of the first post. (Only works for &gt;50 replies). | Please follow the official server install/setup instructions, they work and are easy. | It’s okay to feel intimidated, there’s a lot, but just pick one piece and dig into it. Try to push a piece of code, or learn a concept like regular expressions, or create a classifier, or whatever. Context: Lesson 2: It’s okay to feel intimidated 30 | If you’re stuck, keep going. See image below! Context: Lesson 2: If you’re stuck, keep going 38 | If you’re not sure which learning rate is best from plot, try both and see. | When you put a model into production, you probably want to use CPU for inference, except at massive scale. Context: Lesson 2: Putting Model into Production 17 | Most organizations spend too much time gathering data. Get a small amount first, see how it goes. | If you think you’re not a math person, check out Rachel’s talk: There’s no such thing as “not a math person” 56. My own input: only 6 minutes, everyone should watch it! | . (Source: Robert Bracco) . Q &amp; A . When generating new image dataset, how do you know how many images are enough? What are ways to measure “enough”? . That’s a great question. Another possible problem you have is you don’t have enough data. How do you know if you don’t have enough data? Because you found a good learning rate (i.e. if you make it higher than it goes off into massive losses; if you make it lower, it goes really slowly) and then you train for such a long time that your error starts getting worse. So you know that you trained for long enough. And you’re still not happy with the accuracy﹣it’s not good enough for the teddy bear cuddling level of safety you want. So if that happens, there’s a number of things you can do and we’ll learn pretty much all of them during this course but one of the easiest one is get more data. If you get more data, then you can train for longer, get higher accuracy, lower error rate, without overfitting. . Unfortunately, there is no shortcut. I wish there was. I wish there’s some way to know ahead of time how much data you need. But I will say this﹣most of the time, you need less data than you think. So organizations very commonly spend too much time gathering data, getting more data than it turned out they actually needed. So get a small amount first and see how you go. . | What do you do if you have unbalanced classes such as 200 grizzly and 50 teddies? . Nothing. Try it. It works. A lot of people ask this question about how do I deal with unbalanced data. I’ve done lots of analysis with unbalanced data over the last couple of years and I just can’t make it not work. It always works. There’s actually a paper that said if you want to get it slightly better then the best thing to do is to take that uncommon class and just make a few copies of it. That’s called “oversampling” but I haven’t found a situation in practice where I needed to do that. I’ve found it always just works fine, for me. . | . Links and References . Link to Lesson 2 lecture | Homework notebooks: Notebook 1: lesson2-download.ipynb | Notebook 2: lesson2-sgd.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson2 Detailed Notes. | This is an in-depth tutorial on PyTorch: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e | How (and why) to create a good validation set by @rachel | There’s no such thing as “not a math person” by @rachel | Responder - a web app framework built on top of Starlette | Post about an alternative image downloader/cleaner by @cwerner | A tool for excluding irrelevant images from Google Image Search results by @melonkernel | Machine Learning is Fun - source of image/number GIF animation shown in lesson | A systematic study of the class imbalance problem in convolutional neural networks, mentioned by Jeremy as a way to solve imbalanced datasets. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/12/fast-ai-lesson-2-notes-data-cleaning-and-production.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/12/fast-ai-lesson-2-notes-data-cleaning-and-production.html",
            "date": " • Jul 12, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Fast.ai v3 Lesson 1 Notes: Image Classification",
            "content": "Overview of Lesson . This is the introductory lesson to fastai part 1! . The key outcome of this lesson is that we’ll have trained an image classifier which can recognize pet breeds at state of the art accuracy. The key to this success is the use of transfer learning, which will be a key platform for much of this course. We’ll also see how to analyze the model to understand its failure modes. In this case, we’ll see that the places where the model is making mistakes is in the same areas that even breeding experts can make mistakes. . Task 1 - World Class Image Classifier . Fastai opts to teach deep learning backwards - rather than starting at the level of neurons they start with learning to use the state of the art algorithms and networks from the beginning. Learning to become a practitioner with the best practices first and then gradually learning the technical details later. . | Task 1: Training a world class image classification model. . | Image classification has been one of deep learning’s biggest successes so far. . | 10 years ago separating cat and dog images was a hard problem. With classical methods researchers were scoring ~80%. With today’s algorithms it’s actually too easy and scores on the cats vs dogs dataset are almost 100%. That’s why we used the harder dataset of cat and dog breeds. . | Cat breeds and dog breeds dataset from Oxford: Cats and Dogs Breeds Classification Oxford Dataset | Kaggle. . | This task found in the Jupyter notebook: lesson1-pets.pynb . | . Load the Data . FastAI has its own way of handling different datasets: the DataBunch API. . | This integrates the loading of different types of data, the labeling, splitting into train/val/test, and the data transformations for standardisation, normalisation, and training augmentation. . | To load the cats and dogs dataset: . pat = r&#39;/([^/]+)_ d+.jpg$&#39; data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=64, ).normalize(imagenet_stats) . | What is this doing? Let’s look at the docs: . class ImageDataBunch(fastai.basic_data.DataBunch) | DataBunch suitable for computer vision. ... | from_name_re(path:Union[pathlib.Path, str], fnames:Collection[pathlib.Path], pat:str, valid_pct:float=0.2, **kwargs) from builtins.type | Create from list of `fnames` in `path` with re expression `pat`. . | get_transforms is a function that returns a list of default image transformations for data augmentation. | size=224 resizes the images to 224x224. This is the size that the network we are using (resnet34) has been trained on. | bs is the batchsize. | normalize(imagenet_stats) normalizes the images so that the pixel values are between 0 and 1 (necessary for using the neural network). The network has been pretrained on imagenet data so we need to normalize our new data with respect to the imagenet data. This way the images are placed near the distribution that the network was trained on and so gives the network data that it is ‘used to’ seeing. | . Training a Model using a Pretrained ResNet . ‘ResNet’ is the name of a particular kind of Convolutional Neural Network (CNN). Details of it will be covered later. . | The ResNet we will use has been pretrained. This means that it was trained to solve another image classification problem (namely ImageNet) and we are reusing the learned weights of that network as a starting point for a new imaging problem. . | Why ResNet and not some other architecture? From looking at benchmarks it has been found that ResNet generally ‘just works well’ for image tasks. (See also question in Q &amp; A section below). . | Here’s how to create a CNN with the fastai library: . learn = create_cnn(data, models.resnet34, metrics=error_rate) . | data is the DataBunch object of the cats/dogs data we created earlier. . | Here we are using a variant of ResNet called resnet34. The 34 simply means it has 34 layers. There are others avaiable with 18, 50, and more layers. . | One-cycle policy: . learn.fit_one_cycle(4) . | People train neural networks using Stochastic Gradient Descent (SGD). Here the training set is divided into random batches (say of size 64) and the network weights are updated after each batch. After the network has seen all the batches, this is called an epoch. The rate at which the weights are changed is called the learning rate. Typically people set this to a single value that remains unchanged during an epoch. Not here though. . | The One-cycle policy is a way of training the neural network using SGD faster by varying the learning rate and solver momentum over a group of epochs. . | Sylvain explains (source): . He [Leslie] recommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude. . | Here are plots of how the learning rate and momentum vary over the iterations (batches): . . | The peak of the learning rate has a value of 1x the inputted learning rate. The bottom value is 0.1x the inputted learning rate. The bottom of the momentum value is 0.85x the inputted momentum value. . | The momentum varies contra to the learning rate. What’s the intuition behind this? When the learning rate is high we want momentum to be lower. This enables the SGD to quickly change directions and find a flatter region in parameter space. . | . | Learning Rate Finder . The method is basically successively increasing $ eta$ every batch using either a linear or exponential schedule and looking the loss. While $ eta$ has a good value, the loss will be decreasing. When $ eta$ gets too large the loss will start to increase. You can plot the loss versus $ eta$ and see by eye a learning rate that is largest where the loss is decreasing fastest. . | learn.lr_find() learn.recorder.plot() . | . | More on this will be covered in the next lesson. | . | . Getting Started With the Notebooks . All the course notebooks for part 1 are found here: notebooks [github]. | The course guide can be found here: course guide. | For running and experimenting with the fastai notebooks I personally like to use: kaggle kernels | or google colab. | . | . Jeremy Says… . Don’t try to stop and understand everything. | Don’t waste your time, learn Jupyter keyboard shortcuts. Learn 4 to 5 each day. | Please run the code, really run the code. Don’t go deep on theory. Play with the code, see what goes in and what comes out. | Pick one project. Do it really well. Make it fantastic. | Run this notebook (lesson1-pets.ipynb), but then get your own dataset and run it! (extra emphasis: do this!) If you have a lot of categories, don’t run confusion matrix, run… interp.most_confused(min_val=n) | (Source: Robert Bracco) . Q &amp; A . As GPU mem will be in power of 2, doesn’t size 256 sound more practical considering GPU utilization compared to 224? . The brief answer is that the models are designed so that the final layer is of size 7 by 7, so we actually want something where if you go 7 times 2 a bunch of times (224 = 7x2x2x2x2x2), then you end up with something that’s a good size. Objects often appear in the middle of an image in the ImageNet dataset. After 5 maxpools, a 224x224 will be 7x7 meaning that it will have a centerpoint. A 256x256 image will be 8x8 and not have a distinct centerpoint. . | Why resnet and not inception architecture? . Resnet is Good Enough! See the DAWN benchmarks - the top 4 are all Resnet.You can consider different models for different use cases. For example, if you want to do edge computing, mobile apps, Jeremy still suggests running the model on the local server and port results to the mobile device. But if you want to run something on the low powered device, there are special architectures for that. . Inception is pretty memory intensive. fastai wants to show you ways to run your model without much fine-tuning and still achieve good results. The kind of stuff that always tends to work. Resnet works well on a lot of image classification applications. . | Will the library use multi GPUs in parallel by default? . The library will use multiple CPUs by default but just one GPU by default. We probably won’t be looking at multi GPU until part 2. It’s easy to do and you’ll find it on the forum, but most people won’t be needing to use that now. . | . Links and References . Link to Lesson 1 lecture | Homework notebooks: Notebook 1: lesson1-pets.pynb | . | Detailed lesson notes - thanks to @hiromi | Stanford DAWN Deep Learning Benchmark (DAWNBench) · | [1311.2901] Visualizing and Understanding Convolutional Networks | Another data science student’s blog – The 1cycle policy | Learning Rate Finder Paper | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/computer-vision/fastai/fastai-v3-notes/2019/07/06/fast-ai-lesson-1-notes-image-classification.html",
            "relUrl": "/deep-learning/machine-learning/computer-vision/fastai/fastai-v3-notes/2019/07/06/fast-ai-lesson-1-notes-image-classification.html",
            "date": " • Jul 6, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "My First Pull Request! :)",
            "content": "I’m very proud to have made my first open source contribution! :-) I added a feature to the FastAI deep learning library to make its data types ‘countable’ and thus work with the collections.Counter class. . Problem . While I was working through the image classification homework from lesson 2. I wanted to check the how many images of each class there were in my data block. The best way to count the number of values in a collection in python is to use the collections.Counter class which creates a dictionary mapping value to count. . However when I tried this with the data block I got this: . &gt; Counter(data.train_ds.y) Counter({ Category chimp 1 Category gorilla 1 Category gorilla 1 Category chimp 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 ... . Solution . This problem is caused by fact that there was no __eq__ implemented for the Category class. When different Category objects were compared python’s default equality would only check whether they were literally the same object rather than checking their values. To get an object to work with a dictionary class in python you also have to implement a __hash__ method. . I confirmed this with the hot patch: . &gt; Category.__eq__ = lambda self, that: self.data == that.data &gt; Category.__hash__ = lambda self: hash(self.obj) &gt; Counter(data.train_ds.y) Counter({Category orangutan: 56, Category gorilla: 177, Category chimp: 173}) . With Sylvain Gugger’s guidance, I then implemented __eq__ method properly in fastai for the ground class ItemBase so that all of the different data classes in fastai could have equality. Hash didn’t make sense for all the subclasses (like floats or arrays of numbers), so we compromised on implementing hash methods only on the subclasses where it made sense. . Here is the link to my pull request: https://github.com/fastai/fastai/pull/1717. . Aside: Making Python Objects Counter-Ready . In order to make your python objects play nice with dictionary’s they need to override two python built-ins: . __eq__ | __hash__ | Suppose that the python object contains some value val that defines the object’s uniquness. . Let’s create a python class for categories called Cat: . class Cat: def __init__(self, val): self.val = val def __str__(self): return f&#39;Cat({self.val})&#39; def __repr__(self): return f&#39;Cat({self.val})&#39; . This class won’t work properly with Counters: . &gt; xs = [Cat(2), Cat(2), Cat(1), Cat(3)] &gt; Counter(xs) Counter({Cat(2): 1, Cat(2): 1, Cat(1): 1, Cat(3): 1}) . Equality doesn’t work: . &gt; Cat(2) == Cat(2) False . Two objects with the same value don’t have the same hash: . &gt; hash(Cat(2)) -9223372036573193412 &gt; hash(Cat(2)) 281542562 . You have to implement the hash and equality built-ins: . class Cat: def __init__(self, val): self.val = val def __str__(self): return f&#39;Cat({self.val})&#39; def __repr__(self): return f&#39;Cat({self.val})&#39; def __eq__(self, other): return self.val == other.val def __hash__(self): return hash(self.val) . Now it works: . &gt; xs = [Cat(2), Cat(2), Cat(1), Cat(3)] Counter(xs) Counter({Cat(2): 2, Cat(1): 1, Cat(3): 1}) . How does this work? The Cat objects are being used as keys in the Counter. When a new Cat object comes into the Counter we need to compare it with all the other keys already in the Counter. If a Cat object of the same value is there already then we need to increment the value associated with that Cat object. . For efficiency, however, dictionaries in python don’t store the keys in a big list rather in buckets. When a new Cat object comes into the Counter it is assigned to a bucket using its hash value. Here three things can happen. . If the bucket is empty then store the value there. | If the bucket isn’t empty compare the incoming object with the objects there using eq. If they are the same, increment the counter. | If they are different you have a hash collision. Store the incoming object in the bucket and set the counter value to 1. | In Summary . For correctness with dictionaries: . obj1 == obj2 if obj1.val == obj2.val | hash(obj1) == hash(obj2) if obj1.val == obj2.val | If the object is to be used as a key in a Counter we need to be able to correctly compare it to other keys in the Counter. If two objects are equal then we know that they are the same key and we can increment the counter. Two objects with the same value need to be hashed to the same bucket. . For efficiency with dictionaries: . hash(obj1) should ideally != hash(obj2) if obj1.val != obj2.val | It is possible, though undesirable, that two objects with different values get hashed to the same bucket. This is called a hash collision. This isn’t a correctness problem, rather an efficiency problem. Every hash collision is like an if statement in the dictionary to specially handle those cases. Ideally, every unique value should have its own unique hash so that there are no hash collisions. If we imagine the worst case where our hashing algorithm is something like def hash(x): return 0, then ever item ends up in the same bucket. To look up an item in this dictionary where everything is a hash collision we’d have to brute-force it and on average look at every item individually before we find what we are looking for. This would reduce the dictionary look-up performance to $ mathcal{O}(N)$, the same as an unordered list, instead of the $ mathcal{O}(1)$ performance that it should have. . Links . Link to pull request: https://github.com/fastai/fastai/pull/1717 | Link to forum post: https://forums.fast.ai/t/get-value-counts-from-a-imagedatabunch/38784/21 | .",
            "url": "https://jimypbr.github.io/blog/python/fastai/2019/04/01/my-first-pull-request.html",
            "relUrl": "/python/fastai/2019/04/01/my-first-pull-request.html",
            "date": " • Apr 1, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "SIFTing 3 Million Images",
            "content": ". The Fleuron Project . I have been involved in the Fleuron project this year. The aim of this project is to use computer vision to extract printers’ ornaments from a large corpus of ~150,000 scanned documents (32 million pages) from the 18th century. Printed books in the 18th century were highly decorated with miniature pieces of printed artwork - ‘ornaments’. Their pages were adorned with ornaments that ranged from small floral embellishments to large and intricate head- and tailpieces, depicting all manner of people, places, and things. Printers’ ornaments are of interest to historians from many disciplines, not least for their importance as examples of early graphic design and craftsmanship. They can help solve the mysteries of the book trade, and they can be used to detect piracy and fraud. . In this project an OpenCV based code was developed to automatically detect ornaments in a scanned image of a page and extract them into their own file. This code worked very well and extracted over 3 million images from the data, but it was quite over-sensitive in its detection so there were many false-positives. The code was heuristic based and didn’t use any machine intelligence to further evaluate the potential images for validity. We therefore chose to tune the code to have good recall at the expense of precision – i.e. we would rather it didn’t miss valid images, even if it means that some invalid images get through too. Often these invalid images were of blocks of text so we initially experimented with using OCR to catch these cases. However this had the unwanted effect of making recall worse. We decided a better solution would be to train a machine learning classifier to discriminate between the valid and invalid images. . My contribution to the project was to use the extraction code to generate data, which I then hand-labelled to create a training set to train a machine learning based filter to remove the bad images. The final filtered dataset is presented on the website: http://fleuron.lib.cam.ac.uk (EDIT 2021: Fleuron now defunct. Has since become Compositor), which I also designed and built. In this blog post I will describe the methodology and results of the image filtering part of the project, which was the first time I ever used machine learning in a real project. . Extraction . The first challenge is to extract the ornaments from the raw page scans. This is an example of a page containing two ornaments, at the top of the page and at the start of the text body: . . We required an algorithm that could ignore the text and draw bounding boxes around the two ornaments on the page. To solve this problem we enlisted Dirk Gorissen to develop a method using Python and OpenCV. I will not dive deeply into how Dirk’s algorithm works here. Basically it uses combines heuristics of where ornaments are typically located and how they look with various image filtering techniques to weed out text and other artifacts on the page to leave just the artwork intact. . Here is a demonstration of how each of the different stages of the algorithm work using on the single page shown above as an example: . . Ornaments are visually very dense compared to the text. In the first stage the image is cleaned removing dust and stains in the white space of the page. Then through several iterations of blurring and contouring are applied until just the ornaments are left as single contours as seen in stage 5. A bounding box is then drawn around these contours and content of these boxes is then extracted from the original image. . This method is simple and effective, but it is also apt to falsely classifying blocks of text. In the following example you can see clearly how this can happen: . . After running extraction on all of the pages, I found that in a random sample of the images, most of them were just images of blocks of text! However, given that most of the pages in the dataset contain only text and no ornaments, perhaps this is to be expected even if the algorithm is fairly good at removing text. . After extracting the ornaments from a large sample of the books, I hand labeled a random sample of 15000 images as valid and invalid. Here is a collage of valid images: . . Here is a collage of invalid images that we want to filter out: . . Image Filtering Pipeline . The choice of image representation is essential to getting a well performing machine learning based classifier. The Images are black and white and so we can’t use any colour features and contain very rich textures. The pipeline for the image filtering system: . Create a labelled data set for training | Represent each training image by a vector using Bag of Visual Words | Train a classifier on the vector to discriminate between valid and invalid images | Apply the classifier to unseen images in the data set. | Bag of Visual Words (BoVW) . The Bag of Visual Words (BoVW) method is a common feature representation of images in computer vision. The method is directly inspired by the Bag of Words (BoW) method used in text classificiation. In the BoW method, the basic idea is that a text document is split up into its component words. Each of the words in the document is then matched to a word in the dictionary, and the number of unique words in the document is counted. The text document is then represented as a sparse histogram of word counts that is as long as the dictionary. . This histogram can be interpreted as a vector in some high dimensional space, and two different documents will be represented by two different vectors. So for a dictionary with $D$ words the vector for document $i$ is: . [v_i = [n(w_1,i), n(w_2,i), …, n(w_D, i)]] . Where $n(w)$ counts the number of occurrences of word $w$. The distance between these two vectors (e.g. L2, cosine, etc) can therefore be used as a proxy for the similarity of the two documents. If everything is working well, then a low distance will indicate high similarity and a large distance will represent a high dissimilarity. With this representation we are able to throw machine learning algorithms at the data or do document retrieval. . BoVW is exactly the same method except that instead of using actual words it uses ‘visual words’ extracted from the images. Visual words basically take the form of ‘iconic’ patches or fragments of an image. . 1. Extract notable features from the images . 2. Learn a visual dictionary . Use a clustering algorithm like k-means with a apriori number of clusters (&gt;1000) to learn a set of $k$ compound visual words. . . 3. Quantize features using the visual vocabulary . Now we could then take an image, find its visual words and match each of those words to their nearest equivalent in the dictionary. . 4. Represent images by histogram of visual word counts . By counting how many times a word in the dictionary is matched, the image can be re-represented as a histogram of word counts: . . Similar looking images will have contains many of the same words and counts. . SIFT - The Visual Word . Now that we have outlined the concept of the BoVW method, what do we actually use as the ‘visual word’? To create the visual words I used SIFT - ‘Scale Invariant Feature Transform’. SIFT is a method for detecting multiple interesting keypoints in a grey-scale image and describing each of those points using a 128 dimensional vector. The SIFT descriptor is invariant to scale, rotation, and illumination, which is why it is such a popular method in classification and CBIR. An excellent technical description of SIFT can be found here. . OpenCV has an implementation of a SIFT detector included. The following code finds all the keypoints in an image and draws them back onto the image. . img = cv2.imread(&#39;image_2.png&#39;) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) kp, desc = sift.detectAndCompute(img_gray, None) img2 = cv2.drawKeypoints(img_gray, kp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) cv2.imwrite(&#39;image_sift_2.png&#39;, img2) . Here is the output of this for two images from the dataset, one valid and the other invalid: . There is a simple improvement that can be made to SIFT called RootSIFT. RootSIFT is a small modification to the SIFT descriptor that corrects the L2 distance between two SIFT descriptors. This generally always improves performance for classification and image retrieval. Here is an implementation in python: . def rootsift_descriptor(f): &quot;&quot;&quot; Extract root sift descriptors from image stored in file f :param: f : str or unicode filename :return: desc : numpy array [n_sift_desc, 128] &quot;&quot;&quot; img = cv2.imread(f) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) sift = cv2.SIFT() kp, desc = sift.detectAndCompute(img_gray, None) if desc is None: print(&#39;Warning: No SIFT features found in {}&#39;.format(f)) return None desc /= (desc.sum(axis=1, keepdims=True) + 1e-7) desc = np.sqrt(desc) return desc . Building a Visual Word Dictionary . To create a visual word dictionary we need to first collect and store all the RootSIFT descriptors from a large sample of images from our dataset. Here I used a sample size of 50,000 images. For 50,000 images, $N approx 1~ billion$. The following script iterates through every image in the target directory, finds the RootSIFT descriptors of the image, and then stores them in a large $N times128$ array in a HDF5 file. . import os import glob import cv2 import tables import numpy as np from bovw import rootsift_descriptor from random import sample, shuffle def create_descriptor_bag(filelist): &quot;&quot;&quot; creates an array of descriptor vectors generated from every file in filelist &quot;&quot;&quot; X = np.empty((0, 128), dtype=np.float32) N = len(filelist) for n, f in enumerate(filelist): print(&#39;Processing file: {} of {}...&#39;.format(n, N)) desc = rootsift_descriptor(f) if desc is not None: X = np.vstack((X, desc)) return X def main(): h5f = tables.openFile(&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;, &#39;w&#39;) atom = tables.Atom.from_dtype(np.dtype(&#39;Float32&#39;)) ds = h5f.createEArray(h5f.root, &#39;descriptors&#39;, atom, shape=(0, 128), expectedrows=1000000) PATH = &#39;/home/jb914/ECCO_dict/random50k/&#39; all_files = glob.glob(os.path.join(PATH, &#39;*.png&#39;)) rand_sample = [all_files[i] for i in sample(range(1, len(all_files)), 5000)] chunk_size = 100 for i in xrange(0,len(rand_sample), chunk_size): print(&#39;Creating rootsift descriptor bag...&#39;) X = create_descriptor_bag(rand_sample[i:i+chunk_size]) print(X[0].shape) print(len(X)) print(&#39;Writing file: rootsift_vectors_5000.hdf&#39;) for i in xrange(len(X)): ds.append(X[i][None]) ds.flush() #ds[:] = X h5f.close() if __name__ == &#39;__main__&#39;: main() . HDF5 files are great for storing large multidimensional arrays of data to disk because they store the meta-data of the array dimensions and allow for streaming the data from disk to memory. This is especially useful when the full data is much larger than memory like here. . Creating the dictionary is the hardest and most time consuming part with BoVW. There are many vectors to cluster, the number of words is very large (between 1000 and 1,000,000), and the vectors are high dimensional. This stretches the capabilities of many clustering algorithms in all possible ways. In my experiments I found that the standard K-Means clustering algorithm quickly became intractable for larger numbers of vectors and clusters. Moreover the algorithm is offline - it needs to see all the data at once. Algorithms better suited for this task are approximate k-means (AKM) and mini-batch k-means. . I found success with two open source implementations of these in fastcluster (AKM), and in MiniBatchKMeans from scikit-learn. Fastcluster has the advantage that it uses distributed parallelism via MPI to split the large data up across multiple machines. However this useful code lacks documentation and no longer maintained. MiniBatchKMeans on the other hand isn’t parallel, however it does allow for streaming of the data through memory so it works great with HDF5. . In my experiments I found that setting the dictionary size to 20,000 words was sufficient. . The following script can stream a HDF5 file in a user defined number of chunks performing clustering with those chunks. The total clustering time for this was approximately 24 hours running in serial on a Intel Xeon Ivybridge CPU. . from __future__ import print_function import numpy as np import tables import pickle from sklearn.cluster import MiniBatchKMeans from time import time def main(n_clusters, chunk=0, n_chunks=32, checkpoint_file=None): datah5f = tables.open_file(&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;, &#39;r&#39;) shape = datah5f.root.descriptors.shape datah5f.close() print(&#39;Read in SIFT data with size in chunks: &#39;, shape) print(&#39;Running MiniBatchKMeans with cluster sizes: &#39;, n_clusters) c = n_clusters print(&#39;n_clusters:&#39;, c) if checkpoint_file: mbkm = pickle.load(open(checkpoint_file, &#39;r&#39;)) else: mbkm = MiniBatchKMeans(n_clusters=c, batch_size=10000, init_size=30000, init=&#39;random&#39;, compute_labels=False) step = shape[0] / n_chunks start_i = chunk*step for i in xrange(start_i, shape[0], step): datah5f = tables.open_file(&#39;/scratch/ECCO/rootsift_vectors_{}.hdf&#39;.format(datasize), &#39;r&#39;) X = datah5f.root.descriptors[i:i+step] datah5f.close() t0 = time() mbkm.partial_fit(X) print(&#39; t ({} of {}) Time taken: {}&#39;.format(chunk, n_chunks, time()-t0)) chunk += 1 pickle.dump(mbkm, open(&#39;chkpt_{}.p&#39;.format(n_clusters), &#39;w&#39;)) X = mbkm.cluster_centers_ print(X.shape) f = tables.open_file(&#39;fleuron_codebook_{}.hdf&#39;.format(n_clusters), &#39;w&#39;) atom = tables.Atom.from_dtype(X.dtype) ds = f.create_carray(f.root, &#39;clusters&#39;, atom, X.shape) ds[:] = X f.close() if __name__ == &#39;__main__&#39;: main(n_clusters=20000, chunk=0, n_chunks=4, checkpoint_file=None) . Matching Key Points to the Dictionary . With the dictionary created the next step is to represent all the images in the labeled training set as a histogram of matching keypoints. This is a nearest neighbour matching problem so with a brute force algorithm this is a $O(N)$ so this is slow for very high numbers of words. Faster nearest neighbour matching can be achieved with the FLANN library. OpenCV contains a wrapper for FLANN. I wrote a class that uses the FLANN matcher in OpenCV to match an array of descriptors to a codebook: . FLANN_INDEX_COMPOSITE = 3 FLANN_DIST_L2 = 1 class Codebook(object): def __init__(self, hdffile): clusterf = tables.open_file(hdffile) self._clusters = np.array(clusterf.get_node(&#39;/clusters&#39;)) clusterf.close() self._clusterids = np.array(xrange(0, self._clusters.shape[0]), dtype=np.int) self.n_clusters = self._clusters.shape[0] self._flann = cv2.flann_Index(self._clusters, dict(algorithm=FLANN_INDEX_COMPOSITE, distance=FLANN_DIST_L2, iterations=10, branching=16, trees=50)) def predict(self, Xdesc): &quot;&quot;&quot; Takes Xdesc a (n,m) numpy array of n img descriptors length m and returns (n,1) where every n has been assigned to a cluster id. &quot;&quot;&quot; (_, m) = Xdesc.shape (_, cm) = self._clusters.shape assert m == cm result, dists = self._flann.knnSearch(Xdesc, 1, params={}) return result . The following code takes a list of image files and a dictionary and returns the count vectors for each of those image files using the sparse matrix type from scipy. It is also multi-threaded using the joblib library: . def count_vector(f, codebook): &quot;&quot;&quot; Takes a list of SIFT vectors from an image and matches each SIFT vector to its nearest equivalent in the codebook :param: f : Image file path :return: countvec : sparse vector of counts for each visual-word in the codebook &quot;&quot;&quot; desc = rootsift_descriptor(f) if desc is None: # if no sift features found return 0 count vector return lil_matrix((1, codebook.n_clusters), dtype=np.int) matches = codebook.predict(desc) unique, counts = np.unique(matches, return_counts=True) countvec = lil_matrix((1, codebook.n_clusters), dtype=np.int) countvec[0, unique] = counts return countvec class CountVectorizer(object): def __init__(self, vocabulary_file, n_jobs=1): self._codebook = Codebook(hdffile=vocabulary_file) if n_jobs == -1: self.n_jobs = cpu_count() else: self.n_jobs = n_jobs def transform(self, images): &quot;&quot;&quot; Transform image files to a visual-word count matrix. :param: images : iterable An iterable of str or unicode filenames :return: X : sparse matrix, [n_images, n_visual_words] visual-word count matrix &quot;&quot;&quot; sparse_rows = Parallel(backend=&#39;threading&#39;, n_jobs=self.n_jobs)( (delayed(count_vector)(f, self._codebook) for f in images) ) X = lil_matrix((len(images), self._codebook.n_clusters), dtype=np.int) for i, sparse_row in enumerate(sparse_rows): X[i] = sparse_row return X.tocsr() . Given a list of files the following code will return the count vectors for those images: . vectorizer = CountVectorizer(&#39;codebook_20k.hdf&#39;, n_jobs=-1) Xcounts = vectorizer.transform(images) . tf-idf . Not all words are created equal, some are more frequent than others. This is the same in human language and in the create visual vocabulary. Words like ‘the’, ‘what’, ‘where’, etc will swamp the count vectors of english words in almost all english documents. Clearly they are less interesting than a rare word like ‘disestablishment’ and that we’d like two different documents both containing a word like ‘disestablishment’ to have a high similarity. So we’d like to reweight words which appear in few documents so that they have a higher importance, and words that appear in most documents to have lower importance. In another case, if a document only contained the word ‘disestablishment’ 1000 times should it be 1000 times more relevant than a document containing it once? So within an individual document we may want to reweight words that are repeated over and over so that they cannot artificially dominate. . These two reweightings can be achieved using tf-idf (term frequency inverse document frequency) weighting. This weighting is designed to reflect how important a particular word is in a document corpus. It is perfectly applicable in our visual word case also. Scikit-learn has an implementation of a tf-idf transformer for text classification that we can repurpose here. To following produces the final representation for the training data that we can use in the machine learning algorithm, $X$. . from sklearn.feature_extraction.text import TfidfTransformer transformer = TfidfTransformer() X = transformer.fit_transform(Xcount) . Visualisation of BoVW . That that we’ve transformed the images into tf-idf weighted, 20k dimensional, sparse vectors to visual word counts, we can visualise them and see if there is any apparent structure in this high dimensional data. Great algorithms for visualising high dimensional data are PCA and T-SNE, both of which have implementations in scikit-learn. I found here that PCA worked best. For high dimensional sparse data, the TruncatedSVD algorithm works best: . from sklearn.decomposition import TruncatedSVD svd = TruncatedSVD(n_components=2) Z = svd.fit_transform(X) . We can plot this with the images inlined and with colours representing the valid (red) and invalid (blue) labels: . . You can clearly see that there is clear structure in the higher dimensions and that the valid and invalid images separate quite well from each other. This is quite promising for the performance of a machine learning algorithm! . Classifying the Bad Images with Machine Learning . I tried a number of algorithms including Random forest, logistic regression and linear SVM. I found that SVM with a linear kernel by far performed the best compared to the other algorithms. . from sklearn.svm import LinearSVC clf = LinearSVC() clf.fit(X_train, y_train) . LinearSVC with the default settings performed very well with $97 %$ accuracy. High accuracy is generally a good sign, especially here where the numbers of valid and invalid images are of a similar size. Two other important statistics for classification are precision and recall. . Recall is a measure of what the probability that the classifier will identify and image as invalid given that it is invalid: $P( hat{y}=1 | y=1)$. You can think of recall as the ratio of the number of images correctly classed as invalid over the number of all invalid images. . Precision on the other hand is a measure of the probability that an image is invalid given that the classifier says it is invalid: $P(y=1| hat{y}=1)$. You can think of precision as the ratio of the number of images correctly classed as invalid over the number of all images classified. . The difference between them is subtle (here is a great explanation of the difference), but you may want to favour a trade-off of one for the other depending on your business case. In our case it is worse to misclassify a valid images as invalid because we are losing good images. We would much rather have some invalid images get through than lose good images, which is the same as favouring extra precision over recall. . We can tune the precision by adjusting the class weights of the Linear SVM, such that the penalty for classifying a valid image as invalid is much worse than classifying an invalid image as valid. I used cross-validation to find the best values for these. These give the valid images a weight of 20 and invalid images a weight of 0.1: . from sklearn.svm import LinearSVC clf = LinearSVC(class_weight={0: 20, 1: 0.1}) clf.fit(X_train, y_train) . This yielded the final performance of $95 %$ accuracy, $99.5 %$ precision, and $93.8 %$ recall: . Confusion matrix: [[1134 11] [ 161 2439]] Accuracy score: 0.954072096128 False Positive Rate: 0.00960698689956 False Negative Rate: 0.0619230769231 Precision: 0.995510204082 Recall: 0.938076923077 F1 Score: 0.965940594059 . The performance of approach this is very good. The trained classifier was then applied across the whole image dataset. In the end there were approximately 3 million invalid images and 2 million valid images detected. . Further Ideas . Image search . The BoVW approach is also very useful for image retrieval. This means that given some image we can find duplications and similar looking images in the rest of the image set simply by finding the BoVW vectors that are closest to that image’s own BoVW vector. This is just a nearest neighbour search. It is complicated by the number of images because scaling nearest neighbour search with large numbers of vectors that don’t necessarily fit into memory relies on more complicated algorithms. . Neural Networks . Convolutional Neural Networks (CNNs) have shown great application in image classification in recent years. While they perform well at classification, they also have the advantage that they can discover vector representations of the images given the just the raw pixels. So it doesn’t require all this work with inventing a representation for images such as BoVW. The downside is that they require a lot of data (10s of thousands of examples) to be effective. Rather than hand labelling more examples, it would be quicker to look at the output of images classified by the SVM, and eyeball any false negatives or false positives in there. Artificial data could also be created using image transformations like rotation and inversion. .",
            "url": "https://jimypbr.github.io/blog/python/machine-learning/computer-vision/2016/12/30/fleuronbovw.html",
            "relUrl": "/python/machine-learning/computer-vision/2016/12/30/fleuronbovw.html",
            "date": " • Dec 30, 2016"
        }
        
    
  
    
        ,"post17": {
            "title": "How to do Polymorphism in Clojure (3/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . Arithmetic with Mixed Types . We have so far built a number system with five different types and an add function that can take any two numbers of the same type and add them together with the same interface. . However what if I wanted to add a Complex-r and a Real together? We would need to convert the Real to a Complex-r and then add them together. We could do this by adding more multimethods: . (defmethod add [::complex-r ::real] (...)) (defmethod add [::real ::complex-r] (...)) . You can see that doing this for all combinations of types would lead to a combinatorial explosion of new multimethods! Clojure’s hierarchies can enable us to solve this problem without the need to write a factorial number of functions. . We can side-step the problem of writing a combinatorical number of new functions by noting that to add two different types of number together we have to promote one of the numbers to be the same type as the other. With this in mind we can create a catch-all method for add that catches all the cases of mixed types, coerces the types to be the same and then calls add again with the converted types. It will then call one of the previously defined add methods where the types are the same. . We can implement the catch-all case by creating a new abstraction ::number at the top of the hierarchy where every number we have created so far is a descendant of ::number: . (derive ::complex ::number) (derive ::integer ::number) (derive ::float ::number) (derive ::rational ::number) . We can now create the catch-all add method that will that will matc [::number ::number]: . (defmethod add [::number ::number] [n1 n2] (apply add (coerce-types n1 n2))) . This works because any combination of different types will fail all the rules of the other multimethods except this one, because all of our types are descendants of ::number. This method then calls the coercion function (which I will define later) to convert the arguments into the same type and then calls the add multi-function with these converted arguments. This will then find the correct multimethod for the now uniform types return the result. . Type Coercion with the Numeric Tower . We can convert any Integer to a Rational without a loss of information. You cannot convert any Rational to an Integer without a loss of information though. Similarly any Rational can be converted to a Float, and any Float can be converted to a Complex number. . This chain of conversions is the numeric tower: . Integer -&gt; Rational -&gt; Float -&gt; Complex | . We want to be able to call a function raise on one of our types and get back the same numeric value, but represent by the next type in the tower. The function depends on the type of the argument so we can create another protocol: . (defprotocol PRaise (raise [x] &quot;raises a number type to the next type in the numeric tower with equivalent value&quot;)) (extend-protocol PRaise Int (raise [x] (rational (:n x) 1)) Rational (raise [x] (float (/ (numer x) (double (denom x))))) Float (raise [x] (complex-r (:n x) 0))) . Given a pair of types, e.g. [::integer ::float], we need a way to encode the fact that ::float is higher in the tower than ::integer, and so ::integer needs to be raised until it is a ::float. This could be done with a map associating the types to a rank number, but this is pretty inflexible if we add more types. . The numeric tower is a hierarchy so we are actually better off using Clojure’s ad-hoc hierarchies again. In Clojure there is a global hierarchy structure which we have used so far for the arithmetic, but you are free to create your own hierarchies with make-hierarchy. This exists independently of the global hierarchy we used earlier. This is handy because the numeric tower hierarchy is different from the arithmetic hierarchy. . (def numeric-tower (-&gt; (make-hierarchy) (derive ::complex ::complex-p) (derive ::complex ::complex-r) (derive ::float ::complex) (derive ::rational ::float) (derive ::integer ::rational))) . Using this we can create comparator functions for two keyword types. For example: . =&gt; (higher? ::float ::integer) true =&gt; (lower? ::rational ::complex-r) true . These are easy to implement using the functions for querying hierarchies, ancestors and descendents: . (defn higher? &quot;Is type 1 higher in the numeric tower than type 2?&quot; [t1 t2] (if (contains? (ancestors numeric-tower t2) t1) true false)) (defn lower? &quot;Is type 1 lower in the numeric tower than type 2?&quot; [t1 t2] (if (contains? (descendants numeric-tower t2) t1) true false)) . With these functions we can implement coerce-types: . (defn coerce-types &quot;Given two different number types raise the lesser type up to be the same as the greater type&quot; [x y] (let [t1 (kind x) t2 (kind y)] (cond (lower? t1 t2) (recur (raise x) y) (higher? t1 t2) (recur x (raise y)) :else [x y]))) . Trying this out in a REPL: . =&gt; (raise (integer 3)) #numbers.core.Rational{:n 3, :d 1} =&gt; (raise (float 4)) #numbers.core.Complex-r{:real 4.0, :imag 0} =&gt; (coerce-types (integer 4) (rational 5 6)) [#numbers.core.Rational{:n 4, :d 1} #numbers.core.Rational{:n 5, :d 6}] =&gt; (coerce-types (rational 5 6) (complex-r 7 8)) [#numbers.core.Complex-r{:real 0.8333333333333333, :imag 0} #numbers.core.Complex-r{:real 7, :imag 8}] . Final Product and Further Ideas . We have implemented a number system that can represent integers, floating point numbers, rational numbers, rectangular complex numbers, and polar complex numbers. It can perform basic binary arithmetic operations add, subtract, multiply, and divide on any combination of number types. . Let’s demonstrate the final product in the REPL: . (add (integer 4) (integer 6)) #numbers.core.Int{:n 10} (add (integer 4) (float 6)) #numbers.core.Float{:n 10.0} (add (rational 5 6) (float 6)) #numbers.core.Float{:n 6.833333333333333} (mul (float 6) (complex-p 10 0.5)) #numbers.core.Complex-p{:magnitude 60.0, :angle 0.5} (div (integer 5) (rational 7 5)) #numbers.core.Rational{:n 25, :d 7} (div (integer 5) (complex-r 7 5)) #numbers.core.Complex-p{:magnitude 0.5812381937190965, :angle -0.6202494859828215} . Seems to work fairly well! . I could continue building on this, but that would be beyond the scope of this blog post. There are a few things worth thinking about nonetheless. One aesthetic improvement would be implementing a pretty REPL representation of the custom number types. E.g. having 3/4 instead of Rational(:n 3 :d 4). Clojure has a single function that takes care of printing things: clojure.lang.print-method. This is simply another multimethod like we’ve already been using. Adding nicer printing is straight-forward: . ;; nicer printing for rationals (defmethod print-method Rational [x ^java.io.Writer writer] (.write writer (str (numer x) / (denom x)))) ;; and similarly for the other types... ;; ... . In SICP there is also an exercise to extend the number system to be implemented purely with its own types. So while Integer and Float types would be wrappers for Clojure’s own types, the Rational, Complex-r, and Complex-p types could themselves be composed of any of the other types. So you could have a Rational number whose demoninator and numerator were complex numbers. Or conversely a complex number whose real and imaginary parts were rational numbers. This would be achieved with some modification to the existing code. You would need to replace all uses of Clojure’s primitive arithmetic functions (+, *, etc.) with our multimethods (add, mult, etc), and also create our own implementations of sqrt, sin, cos, and atan that handled our number types. This could be done by adding new protocols. . Final Thoughts . This is a nice non-trivial program from SICP that demonstrates the ideas and challenges in polymorphism. Implementing this in Clojure required us to use just about every feature for polymorphism in Clojure’s core library. But I am satified with how Clojure could handle everything without any requiring any ‘hacks’ or major redesigns. . I think that this highlights one of the good aspects of Clojure’s design and philosophy. Namely, the decoupling (or ‘decomplecting’) of ideas in the language. In this demo there are the following concepts: . Data (in the form of records). | Functions. | Single and multiple dispatch (protocols and multimethods) to functions. | Hierarchies of types. | Functions and data are decoupled because records are just data and you can’t attach methods onto records in Clojure the way you can with class methods in OOP. You don’t need getter/setter methods because records are maps so you just the functions for maps. . Data and dispatch are decoupled. You do not need to know at design time which protocols or multimethods are going to use your record type. In Java where polymorphic single-dispatch is achieved via interfaces or abstract classes you need explicitly implement or extend your class when you write it. In Clojure you can add protocols or multimethods to any existing type when are where you want. . Dispatch in Clojure is simply a fancy wrapper for functions. For a user of a protocol, multimethod, or function it makes no difference how it is implemented - it looks exactly the same. This is a big win for extending or refactoring code without breaking things. For example, say we only had a complex type complex-r and implemented the methods real, imag, magnitude, and angle as functions via defn. But then later we required the complex-p type and it also needed the same methods. Refactoring the existing functions (real, imag, etc) into a protocol will make no difference to code already using these functions with the complex-r type - it looks and acts just like a function. Multimethods are the same. . Type hierarchies can be decoupled from types. In this example we built a hierarchy for the number types using namespaced keywords like ::complex-r. This exists independently from the records we defined. It is coupled to the record types via a one-to-one mapping of the records to a keyword, implemented by the protocol PNumberKind. This decoupling allowed us to create further abstractions such as ::number and ::complex. In Java you’d have to create abstract base classes retroactively and subclass the existing types, which would be a redesign. This decoupling was also useful later for the numeric tower where we actually required a completely different hierarchy of the types. This was made possible in Clojure because you can create multiple hierarchies ad-hoc - hierarchies are just data. You could even implement multimethods with different type hierarchies this way. In OOP this kind of polymorphic dispatch is strongly coupled to your class hierarchies, which you can’t just change. . Source Code . The complete source code of this tutorial can be found here: jimypbr/clojure-numbers. . References and Further Reading . SICP section 2.4 | Watch ‘Simple Made Easy’ by Rich Hickey. It’s a great talk that explains how Clojure aims to decouple key programming concepts from each other: Simple Made Easy, Rich Hickey | For more about multiple dispatch, Eli Bendersky’s series of blog posts are great: http://eli.thegreenplace.net/2016/a-polyglots-guide-to-multiple-dispatch | This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/23/polyclojure3.html",
            "relUrl": "/clojure/functional-programming/2016/11/23/polyclojure3.html",
            "date": " • Nov 23, 2016"
        }
        
    
  
    
        ,"post18": {
            "title": "How to do Polymorphism in Clojure (2/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . More Number Types . Let’s extend our number package with more number types: An Integer type, a Float type for real numbers, and a Rational type for fractions. Like before we create the records and constructors: . (defrecord Int [n]) (defrecord Rational [n d]) (defrecord Float [n]) ;; constructors (defn integer &quot;Make a new integer&quot; [n] {:pre [(number? n)]} (-&gt;Int (int n))) (defn float &quot;Make a new float&quot; [n] {:pre [(number? n)]} (-&gt;Float (double n))) (defn rational &quot;Make a new rational&quot; [n d] {:pre [(number? n) (number? d)]} (let [g (gcd n d)] (-&gt;Rational (/ n g) (/ d g)))) . Trying these out in the REPL: . =&gt; (float 3) #numbers.core.Float{:n 3.0} =&gt; (integer 6) #numbers.core.Int{:n 4} =&gt; (rational 6 3) #numbers.core.Rational{:n 2, :d 1} . Generic Arithmetic . We want to create an add function that can be called with either two integers, two rationals, two reals, or two complex types and do the right thing in every case. Protocols in Clojure allow for single dispatch only. Here we need to dispatch on the types of multiple arguments. . We could hack this with a mega-function that is just a big conditional statement: . (def mega-add &quot;one add to rule them all (don&#39;t do this)&quot; [n1 n2] (cond (and (= (type n1) Int) (= (type n2) Int)) (add-int n1 n2) (and (= (type n1) Float) (= (type n2) Float) (add-real n1 n2) ;; etc :else (throw &quot;unknown types&quot;))) . The problem with this solution is that it is closed for business. If a user of the our number library desired to extend the number system with a new type of number (e.g. a BigInt), they’d have to break in and edit this function directly. . Multi-Methods . Clojure’s core library provides multiple dispatch via multi-methods. While protocols in Clojure perform single-dispatch on just the type of the first argument, multi-methods are much more general and allow the programmer to define their own rules for dispatch using any number of arguments. You are not limited to dispatch with just the types of the arguments, but also their values. . Let’s throw out mega-add and do it properly with multi-methods. The multi-method is defined using the defmulti macro. It takes a docstring and a dispatch function as its arguments. For adding, the dispatch function will be mapped to the two numbers as arguments and so return a vector of the types of the arguments: . (defmulti add &quot;Generic add&quot; class) . So if we provided two Ints then the dispatch would return [Int Int]. With the dispatch machinery is in place, we now need to add the implementations for each of the types. This is done with defmethod, which defines a method for each valid output of the dispatch function: . (defmethod add [Int Int] &quot;Add two integers together&quot; [n1 n2] (integer (+ (:n n1) (:n n2)))) (defmethod add [Float Float] &quot;Add two floats together&quot; [n1 n2] (float (+ (:n n1) (:n n2)))) ;; etc . Trying this out in the repl: . =&gt; (add (integer 4) (integer 6)) #numbers.core.Int{:n 10} =&gt; (add (float 5) (float 10)) #numbers.core.Float{:n 15.0} . Neat! Multi-methods are easy to extend too. If I wanted to create a new number type (e.g. a BigInt), then all I need to do is add a new method with defmethod for the case of [BigInt BigInt]. . Similarly we can reimplement the add function defined previously for the two complex number types, using the new multi-method machinery: . (defmethod add [Complex-r Complex-r] &quot;Add two complex-r numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) (defmethod add [Complex-p Complex-p] &quot;Add two complex-p numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) . This works fine, but previously add for the two different complex number types was a single function, whereas now we have repetition. Moreover we can’t add a Complex-r to a Complex-p like we could before. . Multimethods have provided a lot of extensibility to new number types, but at the same time we have lost the polymorphic nature we had in the arithmetic functions of the two complex types. I will address this problem in the next section. . Keywords and Dispatch Hierarchies . We have an impression that Complex-r and Complex-p are subtypes of some imaginary abstract type Complex. However Clojure does not offer any notion of an ‘abstract type’ as we are used to in OOP. Instead Clojure provides an ad-hoc dynamic hierarchy system. The hierarchy system supports derivation relationships between names (either symbols or keywords), and relationships between classes and names. . The derive function creates these relationships, and the isa? function tests for their existence. We will use namespaced keywords (double colon) to represent the number types: . =&gt; (derive ::complex-r ::complex) =&gt; (derive ::complex-p ::complex) =&gt; (isa? ::complex-r ::complex) true =&gt; (isa? ::complex-p ::complex) true =&gt; (isa? ::complex-r ::complex-p) false =&gt; (ancestors ::complex-r) #{::complex} =&gt; (ancestors ::complex-p) #{::complex} . What we want to do is rewrite the arithmetic multi-methods to dispatch using these namespaced keywords in place of the number types. The complex add method could then be reduced to matching arguments that satisfy: [::complex ::complex]. To do this, we will require a one-to-one mapping of each type to its associated keyword: . Complex-r =&gt; ::complex-r | Complex-p =&gt; ::complex-p | Float =&gt; ::float | Rational =&gt; ::rational | Int =&gt; ::integer | . We could do this with a global lookup table or add the keywords to the record definitions, but these are cludgy solutions. The first requires maintaining some global data, and the second repeats information and forces us to rewrite the record definition, which would break existing code. A cleaner solution is just to create another protocol and extend our number types with it: . (defprotocol PNumberKind (kind [n] &quot;The keyword name for the kind of the number n&quot;)) (extend-protocol PNumberKind Complex-r (kind [z] ::complex-r) Complex-p (kind [z] ::complex-p) Float (kind [z] ::float) Rational (kind [z] ::rational) Int (kind [z] ::integer)) . In the REPL: . =&gt; (kind (integer 3)) :numbers.core/integer =&gt; (kind (complex-r 4 5)) :numbers.core/complex-r . We can now update the dispatch function used by the multimethod to dispatch using kind: . (defmulti add &quot;Generic add&quot; kind) . The methods can now be rewritten as: . (defmethod add [::integer ::integer] &quot;Add two integers together&quot; [n1 n2] (integer (+ (:n n1) (:n n2)))) (defmethod add [::float ::float] &quot;Add two reals together&quot; [n1 n2] (real (+ (:n n1) (:n n2)))) (defmethod add [::complex ::complex] &quot;Add two complex-p numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) . Since we added the rule (derive ::complex-r ::complex) to the hierarchy, the multimethod called with ::complex-r or ::complex-p implicitly satisfy the rule [::complex ::complex]. The hierarchy has therefore allowed us the implement a polymorphic add for adding different representations of complex numbers and their combinations. If we added more complex number representations, the generic add method for complex numbers would accomodate them automatically without modification. . Let’s try this in the REPL: . =&gt; (add (complex-r 2 3) (complex-r 6 7)) #numbers.core.Complex-r{:real 8, :imag 10} =&gt; (add (complex-p 4 5) (complex-p 1 0)) #numbers.core.Complex-r{:real 2.1346487418529048, :imag -3.835697098652554} =&gt; (add (complex-p 3 4) (complex-r 5 6)) #numbers.core.Complex-r{:real 3.039069137409164, :imag 3.7295925140762156} . This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/22/polyclojure2.html",
            "relUrl": "/clojure/functional-programming/2016/11/22/polyclojure2.html",
            "date": " • Nov 22, 2016"
        }
        
    
  
    
        ,"post19": {
            "title": "How to do Polymorphism in Clojure (1/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . Introduction . Through the years of use of conventional programming languages such as Python and C++, my thinking about programming was trapped within the Object Oriented model of these languages. When it came to solving a new problem my thinking would be dictated by how those languages wanted me to think and I was blind to any better way. It wasn’t until I came across the 1980s cult-classic “Structure and Interpretation of Computer Programs” (SICP) and the programming language Clojure that I started to see past thought models I had learned. One of the parts that was a real ‘Ah-ha!’ moment for me was the final section of Chapter 2 on “Data-directed programming”. This chapter was the clearest explanation of the problem of Polymorphism that I’d ever read… . The concern of this section is in implementing a system for calculating with different types of numbers (e.g. complex, integers, rationals, etc). Variants of this example are commonly used beginners books to Java and C++ in order to demonstrated how Object Oriented programming (OOP) can model data and abstractions. However SICP is not a language manual and doesn’t concern itself with showing how to make some feature of a language do what you want. It instead focuses on the problem of trying to model numbers itself, and then creates the necessary structures to solve this problem. SICP uses this problem to motivate the actual purpose of OOP and it then creates a basic implementation of Objects in scheme. The book then goes beyond OOP and implements multiple dispatch from scratch entirely in Scheme. . I found this to be such an enlightening exercise that I started thinking about how well other languages could solve this problem. In this blog post I want to implement the Numbers program in Clojure, a language that provides multiple dispatch. I think this is a non-trivial example that demonstrates every feature for polymorphism in Clojure’s core libraries. . Complex Numbers . A complex number, $z$ is a number expressed in the form $z=u+iv$, where $u$ and $v$ are real numbers and $i^2=-1$. $u$ is called the real part and $v$ is called the imaginary part. . We can represent a complex number as a pair $(u, v)$, called the rectangular form. . | An alternative representation is the polar form where the complex number is represented by the pair $(r, phi)$, where $r$ is the magntitude and $ phi$ is the angle. . | . Rectangular and polar forms are related via the following formulae: . [ begin{eqnarray} u &amp;=&amp; r cos phi v &amp;=&amp; r sin phi r &amp;=&amp; sqrt{u^2 + v^2} phi &amp;=&amp; tan^{-1}(v/u) end{eqnarray}] . In our Complex number package we want to support the following arithmetic operations on pairs of complex numbers: add, sub, mult, and div. . When adding or subtracting complex numbers it is natural to work with their rectangular coordinates: . [ begin{eqnarray} Re(z_1 + z_2) &amp;=&amp; Re(z_1) + Re(z_2) Im(z_1 + z_2) &amp;=&amp; Im(z_1) + Im(z_2) end{eqnarray}] . While when multiplying and dividing complex numbers it is more natural to work with the polar coordinates: . [ begin{eqnarray} Magnitude(z_1 cdot z_2) &amp;=&amp; Magnitude(z_1) cdot Magnitude(z_2) Angle(z_1 cdot z_2) &amp;=&amp; Angle(z_1) + Angle(z_2) end{eqnarray}] . The product is the vector obtained by stretching the length of $z_1$ by the length of $z_2$, and rotating the angle of $z_1$ by the angle of $z_2$. . So there are two different representations which are suitable for different operations. However we want to be able to do all the arithmetic operations on complex numbers regardless of which representation is used. . Rectangular Representation . How can we model this number pair using the tools in Clojure? Clojure allows us to create an object called a Record. A Record is basically a map with a name, a minimum set of keys that it is guaranteed to have, and a constructor. Here’s how we could create a rectangular complex number with records: . (defrecord Complex-r [real imag]) . You can create some Complex-r’s: . =&gt; (-&gt;Complex-r 2 3) #user.Complex-r{:real 2, :imag 3} =&gt; (-&gt;Complex-r -1 16) #user.Complex-r{:real -1, :imag 16} . With records it is good practice to create your own constructor to give you freedom to add post- and pre-conditions when a new record is created. This is just a wrapper function: . (defn complex-r &quot;create a new Complex-r&quot; [re im] {:pre [(number? re) (number? im)]} (-&gt;Complex-r re im)) . I provided some preconditions that assert that the parameters are a type of Clojure’s native number. We can access the real and imaginary parts of a Complex-r: . =&gt; (:real (complex-r 4 5)) 4 =&gt; (:imag (complex-r 4 5)) 5 . However, getting the real and imaginary parts of a complex number using the keywords seems to expose the implementation of Complex-r too much. Better practice would be to wrap those in some functions: . (defn real-r &quot;Get the real part of a complex-r number.&quot; [z] (:real z)) (defn imag-r &quot;Get the imaginary part of a complex-r number.&quot; [z] (:imag z)) . So now we have: . =&gt; (real-r (complex-r 4 5)) 4 =&gt; (imag-r (complex-r 4 5)) 5 . We can also view the magnitude and angle of a rectangular complex number using the formulae above: . (defn magnitude-r &quot;Magnitude of a complex-r number&quot; [z] (Math/sqrt (+ (square (real-r z)) (square (imag-r z))))) (defn angle-r &quot;Angle of a complex-r number&quot; [z] (Math/atan (/ (imag-r z) (real-r z)))) . In the REPL: . =&gt; (magnitude-r (complex-r 3 4)) 5.0 =&gt; (angle-r (complex-r 3 4)) 0.9272952180016121 . Polar Representation . Similarly, we can implement the Polar form of complex numbers as another record: . (defrecord Complex-p [magnitude angle]) (defn complex-p &quot;create a new Complex-p&quot; [magnitude angle] {:pre [(number? magnitude) (number? angle)]} (-&gt;Complex-p magnitude angle)) . Again for the Polar representation we need to write some functions that will give us real and imaginary parts, and the magnitude and angle of a polar number. . (defn real-p &quot;real part of a complex-p number&quot; [z] (* (:magnitude z) (Math/sin (:angle z)))) (defn imag-p &quot;imaginary part of a complex-p number&quot; [z] (* (:magnitude z) (Math/cos (:angle z)))) (defn magnitude-p &quot;magnitude of a complex-p number&quot; [z] (:magnitude z)) (defn angle-p &quot;angle of a complex-p number&quot; [z] (:angle z)) . In the REPL: . =&gt; (angle-p (complex-p 5 0.5)) 0.5 =&gt; (magnitude-p (complex-p 5 0.5)) 5 =&gt; (real-p (complex-p 5 0.5)) 4.387912809451864 =&gt; (imag-p (complex-p 5 0.5)) 2.397127693021015 . Single Dispatch with Protocols . At this point we have two different types of complex number representations and two sets of functions that are specialised to handle each type. This is obviously bad because a user of this numbers module has to pay attention at all times to whether they are using Complex-r or Complex-p types. They need to specialise whatever code they write with them. . Rather than having real-r and real-p functions we instead want to have a single function real that takes any type of complex number and performs dispatch at runtime based on the type of the argument it has received. I.e. dispatch based on the type of the first argument passed to the function. Dispatch based on a single argument is called single dispatch. . Clojure allows single dispatch through Protocols. A protocol is a named set of functions and their signatures, with no implementions. The functions dispatch on the type of their first argument, and thus must have at least one argument. Implementations of the protocol functions have to be written for each type implementing the protocol. They are very similar to Java interfaces, but with one important improvement: which protocols are implemented for a type is not a design time decesion by the code author, rather protocols can extend any type whenever and wherever you want. . We create a protocol for complex numbers using defprotocol: . (defprotocol PComplex (real [z] &quot;Real part of a complex number&quot;) (imag [z] &quot;Imaginary part of a complex number&quot;) (magnitude [z] &quot;Magnitude of a complex number&quot;) (angle [z] &quot;Angle of a complex number&quot;)) . Implement the PComplex protocol for each of our types: . (extend-protocol PComplex ;; implementation of methods for Complex-r type Complex-r (real [z] (:real z)) (imag [z] (:imag z)) (magnitude [z] (Math/sqrt (+ (square (real z)) (square (imag z)))) (angle [z] (Math/atan (/ (imag z) (real z)))) ;; implemention of methods for Complex-p type Complex-p (real [z] (* (:magnitude z) (Math/sin (:angle z)))) (imag [z] (* (:magnitude z) (Math/cos (:angle z)))) (magnitude [z] (:magnitude z)) (angle [z] (:angle z))) . Trying this out in a REPL: . =&gt; (def z1 (complex-r 5 6)) =&gt; (def z2 (complex-p 3 1)) =&gt; (real z1) 5 =&gt; (real z2) 1.6209069176044193 =&gt; (magnitude z1) 7.810249675906654 =&gt; (magnitude z2) 5 . With protocol we now have a generic set of functions for dealing with any type of complex number. If we created a new type of complex number then we’d simple make it implement the PComplex protocol. . Arithmetic With Complex Numbers . The PComplex protocol allows us to write code that works with complex numbers and does not need to worry whether whether they are rectangular or polar. We can now write single implementations the arithmetic functions add, sub, mult, and div using the formulas above. . (defn add &quot;Add two complex numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) (defn sub &quot;Subtract two complex numbers from each other&quot; [z1 z2] (complex-r (- (real z1) (real z2)) (- (imag z1) (imag z2)))) (defn mult &quot;Multiply two complex numbers together&quot; [z1 z2] (complex-p (* (magnitude z1) (magnitude z2)) (+ (angle z1) (angle z2)))) (defn div &quot;Divide two complex numbers by each other&quot; [z1 z2] (complex-p (/ (magnitude z1) (magnitude z2)) (- (angle z1) (angle z2)))) . Since the functions from PComplex work transparently for both representations of complex numbers we only need to write one function for add and it works not only for both types, but also all combinations of them for free! . Let’s try it all out in the REPL: . =&gt; (add (complex-r 5 5) (complex-r 6 6)) #numbers.core.Complex-r{:real 11, :imag 11} =&gt; (mul (complex-r 5 5) (complex-r 5 5)) #numbers.core.Complex-p{:magnitude 50.00000000000001, :angle 1.5707963267948966} =&gt; (add (complex-p 5 0.1) (complex-p 10 0)) #numbers.core.Complex-r{:real 14.975020826390129, :imag 0.4991670832341408} =&gt; (mul (complex-p 5 0.1) (complex-p 10 0)) #numbers.core.Complex-p{:magnitude 50, :angle 0.1} =&gt; (add (complex-r 5 5) (complex-p 10 0)) #numbers.core.Complex-r{:real 15.0, :imag 5.0} =&gt; (mul (complex-r 5 5) (complex-p 10 0)) #numbers.core.Complex-p{:magnitude 70.71067811865476, :angle 0.7853981633974483} . Without the polymorphism obtained from the protocol we would have to write 16 separate functions instead of just these 4. Moreover, if we wanted to create more complex number representations there would be a combinatorial explosion in the number of arithmetic functions we’d need to write. . This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/21/polyclojure1.html",
            "relUrl": "/clojure/functional-programming/2016/11/21/polyclojure1.html",
            "date": " • Nov 21, 2016"
        }
        
    
  
    
        ,"post20": {
            "title": "Experiments with Julia",
            "content": "SIMD Support . Since version 0.3 Julia has some vectorisation capabilities that can exploit SIMD instructions when executing loops. It seems to require some nudging though. There are macros that are a bit like the pragmas in OpenMP. . Example 1. SAXPY . function saxpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) for i = 1:n y[i] += a*x[i] end end . Sadly, in version 0.3.10 this obvious candidate does not auto-vectorise. You can inspect how this code has compiled nicely in Julia by using the macros @code_llvm to see the LLVM IR or @code_native to see the ASM. The ASM produced is: . Source line: 48 mov R8, QWORD PTR [RSI + 16] xor R11D, R11D xor ECX, ECX cmp RCX, R8 jae 65 cmp RCX, QWORD PTR [RDI + 16] jae 55 lea R10, QWORD PTR [4*R11] mov RAX, QWORD PTR [RSI + 8] sub RAX, R10 mov RDX, QWORD PTR [RDI + 8] sub RDX, R10 movss XMM1, DWORD PTR [RDX] mulss XMM1, XMM0 addss XMM1, DWORD PTR [RAX] movss DWORD PTR [RAX], XMM1 dec R11 inc RCX cmp R9, RCX jne -72 pop RBP ret movabs RAX, 140636405232096 mov RDI, QWORD PTR [RAX] movabs RAX, 140636390845696 mov ESI, 48 call RAX . Note the scalar instructions movss, mulss, and addss. . Example 2: SAXPY + SIMD . To make it generate vectorised instructions you have to use the explicit vectorisation macros @simd and @inbounds macros. @simd gives the compiler license to vectorise without checking the legality of the transformation. @inbounds is an optimisation that turns off subscript checking, because subscript checking might throw an exception and so isn’t vectorisable. . function axpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) @simd for i = 1:n @inbounds y[i] += a*x[i] end end . This now compiles with SIMD instructions: . ... Source line: 48 mov R8, QWORD PTR [RDI + 8] mov R9, QWORD PTR [RSI + 8] xor EDI, EDI mov RSI, RAX and RSI, -8 je 79 pshufd XMM1, XMM0, 0 # xmm1 = xmm0[0,0,0,0] xor EDI, EDI lea RCX, QWORD PTR [4*RDI] mov RDX, R8 sub RDX, RCX movups XMM3, XMMWORD PTR [RDX] movups XMM2, XMMWORD PTR [RDX + 16] mulps XMM3, XMM1 mov RDX, R9 sub RDX, RCX movups XMM5, XMMWORD PTR [RDX] movups XMM4, XMMWORD PTR [RDX + 16] addps XMM5, XMM3 movups XMMWORD PTR [RDX], XMM5 mulps XMM2, XMM1 addps XMM2, XMM4 movups XMMWORD PTR [RDX + 16], XMM2 add RDI, -8 mov RCX, RSI add RCX, RDI jne -69 mov RDI, RSI sub RAX, RDI je 41 ... . Note the instructions movups, addps, mulps… (English: move, unaligned, packed, single-precision). Packed =&gt; Vector. . We can also see from the LLVM IR: . define void @julia_axpy_21664(float, %jl_value_t*, %jl_value_t*) { ... br label %vector.body vector.body: ; preds = %vector.body, %vector.ph %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ] %25 = getelementptr float* %20, i64 %index, !dbg !4955 %26 = bitcast float* %25 to &lt;4 x float&gt;* %wide.load = load &lt;4 x float&gt;* %26, align 4 %.sum18 = or i64 %index, 4 %27 = getelementptr float* %20, i64 %.sum18 %28 = bitcast float* %27 to &lt;4 x float&gt;* %wide.load9 = load &lt;4 x float&gt;* %28, align 4 %29 = getelementptr float* %24, i64 %index, !dbg !4955 %30 = bitcast float* %29 to &lt;4 x float&gt;* %wide.load10 = load &lt;4 x float&gt;* %30, align 4 %31 = getelementptr float* %24, i64 %.sum18 %32 = bitcast float* %31 to &lt;4 x float&gt;* %wide.load11 = load &lt;4 x float&gt;* %32, align 4 %33 = fmul &lt;4 x float&gt; %wide.load10, %broadcast.splat13 %34 = fmul &lt;4 x float&gt; %wide.load11, %broadcast.splat13 %35 = fadd &lt;4 x float&gt; %wide.load, %33 %36 = fadd &lt;4 x float&gt; %wide.load9, %34 store &lt;4 x float&gt; %35, &lt;4 x float&gt;* %26, align 4 store &lt;4 x float&gt; %36, &lt;4 x float&gt;* %28, align 4 %index.next = add i64 %index, 8 %37 = icmp eq i64 %index.next, %n.vec br i1 %37, label %middle.block, label %vector.body middle.block: ; preds = %vector.body, %if %resume.val = phi i64 [ 0, %if ], [ %n.vec, %vector.body ] %cmp.n = icmp eq i64 %15, %resume.val br i1 %cmp.n, label %L7, label %L ... . Timing of a function is easy to do: . x = rand(Float32, 10000000) y = rand(Float32, 10000000) a = float32(0.1) @time axpy(a,x,y) . However, you probably want to run this more than once since the first time you call a function, Julia JITs it so the timing won’t be representative. Here are the timings of axpy with and without @simd with length(x) == 10000000: . @simd? Time (s) . yes | 0.006527373 | . no | 0.013172804 | . Setup: Hardware: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz (AVX) Julia version: 0.3.11 . Example 3: SAXPY + Implicit vectorisation . In my experiments, I found that it is sometimes possible to get implicit vectorisation by using just @inbounds to disable bounds checking: . function axpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) for i = 1:n @inbounds y[i] += a*x[i] end end . This generates the same ASM as Example 2. Other simple cases have required @simd as well, so explicit vectorisation seems to be the only option at the moment. . SIMD Limitations in Julia . No SIMD functions. Function calls have to be inlined. Julia can manage to inline short functions itself. | Code must be type-stable. This means that there isn’t implicit type conversion in the loop. This will prevent vectorisation and probably make it run slow serially too. | SIMD macro doesn’t have the bells and whistles of the OpenMP SIMD pragma. | Doesn’t appear to be any way to specify alignment of memory. | No outer loop vectorisation. | I can’t find any diagnostic information about how things were optimised. | Does handle any branching besides some ifelse function. | . More Info on Vectorisation . See Intel page Vectorization in Julia. . Case Study: Fitzhugh-Nagamo PDE . Experiment with a non-trivial example of a PDE solver using the reaction-diffusion system described by: . [ begin{eqnarray} frac{ partial u}{ partial t} &amp;=&amp; a nabla^2 u + u - u^3 - v - k tau frac{ partial v}{ partial t} &amp;=&amp; b nabla^2 v + u - v end{eqnarray}] . With Neumann boundary conditions, boundary of [1,1]^2, N=100^2, and T=[0.,10.]. This example is based on this online python example. I implement this in C++ and Julia and compare the performance and the vectorisation. $y=x^2$. . Here is the code in Julia: . Click to expand code… function fitzhugh_nagumo(size, T) # Define the constants const aa = 2.8e-4 const bb = 5.0e-3 const τ = 0.1 const κ = -0.005 const dx = 2.0 / size const dt = 0.9 * dx^2 / 2.0 const invdx2 = 1.0 / dx^2 const dt_τ = dt / τ # Random initial fields u_old = rand(Float64, (size,size)) v_old = rand(Float64, (size,size)) u_new = Array(Float64, (size,size)) v_new = Array(Float64, (size,size)) for t = 0.0 : dt : T for j = 2:size-1 for i = 2:size-1 Δu = invdx2 * (u_old[i+1,j] + u_old[i-1,j] + u_old[i,j+1] + u_old[i,j-1] - 4*u_old[i,j]) Δv = invdx2 * (v_old[i+1,j] + v_old[i-1,j] + v_old[i,j+1] + v_old[i,j-1] - 4*v_old[i,j]) u_new[i,j] = u_old[i,j] + dt * (aa*Δu + u_old[i,j] - u_old[i,j]*u_old[i,j]*u_old[i,j] - v_old[i,j] + κ) v_new[i,j] = v_old[i,j] + dt_τ * (bb*Δv + u_old[i,j] - v_old[i,j]) end end for i = 1:size u_new[i,1] = u_new[i,2] u_new[i,size] = u_new[i,size-1] v_new[i,1] = v_new[i,2] v_new[i,size] = v_new[i,size-1] end for j = 1:size u_new[1,j] = u_new[2,j] u_new[size,j] = u_new[size-1,j] v_new[1,j] = v_new[2,j] v_new[size,j] = v_new[size-1,j] end # swap new and old u_new, u_old = u_old, u_new v_new, v_old = v_old, v_new end return (u_old, v_old) end . Here is the code in C++. . Click to expand code… #include &lt;iostream&gt; #include &lt;random&gt; #include &lt;algorithm&gt; // swap #define N 100 std::random_device rd; std::mt19937 mt(rd()); double fitzhugh_nagumo(double T) { // Define the constants const double aa = 2.8e-4; const double bb = 5.0e-3; const double tau = 0.1; const double kappa = -0.005; const double dx = 2.0 / N; const double dt = 0.9 * dx*dx / 2.0; const double invdx2 = 1.0 / (dx*dx); const double dt_tau = dt / tau; // Random initial fields double u_old[N][N]; double v_old[N][N]; double u_new[N][N]; double v_new[N][N]; std::uniform_real_distribution&lt;double&gt; rng(0.,1.); for (int i = 0; i &lt; N; ++i) { for (int j = 0; j &lt; N; ++j) { u_old[i][j] = rng(mt); v_old[i][j] = rng(mt); } } // Solver int Nt = (int) (T / dt); std::cout &lt;&lt; &quot;Nt = &quot; &lt;&lt; Nt &lt;&lt; std::endl; for (int t = 0; t &lt; Nt; ++t) { // evolve inner coordinates for (int i = 1; i &lt; N-1; ++i) { for (int j = 1; j &lt; N-1; ++j) { double delta_u = invdx2 * (u_old[i+1][j] + u_old[i-1][j] + u_old[i][j+1] + u_old[i][j-1] - 4*u_old[i][j]); double delta_v = invdx2 * (v_old[i+1][j] + v_old[i-1][j] + v_old[i][j+1] + v_old[i][j-1] - 4*v_old[i][j]); u_new[i][j] = u_old[i][j] + dt * (aa*delta_u + u_old[i][j] - u_old[i][j]*u_old[i][j]*u_old[i][j] - v_old[i][j] + kappa); v_new[i]After i[j] = v_old[i][j] + dt_tau * (bb * delta_v + u_old[i][j] - v_old[i][j]); } } // neumann boundary conditions for (int i = 0; i &lt; N; ++i) { u_new[i][0] = u_new[i][1]; u_new[i][N-1] = u_new[i][N-2]; v_new[i][0] = v_new[i][1]; v_new[i][N-1] = v_new[i][N-2]; } for (int j = 0; j &lt; N; ++j) { u_new[0][j] = u_new[1][j]; u_new[N-1][j] = u_new[N-2][j]; v_new[0][j] = v_new[1][j]; v_new[N-1][j] = v_new[N-2][j]; } // Swap old and new std::swap(u_new, u_old); std::swap(v_new, v_old); } return u_old[0][0]; } . Results . Setup: . Hardware: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz (AVX) | Julia version: 0.3.10 | C++ Compiler: g++4.8 | . Language Notes Time (s) . Julia | None | 5.993 | . Julia | @inbounds | 3.105 | . Julia | @inbounds + @simd | 3.0603 | . C++ | -O0 -std=c++11 | 22.790 | . C++ | -Ofast -std=c++11 -fno-tree-vectorize -fno-tree-slp-vectorize | 3.2096 | . C++ | -Ofast -std=c++11 | 2.142 | . The best time in C++ is only 1.4x better than the best time in Julia. Inspecting the ASM of Julia + @inbounds + @simd shows that even with these macros Julia is still not generating vector instructions. :(. If I disable vectorisation in the C++ compiler, the times between Julia and C++ are much closer. This suggests that Julia could get even higher performance if it could generate vector instructions. I suppose that newer versions of Julia will improve this in the future. I find these results very impressive nonetheless. It will be worth trying with a newer LLVM version too. . Notes . The thing holding back performance in the Julia code was the use of u[i,j]^3 instead of u[i,j]*u[i,j]*u[i,j]. The former version compiles to a call of pow(double, double) from libm! The latter does what you expect. This is a known bug. Fixed when Julia is built against LLVM 3.6. Version I’m using is built with LLVM v3.4. .",
            "url": "https://jimypbr.github.io/blog/julia/c++/simd/2015/11/21/julia.html",
            "relUrl": "/julia/c++/simd/2015/11/21/julia.html",
            "date": " • Nov 21, 2015"
        }
        
    
  
    
        ,"post21": {
            "title": "Hello",
            "content": "Typical hello world first blog post… .",
            "url": "https://jimypbr.github.io/blog/2015/11/20/hello.html",
            "relUrl": "/2015/11/20/hello.html",
            "date": " • Nov 20, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jimypbr.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jimypbr.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}