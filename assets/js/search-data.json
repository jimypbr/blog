{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jimypbr.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jimypbr.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "SGD From Scratch in PyTorch",
            "content": "Overview . In this post I explore Stochastic Gradient Descent (SGD) which is an optimization method commonly used in neural networks. This continues Lesson 2 of fast.ai on Stochastic Gradient Descent (SGD). I will copy from the fast.ai notebook on SGD and dig deeper into the what’s going on there. . Linear Regression . We will start with the simplest model - the Linear model. Mathematically this is represented as: . [ vec{y} = X vec{a} + vec{b}] . Where $X$ is a matrix where each of the rows is a data point, $ vec{a}$ is the vector of model weights, and $ vec{b}$ is a bias vector. In the 1D case, these would correspond to the familiar ‘slope’ and ‘intercept’ of a line. We can make this more compact by combining the bias inside of the model weights and adding an extra column to $X$ with all values set to one. These are represented in Pytorch as tensors. . In Pytorch, a tensor is a data structure that encompasses arrays of any dimension. A vector is a tensor of rank 1, while a matrix is a tensor of rank 2. For simplicity we will stick to the case of a 1D linear model. In PyTorch $X$ would then be: . n=100 x = torch.ones(n,2) x[:,0].uniform_(-1.,1) . The model has two parameters and there are n=100 datapoints. x therefore has shape (100, 2). The .uniform_(-1., 1) generates floating point numbers between -1 and 1. The trailing _ is PyTorch convention that the function operates inplace. . Let’s look at the first 5 values of x: . &gt; x[:5] tensor([[ 0.7893, 1.0000], [-0.7556, 1.0000], [-0.0055, 1.0000], [-0.2465, 1.0000], [ 0.0080, 1.0000]]) . Notice how the second column is all 1s - this is the bias. . We’ll now set the true values for the model weights, $a$, to slope=3 and intersection=10: . a = tensor(3.,10) a_true = a . With x and a set we can now generate some fake data with some small normally distributed random noise: . y = x@a + torch.randn(n) * 0.6 . . Loss Function . We want to find parameters (weights) a such that they minimize the error between the points and the line x@a. Note that here a is unknown. For a regression problem the most common error function or loss function is the mean squared error. In python this function is: . def mse(y_hat, y): return ((y_hat-y)**2).mean() . Where y is the true value and y_hat is the predicted value. . We start with guess at the value of the weights a: . a = tensor(-1, 1) . We can make prediction for y, y_hat, and compute the error against the known values: . &gt; y_hat = x@a &gt; mse(y_hat, y) tensor(92.9139) . So far we have specified the model (linear regression) and the evaluation criteria (or loss function). Now we need to handle optimization; that is, how do we find the best values for a? How do we find the best fitting linear regression. . Gradient Descent . We would like to find the values of a that minimize mse_loss. Gradient descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient. Here is gradient descent implemented in PyTorch: . a = nn.Parameter(a) lr = 1e-1 def update(): y_hat = x@a loss = mse(y, y_hat) loss.backward() with torch.no_grad(): # don&#39;t compute the gradient in here a.sub_(lr * a.grad) a.grad.zero_() for t in range(100): update() . We are going to create a loop. We’re going to loop through 100 times, and we’re going to call a function called update. That function is going to: . Calculate y_hat (i.e. our prediction) | Calculate loss (i.e. our mean squared error) | Calculate the gradient. In PyTorch, calculating the gradient is done by using a method called backward. Mean squared error was just a simple standard mathematical function. PyTorch keeps track of how it was calculated and lets us automatically calculate the derivative. So if you do a mathematical operation on a tensor in PyTorch, you can call backward to calculate the derivative and the derivative gets stuck inside an attribute called .grad. | Then take the weights a and subtract the gradient from them (sub_). There is an underscore there because that’s going to do it in-place. It’s going to actually update those coefficients a to subtract the gradients from them. Why do we subtract? Because the gradient tells us if the whole thing moves downwards, the loss goes up. If the whole thing moves upwards, the loss goes down. So we want to do the opposite of the thing that makes it go up. We want our loss to be small. That’s why we subtract. | lr is our learning rate. All it is is the thing that we multiply by the gradient. | . Animate it! . Here is an animation of the training gradient descent with learning rate LR=0.1 . . Notice how it seems to spring up to find the intercept first then adjusts to get the slope right. The starting guess at the intercept is 1, while the real value is 10. At the start this would cause the biggest loss so the we would expect the gradient on the intercept parameter to be higher than the gradient on the slope parameter. . It sucessfully recovers, more or less, the weights that we generated the data with: . &gt; a tensor([3.0332, 9.9738] . Stochastic Gradient Descent . The gradient descent algorithm calculates the loss across the entire dataset every iteration. For this problem this works great, but it won’t scale. If we were training on imagenet then we’d have to compute the loss on 1.5 million images just to do a single update of the parameters. This would be both incredibly slow and also impossible to fit into computer memory. Instead we grab random mini-batches of 64, or so, data points and compute the loss and gradient with those and then update the weights. As code this looks almost identical to before, but with some random indexes added to x and y: . def update_mini(rand_idx): y_hat = x[rand_idx]@a loss = mse(y[rand_idx], y_hat) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . Using mini-batches approximates the gradient, but also adds random noise to the optimiser causing the parameters to ‘jump around’ a little. This can make it require more iterations to converge. We will see this visually in the next section. On the other hand, some random noise is a good thing in training neural networks because it allows the optimiser to better explore the high dimensional parameter space and potentially find a solution with a lower loss. . Animate it! . Here is an animation of the training with batch size of 16: . . It converges on the same answer as gradient descent, but it is a little slower and has a bit of jitter that isn’t in the gradient descent animation. . Experiments with the Learning Rate and Batch Size . We can gain a better understanding of how SGD works by playing with the parameters, learning rate and batch size, and visualising the learning process. . Learning Rate . Here the learning rate in SGD is varied, keeping the batch size fixed at 16. . Parameters Animation . SGD LR=1e-2bs=16 | | . SGD LR=1e-1bs=16 | | . SGD LR=1.0 bs=16 | | . With the learning rate of 0.01 it too small and it takes an age, but it does eventually converge on the right answer. With a learning rate of 1.0 the whole thing goes off the rails and it can’t get anywhere near the right answer. . Batch Size . Here the batch size in SGD is varied, holding the learning rate fixed at LR=0.1: . Parameters Animation . SGD bs=1 | | . SGD bs=2 | | . SGD bs=4 | | . SGD bs=8 | | . SGD bs=16 | | . SGD bs=32 | | . SGD bs=64 | | . All of the instances do converge to the right answer in this case (though in general that wouldn’t be the case for all problems). For bs=1 it jumps around a lot even after it gets into the right place. This is because the weights are updated using only one data point every iteration. So it jitters around the right solution and will never stop jittering with more iterations. . However with increasing batch size the jitter gets less and less. At batch size of 64 the animation is almost identical to the gradient descent animation. This makes sense since n=100 so with bs=64 we have almost gone back to the full gradient descent algorithm (which would be bs=n ). . References . Link to Lesson 2 lecture | SGD From Scratch Notebook | Lesson2 Detailed Notes by @hiromi. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html",
            "date": " • Jul 13, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Fast.ai v3 Lesson 2 Notes: Data Cleaning and Production",
            "content": ". Overview of Lesson . This lesson has two parts. The first part is about constructing a image classifier from your own data. It details data collection from Google images, creating a validation set, and cleaning the data using the model. . In the second part, we construct a simple linear model from scratch in PyTorch and train it using gradient descent and stochastic gradient descent. That part got quite lengthy so I made it its own blog post here. . Download Your Own Image Data . There is a trick to downloading data from google images. You can do the search manually for the images, then run some javascript magic to get the URLs for the images. You can then save these in a file and then download them from the command line. . Go to Google images and search for your desired images. . | Open the browser javascript console: (⌘+⎇+J on Mac, Crtl+Shift+J on Windows/Linux). . | Run the following the console: . urls = Array.from(document.querySelectorAll(&#39;.rg_di.rg_meta&#39;)).map(el=&gt;JSON.parse(el.textContent).ou); window.open(&#39;data:text/csv;charset=utf-8,&#39; + escape(urls.join(&#39; n&#39;))); . | This initiates a download of a CSV that contains all the urls to the images shown on Google images. . | Use fastai’s download_images function and pass it the path to the CSV file as the argument. . | Remove images that aren’t valid. Use fastai’s verify_images to delete these. . | Then Train With A CNN . Following the steps from Lesson 1: . Load data using the DataBunch API: . np.random.seed(42) # fix seed for to get same validation set data = ImageDataBunch.from_folder(path, train=&#39;.&#39;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats) . | Create the CNN learner and specify the architecture: . learn = create_cnn(data, models.resnet34, metrics=error_rate) . | First fit the head of the pretrained CNN with a few cycles: . learn.fit_one_cycle(4) ... . | Then unfreeze the body of the pretrained CNN: . learn.unfreeze() . | Run the learning rate finder: . learn.lr_find() . | Inspect the learning rate graph and find the strongest downward slope whose negative trend persists for while with the increasing learning rate. Try to pick a learning rate corresponding to the steepest part of this slope. . . | Train the whole network again for a few cycles using a range of learning rates for each layer group, with the learning rate you picked being the highest. This is called Discriminative Layer Training in fastai. . learn.fit_one_cycle(2, max_lr=slice(3e-5, 3)) . | In the Bear example Jeremy does this produces an error rate of 1.4% with a few hundred images and a few minutes of training time on a GPU. . Intepretation . For a classification task such as the Bear example in the lecture, you want to look at the confusion matrix to see where it is failing (well, except where you have loads of classes). FastAI has a handy class for interpreting classification results: . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . . Pretty good - only one mistake! . Cleaning Up Your Dataset . Maybe there is noise or mistakes in your dataset. If we download images from google then perhaps there are images that are of the wrong thing. We want to clean it up. Here is where human intelligence and a computer learner can be combined! It’s very unlikely that a mislabeled data is going to be predicted correctly and with high confidence. We can look at the instances that the computer learner is least confident about - i.e. the instances with the highest loss. There is a nice widget for Jupyter notebook for inspecting and deleting things called FileDeleter: . from fastai.widgets import * losses,idxs = interp.top_losses() top_loss_paths = data.valid_ds.x[idxs] . After cycling through FileDeleter and deleting the bad data you should eventually see fewer and fewer bad data points. At this point you are done and should retrain your model on the cleaned dataset. . Generally these CNN models are pretty good at handling small amounts of noise so this data cleaning will normally give you a small improvement. . Putting your Model into Production . You probably want to use CPU for inference, except for massive scale (and you almost certainly don’t need to train in real time). GPU is only effective if you can get things into neat batches with sizes like 64, which exploits the GPU parallelism. In PyTorch you can specify CPU via: . fastai.defaults.device = torch.device(&#39;cpu&#39;) . Let’s use the trained model for inference. We upload an image of a bear and store that in a variable img: . img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) . . And as per usual, we created a data bunch, but this time, we’re not going to create a data bunch from a folder full of images, we’re going to create a special kind of data bunch which is one that’s going to grab one single image at a time. So we’re not actually passing it any data. The only reason we pass it a path is so that it knows where to load our model from. That’s just the path that’s the folder that the model is going to be in. . You also need to pass it the same transformations , size, and normalizations that you used when training the CNN. You then create_cnn with this fake dataset and then load the weights that were saved in the training phase: . classes = [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddys&#39;] data2 = ImageDataBunch.single_from_classes(path, classes, tfms=get_transforms(), size=224).normalize(imagenet_stats) learn = create_cnn(data2, models.resnet34) learn.load(&#39;stage-2&#39;) . Then prediction is done using the predict method and passing in the real single image data: . pred_class,pred_idx,outputs = learn.predict(img) pred_class &gt; &#39;black&#39; . This is the engine of an web-app. The rest of the app can be coded up in a framework like Flask or Starlette. Here is a great example that uses Starlette: cougar-or-not. . There are services for hosting, such as: https://www.pythonanywhere.com/ . Things That Can Go Wrong . The problems will basically be either: . The learning rate is too high or too low | The number of epochs is too many or too few | . Learning rate too high: basically ruins everything and results in a super high validation loss . Learning rate too low: the error rate goes down really slowly. The other thing you see if your learning rate is too small is that your training loss will be higher than your validation loss. You never want a model where your training loss is higher than your validation loss. That always means you are under-fitting which means either your learning rate is too low or your number of epochs is too low. So if you have a model like that, train it some more or train it with a higher learning rate. . Number of epochs too few: training loss much higher than validation loss, which is a symptom of under-fitting. It needs to learn more. . Number of epochs too many: Too many epochs create something called “overfitting”. If you train for too long as we’re going to learn about it, it will learn to recognize your particular teddy bears but not teddy bears in general. . This is a good post about diagnosing your fit in machine learning: machine learning mastery. . The Truth About Overfitting . The only thing that tells you you are overfitting is that the error rate improves for a while and then starts getting worse again. . Myth: If the training loss is less than the validation loss then the model is overfitting. Absolutely not true. . Any model that is trained correctly will always have a lower training loss than validation loss . You want your model to have a low error. So as long as you’re training and your model error is improving, you’re not overfitting. . In Jeremy’s option, despite what you hear, it’s actually very hard to overtrain in deep learning. . Underfitting . How can the training loss be higher than the validation loss? This doesn’t really seem like it could happen except if you had some contrived validation set. It can however happen quite easily with training neural networks because of dropout. . Dropout is turned on while training and turned off in the validation. If the result is made much worse by dropout then it means that the network has not learned sufficiently well and it is therefore underfitting. Ways to fix this are: train with more epochs, use higher learning rate, use less dropout, or adjust weight decay parameters. . Batch Norm also operates differently at training and test time. . SGD From Scratch . This part kind of outgrew this blog post so I have spun this out into its own blog post here. . Jeremy Says… . If forum posts are overwhelming, click “summarize this topic” at the bottom of the first post. (Only works for &gt;50 replies). | Please follow the official server install/setup instructions, they work and are easy. | It’s okay to feel intimidated, there’s a lot, but just pick one piece and dig into it. Try to push a piece of code, or learn a concept like regular expressions, or create a classifier, or whatever. Context: Lesson 2: It’s okay to feel intimidated 30 | If you’re stuck, keep going. See image below! Context: Lesson 2: If you’re stuck, keep going 38 | If you’re not sure which learning rate is best from plot, try both and see. | When you put a model into production, you probably want to use CPU for inference, except at massive scale. Context: Lesson 2: Putting Model into Production 17 | Most organizations spend too much time gathering data. Get a small amount first, see how it goes. | If you think you’re not a math person, check out Rachel’s talk: There’s no such thing as “not a math person” 56. My own input: only 6 minutes, everyone should watch it! | . (Source: Robert Bracco) . Q &amp; A . When generating new image dataset, how do you know how many images are enough? What are ways to measure “enough”? . That’s a great question. Another possible problem you have is you don’t have enough data. How do you know if you don’t have enough data? Because you found a good learning rate (i.e. if you make it higher than it goes off into massive losses; if you make it lower, it goes really slowly) and then you train for such a long time that your error starts getting worse. So you know that you trained for long enough. And you’re still not happy with the accuracy﹣it’s not good enough for the teddy bear cuddling level of safety you want. So if that happens, there’s a number of things you can do and we’ll learn pretty much all of them during this course but one of the easiest one is get more data. If you get more data, then you can train for longer, get higher accuracy, lower error rate, without overfitting. . Unfortunately, there is no shortcut. I wish there was. I wish there’s some way to know ahead of time how much data you need. But I will say this﹣most of the time, you need less data than you think. So organizations very commonly spend too much time gathering data, getting more data than it turned out they actually needed. So get a small amount first and see how you go. . | What do you do if you have unbalanced classes such as 200 grizzly and 50 teddies? . Nothing. Try it. It works. A lot of people ask this question about how do I deal with unbalanced data. I’ve done lots of analysis with unbalanced data over the last couple of years and I just can’t make it not work. It always works. There’s actually a paper that said if you want to get it slightly better then the best thing to do is to take that uncommon class and just make a few copies of it. That’s called “oversampling” but I haven’t found a situation in practice where I needed to do that. I’ve found it always just works fine, for me. . | . Links and References . Link to Lesson 2 lecture | Homework notebooks: Notebook 1: lesson2-download.ipynb | Notebook 2: lesson2-sgd.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson2 Detailed Notes. | This is an in-depth tutorial on PyTorch: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e | How (and why) to create a good validation set by @rachel | There’s no such thing as “not a math person” by @rachel | Responder - a web app framework built on top of Starlette | Post about an alternative image downloader/cleaner by @cwerner | A tool for excluding irrelevant images from Google Image Search results by @melonkernel | Machine Learning is Fun - source of image/number GIF animation shown in lesson | A systematic study of the class imbalance problem in convolutional neural networks, mentioned by Jeremy as a way to solve imbalanced datasets. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/12/fast-ai-lesson-2-notes-data-cleaning-and-production.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/12/fast-ai-lesson-2-notes-data-cleaning-and-production.html",
            "date": " • Jul 12, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Fast.ai v3 Lesson 1 Notes: Image Classification",
            "content": ". Overview of Lesson . This is the introductory lesson to fastai part 1! . The key outcome of this lesson is that we’ll have trained an image classifier which can recognize pet breeds at state of the art accuracy. The key to this success is the use of transfer learning, which will be a key platform for much of this course. We’ll also see how to analyze the model to understand its failure modes. In this case, we’ll see that the places where the model is making mistakes is in the same areas that even breeding experts can make mistakes. . Task 1 - World Class Image Classifier . Fastai opts to teach deep learning backwards - rather than starting at the level of neurons they start with learning to use the state of the art algorithms and networks from the beginning. Learning to become a practitioner with the best practices first and then gradually learning the technical details later. . | Task 1: Training a world class image classification model. . | Image classification has been one of deep learning’s biggest successes so far. . | 10 years ago separating cat and dog images was a hard problem. With classical methods researchers were scoring ~80%. With today’s algorithms it’s actually too easy and scores on the cats vs dogs dataset are almost 100%. That’s why we used the harder dataset of cat and dog breeds. . | Cat breeds and dog breeds dataset from Oxford: Cats and Dogs Breeds Classification Oxford Dataset | Kaggle. . | This task found in the Jupyter notebook: lesson1-pets.pynb . | . Load the Data . FastAI has its own way of handling different datasets: the DataBunch API. . | This integrates the loading of different types of data, the labeling, splitting into train/val/test, and the data transformations for standardisation, normalisation, and training augmentation. . | To load the cats and dogs dataset: . pat = r&#39;/([^/]+)_ d+.jpg$&#39; data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=64, ).normalize(imagenet_stats) . | What is this doing? Let’s look at the docs: . class ImageDataBunch(fastai.basic_data.DataBunch) | DataBunch suitable for computer vision. ... | from_name_re(path:Union[pathlib.Path, str], fnames:Collection[pathlib.Path], pat:str, valid_pct:float=0.2, **kwargs) from builtins.type | Create from list of `fnames` in `path` with re expression `pat`. . | get_transforms is a function that returns a list of default image transformations for data augmentation. | size=224 resizes the images to 224x224. This is the size that the network we are using (resnet34) has been trained on. | bs is the batchsize. | normalize(imagenet_stats) normalizes the images so that the pixel values are between 0 and 1 (necessary for using the neural network). The network has been pretrained on imagenet data so we need to normalize our new data with respect to the imagenet data. This way the images are placed near the distribution that the network was trained on and so gives the network data that it is ‘used to’ seeing. | . Training a Model using a Pretrained ResNet . ‘ResNet’ is the name of a particular kind of Convolutional Neural Network (CNN). Details of it will be covered later. . | The ResNet we will use has been pretrained. This means that it was trained to solve another image classification problem (namely ImageNet) and we are reusing the learned weights of that network as a starting point for a new imaging problem. . | Why ResNet and not some other architecture? From looking at benchmarks it has been found that ResNet generally ‘just works well’ for image tasks. (See also question in Q &amp; A section below). . | Here’s how to create a CNN with the fastai library: . learn = create_cnn(data, models.resnet34, metrics=error_rate) . | data is the DataBunch object of the cats/dogs data we created earlier. . | Here we are using a variant of ResNet called resnet34. The 34 simply means it has 34 layers. There are others avaiable with 18, 50, and more layers. . | One-cycle policy: . learn.fit_one_cycle(4) . | People train neural networks using Stochastic Gradient Descent (SGD). Here the training set is divided into random batches (say of size 64) and the network weights are updated after each batch. After the network has seen all the batches, this is called an epoch. The rate at which the weights are changed is called the learning rate. Typically people set this to a single value that remains unchanged during an epoch. Not here though. . | The One-cycle policy is a way of training the neural network using SGD faster by varying the learning rate and solver momentum over a group of epochs. . | Sylvain explains (source): . He [Leslie] recommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude. . | Here are plots of how the learning rate and momentum vary over the iterations (batches): . . | The peak of the learning rate has a value of 1x the inputted learning rate. The bottom value is 0.1x the inputted learning rate. The bottom of the momentum value is 0.85x the inputted momentum value. . | The momentum varies contra to the learning rate. What’s the intuition behind this? When the learning rate is high we want momentum to be lower. This enables the SGD to quickly change directions and find a flatter region in parameter space. . | . | Learning Rate Finder . The method is basically successively increasing $ eta$ every batch using either a linear or exponential schedule and looking the loss. While $ eta$ has a good value, the loss will be decreasing. When $ eta$ gets too large the loss will start to increase. You can plot the loss versus $ eta$ and see by eye a learning rate that is largest where the loss is decreasing fastest. . | learn.lr_find() learn.recorder.plot() . | . | More on this will be covered in the next lesson. | . | . Getting Started With the Notebooks . All the course notebooks for part 1 are found here: notebooks [github]. | The course guide can be found here: course guide. | For running and experimenting with the fastai notebooks I personally like to use: kaggle kernels | or google colab. | . | . Jeremy Says… . Don’t try to stop and understand everything. | Don’t waste your time, learn Jupyter keyboard shortcuts. Learn 4 to 5 each day. | Please run the code, really run the code. Don’t go deep on theory. Play with the code, see what goes in and what comes out. | Pick one project. Do it really well. Make it fantastic. | Run this notebook (lesson1-pets.ipynb), but then get your own dataset and run it! (extra emphasis: do this!) If you have a lot of categories, don’t run confusion matrix, run… interp.most_confused(min_val=n) | (Source: Robert Bracco) . Q &amp; A . As GPU mem will be in power of 2, doesn’t size 256 sound more practical considering GPU utilization compared to 224? . The brief answer is that the models are designed so that the final layer is of size 7 by 7, so we actually want something where if you go 7 times 2 a bunch of times (224 = 7x2x2x2x2x2), then you end up with something that’s a good size. Objects often appear in the middle of an image in the ImageNet dataset. After 5 maxpools, a 224x224 will be 7x7 meaning that it will have a centerpoint. A 256x256 image will be 8x8 and not have a distinct centerpoint. . | Why resnet and not inception architecture? . Resnet is Good Enough! See the DAWN benchmarks - the top 4 are all Resnet.You can consider different models for different use cases. For example, if you want to do edge computing, mobile apps, Jeremy still suggests running the model on the local server and port results to the mobile device. But if you want to run something on the low powered device, there are special architectures for that. . Inception is pretty memory intensive. fastai wants to show you ways to run your model without much fine-tuning and still achieve good results. The kind of stuff that always tends to work. Resnet works well on a lot of image classification applications. . | Will the library use multi GPUs in parallel by default? . The library will use multiple CPUs by default but just one GPU by default. We probably won’t be looking at multi GPU until part 2. It’s easy to do and you’ll find it on the forum, but most people won’t be needing to use that now. . | . Links and References . Link to Lesson 1 lecture | Homework notebooks: Notebook 1: lesson1-pets.pynb | . | Detailed lesson notes - thanks to @hiromi | Stanford DAWN Deep Learning Benchmark (DAWNBench) · | [1311.2901] Visualizing and Understanding Convolutional Networks | Another data science student’s blog – The 1cycle policy | Learning Rate Finder Paper | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/computer-vision/fastai/fastai-v3-notes/2019/07/06/fast-ai-lesson-1-notes-image-classification.html",
            "relUrl": "/deep-learning/machine-learning/computer-vision/fastai/fastai-v3-notes/2019/07/06/fast-ai-lesson-1-notes-image-classification.html",
            "date": " • Jul 6, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "My First Pull Request! :)",
            "content": "I’m very proud to have made my first open source contribution! :-) I added a feature to the FastAI deep learning library to make its data types ‘countable’ and thus work with the collections.Counter class. . Problem . While I was working through the image classification homework from lesson 2. I wanted to check the how many images of each class there were in my data block. The best way to count the number of values in a collection in python is to use the collections.Counter class which creates a dictionary mapping value to count. . However when I tried this with the data block I got this: . &gt; Counter(data.train_ds.y) Counter({ Category chimp 1 Category gorilla 1 Category gorilla 1 Category chimp 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 ... . Solution . This problem is caused by fact that there was no __eq__ implemented for the Category class. When different Category objects were compared python’s default equality would only check whether they were literally the same object rather than checking their values. To get an object to work with a dictionary class in python you also have to implement a __hash__ method. . I confirmed this with the hot patch: . &gt; Category.__eq__ = lambda self, that: self.data == that.data &gt; Category.__hash__ = lambda self: hash(self.obj) &gt; Counter(data.train_ds.y) Counter({Category orangutan: 56, Category gorilla: 177, Category chimp: 173}) . With Sylvain Gugger’s guidance, I then implemented __eq__ method properly in fastai for the ground class ItemBase so that all of the different data classes in fastai could have equality. Hash didn’t make sense for all the subclasses (like floats or arrays of numbers), so we compromised on implementing hash methods only on the subclasses where it made sense. . Here is the link to my pull request: https://github.com/fastai/fastai/pull/1717. . Aside: Making Python Objects Counter-Ready . In order to make your python objects play nice with dictionary’s they need to override two python built-ins: . __eq__ | __hash__ | Suppose that the python object contains some value val that defines the object’s uniquness. . Let’s create a python class for categories called Cat: . class Cat: def __init__(self, val): self.val = val def __str__(self): return f&#39;Cat({self.val})&#39; def __repr__(self): return f&#39;Cat({self.val})&#39; . This class won’t work properly with Counters: . &gt; xs = [Cat(2), Cat(2), Cat(1), Cat(3)] &gt; Counter(xs) Counter({Cat(2): 1, Cat(2): 1, Cat(1): 1, Cat(3): 1}) . Equality doesn’t work: . &gt; Cat(2) == Cat(2) False . Two objects with the same value don’t have the same hash: . &gt; hash(Cat(2)) -9223372036573193412 &gt; hash(Cat(2)) 281542562 . You have to implement the hash and equality built-ins: . class Cat: def __init__(self, val): self.val = val def __str__(self): return f&#39;Cat({self.val})&#39; def __repr__(self): return f&#39;Cat({self.val})&#39; def __eq__(self, other): return self.val == other.val def __hash__(self): return hash(self.val) . Now it works: . &gt; xs = [Cat(2), Cat(2), Cat(1), Cat(3)] Counter(xs) Counter({Cat(2): 2, Cat(1): 1, Cat(3): 1}) . How does this work? The Cat objects are being used as keys in the Counter. When a new Cat object comes into the Counter we need to compare it with all the other keys already in the Counter. If a Cat object of the same value is there already then we need to increment the value associated with that Cat object. . For efficiency, however, dictionaries in python don’t store the keys in a big list rather in buckets. When a new Cat object comes into the Counter it is assigned to a bucket using its hash value. Here three things can happen. . If the bucket is empty then store the value there. | If the bucket isn’t empty compare the incoming object with the objects there using eq. If they are the same, increment the counter. | If they are different you have a hash collision. Store the incoming object in the bucket and set the counter value to 1. | In Summary . For correctness with dictionaries: . obj1 == obj2 if obj1.val == obj2.val | hash(obj1) == hash(obj2) if obj1.val == obj2.val | If the object is to be used as a key in a Counter we need to be able to correctly compare it to other keys in the Counter. If two objects are equal then we know that they are the same key and we can increment the counter. Two objects with the same value need to be hashed to the same bucket. . For efficiency with dictionaries: . hash(obj1) should ideally != hash(obj2) if obj1.val != obj2.val | It is possible, though undesirable, that two objects with different values get hashed to the same bucket. This is called a hash collision. This isn’t a correctness problem, rather an efficiency problem. Every hash collision is like an if statement in the dictionary to specially handle those cases. Ideally, every unique value should have its own unique hash so that there are no hash collisions. If we imagine the worst case where our hashing algorithm is something like def hash(x): return 0, then ever item ends up in the same bucket. To look up an item in this dictionary where everything is a hash collision we’d have to brute-force it and on average look at every item individually before we find what we are looking for. This would reduce the dictionary look-up performance to $ mathcal{O}(N)$, the same as an unordered list, instead of the $ mathcal{O}(1)$ performance that it should have. . Links . Link to pull request: https://github.com/fastai/fastai/pull/1717 | Link to forum post: https://forums.fast.ai/t/get-value-counts-from-a-imagedatabunch/38784/21 | .",
            "url": "https://jimypbr.github.io/blog/python/fastai/2019/04/01/my-first-pull-request.html",
            "relUrl": "/python/fastai/2019/04/01/my-first-pull-request.html",
            "date": " • Apr 1, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "SIFTing Images",
            "content": ". The Fleuron Project . I have been involved in the Fleuron project this year. The aim of this project is to use computer vision to extract printers’ ornaments from a large corpus of ~150,000 scanned documents (32 million pages) from the 18th century. Printed books in the 18th century were highly decorated with miniature pieces of printed artwork - ‘ornaments’. Their pages were adorned with ornaments that ranged from small floral embellishments to large and intricate head- and tailpieces, depicting all manner of people, places, and things. Printers’ ornaments are of interest to historians from many disciplines, not least for their importance as examples of early graphic design and craftsmanship. They can help solve the mysteries of the book trade, and they can be used to detect piracy and fraud. . In this project an OpenCV based code was developed to automatically detect ornaments in a scanned image of a page and extract them into their own file. This code worked very well and extracted over 3 million images from the data, but it was quite over-sensitive in its detection so there were many false-positives. The code was heuristic based and didn’t use any machine intelligence to further evaluate the potential images for validity. We therefore chose to tune the code to have good recall at the expense of precision – i.e. we would rather it didn’t miss valid images, even if it means that some invalid images get through too. Often these invalid images were of blocks of text so we initially experimented with using OCR to catch these cases. However this had the unwanted effect of making recall worse. We decided a better solution would be to train a machine learning classifier to discriminate between the valid and invalid images. . My contribution to the project was to use the extraction code to generate data, which I then hand-labelled to create a training set to train a machine learning based filter to remove the bad images. The final filtered dataset is presented on the website: http://fleuron.lib.cam.ac.uk (EDIT 2021: Fleuron now defunct. Has since become Compositor), which I also designed and built. In this blog post I will describe the methodology and results of the image filtering part of the project. . Extraction . The first challenge is to extract the ornaments from the raw page scans. This is an example of a page containing two ornaments, at the top of the page and at the start of the text body: . . We required an algorithm that could ignore the text and draw bounding boxes around the two ornaments on the page. To solve this problem we enlisted Dirk Gorissen to develop a method using Python and OpenCV. I will not dive deeply into how Dirk’s algorithm works here. Basically it uses combines heuristics of where ornaments are typically located and how they look with various image filtering techniques to weed out text and other artifacts on the page to leave just the artwork intact. . Here is a demonstration of how each of the different stages of the algorithm work using on the single page shown above as an example: . . Ornaments are visually very dense compared to the text. In the first stage the image is cleaned removing dust and stains in the white space of the page. Then through several iterations of blurring and contouring are applied until just the ornaments are left as single contours as seen in stage 5. A bounding box is then drawn around these contours and content of these boxes is then extracted from the original image. . This method is simple and effective, but it is also apt to falsely classifying blocks of text. In the following example you can see clearly how this can happen: . . After running extraction on all of the pages, I found that in a random sample of the images, most of them were just images of blocks of text! However, given that most of the pages in the dataset contain only text and no ornaments, perhaps this is to be expected even if the algorithm is fairly good at removing text. . After extracting the ornaments from a large sample of the books, I hand labeled a random sample of 15000 images as valid and invalid. Here is a collage of valid images: . . Here is a collage of invalid images that we want to filter out: . . Image Filtering Pipeline . The choice of image representation is essential to getting a well performing machine learning based classifier. The Images are black and white and so we can’t use any colour features and contain very rich textures. The pipeline for the image filtering system: . Create a labelled data set for training | Represent each training image by a vector using Bag of Visual Words | Train a classifier on the vector to discriminate between valid and invalid images | Apply the classifier to unseen images in the data set. | Bag of Visual Words (BoVW) . The Bag of Visual Words (BoVW) method is a common feature representation of images in computer vision. The method is directly inspired by the Bag of Words (BoW) method used in text classificiation. In the BoW method, the basic idea is that a text document is split up into its component words. Each of the words in the document is then matched to a word in the dictionary, and the number of unique words in the document is counted. The text document is then represented as a sparse histogram of word counts that is as long as the dictionary. . This histogram can be interpreted as a vector in some high dimensional space, and two different documents will be represented by two different vectors. So for a dictionary with $D$ words the vector for document $i$ is: . [v_i = [n(w_1,i), n(w_2,i), …, n(w_D, i)]] . Where $n(w)$ counts the number of occurrences of word $w$. The distance between these two vectors (e.g. L2, cosine, etc) can therefore be used as a proxy for the similarity of the two documents. If everything is working well, then a low distance will indicate high similarity and a large distance will represent a high dissimilarity. With this representation we are able to throw machine learning algorithms at the data or do document retrieval. . BoVW is exactly the same method except that instead of using actual words it uses ‘visual words’ extracted from the images. Visual words basically take the form of ‘iconic’ patches or fragments of an image. . 1. Extract notable features from the images . 2. Learn a visual dictionary . Use a clustering algorithm like k-means with a apriori number of clusters (&gt;1000) to learn a set of $k$ compound visual words. . . 3. Quantize features using the visual vocabulary . Now we could then take an image, find its visual words and match each of those words to their nearest equivalent in the dictionary. . 4. Represent images by histogram of visual word counts . By counting how many times a word in the dictionary is matched, the image can be re-represented as a histogram of word counts: . . Similar looking images will have contains many of the same words and counts. . SIFT - The Visual Word . Now that we have outlined the concept of the BoVW method, what do we actually use as the ‘visual word’? To create the visual words I used SIFT - ‘Scale Invariant Feature Transform’. SIFT is a method for detecting multiple interesting keypoints in a grey-scale image and describing each of those points using a 128 dimensional vector. The SIFT descriptor is invariant to scale, rotation, and illumination, which is why it is such a popular method in classification and CBIR. An excellent technical description of SIFT can be found here. . OpenCV has an implementation of a SIFT detector included. The following code finds all the keypoints in an image and draws them back onto the image. . img = cv2.imread(&#39;image_2.png&#39;) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) kp, desc = sift.detectAndCompute(img_gray, None) img2 = cv2.drawKeypoints(img_gray, kp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) cv2.imwrite(&#39;image_sift_2.png&#39;, img2) . Here is the output of this for two images from the dataset, one valid and the other invalid: . There is a simple improvement that can be made to SIFT called RootSIFT. RootSIFT is a small modification to the SIFT descriptor that corrects the L2 distance between two SIFT descriptors. This generally always improves performance for classification and image retrieval. Here is an implementation in python: . def rootsift_descriptor(f): &quot;&quot;&quot; Extract root sift descriptors from image stored in file f :param: f : str or unicode filename :return: desc : numpy array [n_sift_desc, 128] &quot;&quot;&quot; img = cv2.imread(f) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) sift = cv2.SIFT() kp, desc = sift.detectAndCompute(img_gray, None) if desc is None: print(&#39;Warning: No SIFT features found in {}&#39;.format(f)) return None desc /= (desc.sum(axis=1, keepdims=True) + 1e-7) desc = np.sqrt(desc) return desc . Building a Visual Word Dictionary . To create a visual word dictionary we need to first collect and store all the RootSIFT descriptors from a large sample of images from our dataset. Here I used a sample size of 50,000 images. For 50,000 images, $N approx 1~ billion$. The following script iterates through every image in the target directory, finds the RootSIFT descriptors of the image, and then stores them in a large $N times128$ array in a HDF5 file. . import os import glob import cv2 import tables import numpy as np from bovw import rootsift_descriptor from random import sample, shuffle def create_descriptor_bag(filelist): &quot;&quot;&quot; creates an array of descriptor vectors generated from every file in filelist &quot;&quot;&quot; X = np.empty((0, 128), dtype=np.float32) N = len(filelist) for n, f in enumerate(filelist): print(&#39;Processing file: {} of {}...&#39;.format(n, N)) desc = rootsift_descriptor(f) if desc is not None: X = np.vstack((X, desc)) return X def main(): h5f = tables.openFile(&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;, &#39;w&#39;) atom = tables.Atom.from_dtype(np.dtype(&#39;Float32&#39;)) ds = h5f.createEArray(h5f.root, &#39;descriptors&#39;, atom, shape=(0, 128), expectedrows=1000000) PATH = &#39;/home/jb914/ECCO_dict/random50k/&#39; all_files = glob.glob(os.path.join(PATH, &#39;*.png&#39;)) rand_sample = [all_files[i] for i in sample(range(1, len(all_files)), 5000)] chunk_size = 100 for i in xrange(0,len(rand_sample), chunk_size): print(&#39;Creating rootsift descriptor bag...&#39;) X = create_descriptor_bag(rand_sample[i:i+chunk_size]) print(X[0].shape) print(len(X)) print(&#39;Writing file: rootsift_vectors_5000.hdf&#39;) for i in xrange(len(X)): ds.append(X[i][None]) ds.flush() #ds[:] = X h5f.close() if __name__ == &#39;__main__&#39;: main() . HDF5 files are great for storing large multidimensional arrays of data to disk because they store the meta-data of the array dimensions and allow for streaming the data from disk to memory. This is especially useful when the full data is much larger than memory like here. . Creating the dictionary is the hardest and most time consuming part with BoVW. There are many vectors to cluster, the number of words is very large (between 1000 and 1,000,000), and the vectors are high dimensional. This stretches the capabilities of many clustering algorithms in all possible ways. In my experiments I found that the standard K-Means clustering algorithm quickly became intractable for larger numbers of vectors and clusters. Moreover the algorithm is offline - it needs to see all the data at once. Algorithms better suited for this task are approximate k-means (AKM) and mini-batch k-means. . I found success with two open source implementations of these in fastcluster (AKM), and in MiniBatchKMeans from scikit-learn. Fastcluster has the advantage that it uses distributed parallelism via MPI to split the large data up across multiple machines. However this useful code lacks documentation and no longer maintained. MiniBatchKMeans on the other hand isn’t parallel, however it does allow for streaming of the data through memory so it works great with HDF5. . In my experiments I found that setting the dictionary size to 20,000 words was sufficient. . The following script can stream a HDF5 file in a user defined number of chunks performing clustering with those chunks. The total clustering time for this was approximately 24 hours running in serial on a Intel Xeon Ivybridge CPU. . from __future__ import print_function import numpy as np import tables import pickle from sklearn.cluster import MiniBatchKMeans from time import time def main(n_clusters, chunk=0, n_chunks=32, checkpoint_file=None): datah5f = tables.open_file(&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;, &#39;r&#39;) shape = datah5f.root.descriptors.shape datah5f.close() print(&#39;Read in SIFT data with size in chunks: &#39;, shape) print(&#39;Running MiniBatchKMeans with cluster sizes: &#39;, n_clusters) c = n_clusters print(&#39;n_clusters:&#39;, c) if checkpoint_file: mbkm = pickle.load(open(checkpoint_file, &#39;r&#39;)) else: mbkm = MiniBatchKMeans(n_clusters=c, batch_size=10000, init_size=30000, init=&#39;random&#39;, compute_labels=False) step = shape[0] / n_chunks start_i = chunk*step for i in xrange(start_i, shape[0], step): datah5f = tables.open_file(&#39;/scratch/ECCO/rootsift_vectors_{}.hdf&#39;.format(datasize), &#39;r&#39;) X = datah5f.root.descriptors[i:i+step] datah5f.close() t0 = time() mbkm.partial_fit(X) print(&#39; t ({} of {}) Time taken: {}&#39;.format(chunk, n_chunks, time()-t0)) chunk += 1 pickle.dump(mbkm, open(&#39;chkpt_{}.p&#39;.format(n_clusters), &#39;w&#39;)) X = mbkm.cluster_centers_ print(X.shape) f = tables.open_file(&#39;fleuron_codebook_{}.hdf&#39;.format(n_clusters), &#39;w&#39;) atom = tables.Atom.from_dtype(X.dtype) ds = f.create_carray(f.root, &#39;clusters&#39;, atom, X.shape) ds[:] = X f.close() if __name__ == &#39;__main__&#39;: main(n_clusters=20000, chunk=0, n_chunks=4, checkpoint_file=None) . Matching Key Points to the Dictionary . With the dictionary created the next step is to represent all the images in the labeled training set as a histogram of matching keypoints. This is a nearest neighbour matching problem so with a brute force algorithm this is a $O(N)$ so this is slow for very high numbers of words. Faster nearest neighbour matching can be achieved with the FLANN library. OpenCV contains a wrapper for FLANN. I wrote a class that uses the FLANN matcher in OpenCV to match an array of descriptors to a codebook: . FLANN_INDEX_COMPOSITE = 3 FLANN_DIST_L2 = 1 class Codebook(object): def __init__(self, hdffile): clusterf = tables.open_file(hdffile) self._clusters = np.array(clusterf.get_node(&#39;/clusters&#39;)) clusterf.close() self._clusterids = np.array(xrange(0, self._clusters.shape[0]), dtype=np.int) self.n_clusters = self._clusters.shape[0] self._flann = cv2.flann_Index(self._clusters, dict(algorithm=FLANN_INDEX_COMPOSITE, distance=FLANN_DIST_L2, iterations=10, branching=16, trees=50)) def predict(self, Xdesc): &quot;&quot;&quot; Takes Xdesc a (n,m) numpy array of n img descriptors length m and returns (n,1) where every n has been assigned to a cluster id. &quot;&quot;&quot; (_, m) = Xdesc.shape (_, cm) = self._clusters.shape assert m == cm result, dists = self._flann.knnSearch(Xdesc, 1, params={}) return result . The following code takes a list of image files and a dictionary and returns the count vectors for each of those image files using the sparse matrix type from scipy. It is also multi-threaded using the joblib library: . def count_vector(f, codebook): &quot;&quot;&quot; Takes a list of SIFT vectors from an image and matches each SIFT vector to its nearest equivalent in the codebook :param: f : Image file path :return: countvec : sparse vector of counts for each visual-word in the codebook &quot;&quot;&quot; desc = rootsift_descriptor(f) if desc is None: # if no sift features found return 0 count vector return lil_matrix((1, codebook.n_clusters), dtype=np.int) matches = codebook.predict(desc) unique, counts = np.unique(matches, return_counts=True) countvec = lil_matrix((1, codebook.n_clusters), dtype=np.int) countvec[0, unique] = counts return countvec class CountVectorizer(object): def __init__(self, vocabulary_file, n_jobs=1): self._codebook = Codebook(hdffile=vocabulary_file) if n_jobs == -1: self.n_jobs = cpu_count() else: self.n_jobs = n_jobs def transform(self, images): &quot;&quot;&quot; Transform image files to a visual-word count matrix. :param: images : iterable An iterable of str or unicode filenames :return: X : sparse matrix, [n_images, n_visual_words] visual-word count matrix &quot;&quot;&quot; sparse_rows = Parallel(backend=&#39;threading&#39;, n_jobs=self.n_jobs)( (delayed(count_vector)(f, self._codebook) for f in images) ) X = lil_matrix((len(images), self._codebook.n_clusters), dtype=np.int) for i, sparse_row in enumerate(sparse_rows): X[i] = sparse_row return X.tocsr() . Given a list of files the following code will return the count vectors for those images: . vectorizer = CountVectorizer(&#39;codebook_20k.hdf&#39;, n_jobs=-1) Xcounts = vectorizer.transform(images) . tf-idf . Not all words are created equal, some are more frequent than others. This is the same in human language and in the create visual vocabulary. Words like ‘the’, ‘what’, ‘where’, etc will swamp the count vectors of english words in almost all english documents. Clearly they are less interesting than a rare word like ‘disestablishment’ and that we’d like two different documents both containing a word like ‘disestablishment’ to have a high similarity. So we’d like to reweight words which appear in few documents so that they have a higher importance, and words that appear in most documents to have lower importance. In another case, if a document only contained the word ‘disestablishment’ 1000 times should it be 1000 times more relevant than a document containing it once? So within an individual document we may want to reweight words that are repeated over and over so that they cannot artificially dominate. . These two reweightings can be achieved using tf-idf (term frequency inverse document frequency) weighting. This weighting is designed to reflect how important a particular word is in a document corpus. It is perfectly applicable in our visual word case also. Scikit-learn has an implementation of a tf-idf transformer for text classification that we can repurpose here. To following produces the final representation for the training data that we can use in the machine learning algorithm, $X$. . from sklearn.feature_extraction.text import TfidfTransformer transformer = TfidfTransformer() X = transformer.fit_transform(Xcount) . Visualisation of BoVW . That that we’ve transformed the images into tf-idf weighted, 20k dimensional, sparse vectors to visual word counts, we can visualise them and see if there is any apparent structure in this high dimensional data. Great algorithms for visualising high dimensional data are PCA and T-SNE, both of which have implementations in scikit-learn. I found here that PCA worked best. For high dimensional sparse data, the TruncatedSVD algorithm works best: . from sklearn.decomposition import TruncatedSVD svd = TruncatedSVD(n_components=2) Z = svd.fit_transform(X) . We can plot this with the images inlined and with colours representing the valid (red) and invalid (blue) labels: . . You can clearly see that there is clear structure in the higher dimensions and that the valid and invalid images separate quite well from each other. This is quite promising for the performance of a machine learning algorithm! . Classifying the Bad Images with Machine Learning . I tried a number of algorithms including Random forest, logistic regression and linear SVM. I found that SVM with a linear kernel by far performed the best compared to the other algorithms. . from sklearn.svm import LinearSVC clf = LinearSVC() clf.fit(X_train, y_train) . LinearSVC with the default settings performed very well with $97 %$ accuracy. High accuracy is generally a good sign, especially here where the numbers of valid and invalid images are of a similar size. Two other important statistics for classification are precision and recall. . Recall is a measure of what the probability that the classifier will identify and image as invalid given that it is invalid: $P( hat{y}=1 | y=1)$. You can think of recall as the ratio of the number of images correctly classed as invalid over the number of all invalid images. . Precision on the other hand is a measure of the probability that an image is invalid given that the classifier says it is invalid: $P(y=1| hat{y}=1)$. You can think of precision as the ratio of the number of images correctly classed as invalid over the number of all images classified. . The difference between them is subtle (here is a great explanation of the difference), but you may want to favour a trade-off of one for the other depending on your business case. In our case it is worse to misclassify a valid images as invalid because we are losing good images. We would much rather have some invalid images get through than lose good images, which is the same as favouring extra precision over recall. . We can tune the precision by adjusting the class weights of the Linear SVM, such that the penalty for classifying a valid image as invalid is much worse than classifying an invalid image as valid. I used cross-validation to find the best values for these. These give the valid images a weight of 20 and invalid images a weight of 0.1: . from sklearn.svm import LinearSVC clf = LinearSVC(class_weight={0: 20, 1: 0.1}) clf.fit(X_train, y_train) . This yielded the final performance of $95 %$ accuracy, $99.5 %$ precision, and $93.8 %$ recall: . Confusion matrix: [[1134 11] [ 161 2439]] Accuracy score: 0.954072096128 False Positive Rate: 0.00960698689956 False Negative Rate: 0.0619230769231 Precision: 0.995510204082 Recall: 0.938076923077 F1 Score: 0.965940594059 . The performance of approach this is very good. The trained classifier was then applied across the whole image dataset. In the end there were approximately 3 million invalid images and 2 million valid images detected. . Further Ideas . Image search . The BoVW approach is also very useful for image retrieval. This means that given some image we can find duplications and similar looking images in the rest of the image set simply by finding the BoVW vectors that are closest to that image’s own BoVW vector. This is just a nearest neighbour search. It is complicated by the number of images because scaling nearest neighbour search with large numbers of vectors that don’t necessarily fit into memory relies on more complicated algorithms. . Neural Networks . Convolutional Neural Networks (CNNs) have shown great application in image classification in recent years. While they perform well at classification, they also have the advantage that they can discover vector representations of the images given the just the raw pixels. So it doesn’t require all this work with inventing a representation for images such as BoVW. The downside is that they require a lot of data (10s of thousands of examples) to be effective. Rather than hand labelling more examples, it would be quicker to look at the output of images classified by the SVM, and eyeball any false negatives or false positives in there. Artificial data could also be created using image transformations like rotation and inversion. .",
            "url": "https://jimypbr.github.io/blog/python/machine-learning/computer-vision/2016/12/30/fleuronbovw.html",
            "relUrl": "/python/machine-learning/computer-vision/2016/12/30/fleuronbovw.html",
            "date": " • Dec 30, 2016"
        }
        
    
  
    
        ,"post7": {
            "title": "How to do Polymorphism in Clojure (3/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . Arithmetic with Mixed Types . We have so far built a number system with five different types and an add function that can take any two numbers of the same type and add them together with the same interface. . However what if I wanted to add a Complex-r and a Real together? We would need to convert the Real to a Complex-r and then add them together. We could do this by adding more multimethods: . (defmethod add [::complex-r ::real] (...)) (defmethod add [::real ::complex-r] (...)) . You can see that doing this for all combinations of types would lead to a combinatorial explosion of new multimethods! Clojure’s hierarchies can enable us to solve this problem without the need to write a factorial number of functions. . We can side-step the problem of writing a combinatorical number of new functions by noting that to add two different types of number together we have to promote one of the numbers to be the same type as the other. With this in mind we can create a catch-all method for add that catches all the cases of mixed types, coerces the types to be the same and then calls add again with the converted types. It will then call one of the previously defined add methods where the types are the same. . We can implement the catch-all case by creating a new abstraction ::number at the top of the hierarchy where every number we have created so far is a descendant of ::number: . (derive ::complex ::number) (derive ::integer ::number) (derive ::float ::number) (derive ::rational ::number) . We can now create the catch-all add method that will that will matc [::number ::number]: . (defmethod add [::number ::number] [n1 n2] (apply add (coerce-types n1 n2))) . This works because any combination of different types will fail all the rules of the other multimethods except this one, because all of our types are descendants of ::number. This method then calls the coercion function (which I will define later) to convert the arguments into the same type and then calls the add multi-function with these converted arguments. This will then find the correct multimethod for the now uniform types return the result. . Type Coercion with the Numeric Tower . We can convert any Integer to a Rational without a loss of information. You cannot convert any Rational to an Integer without a loss of information though. Similarly any Rational can be converted to a Float, and any Float can be converted to a Complex number. . This chain of conversions is the numeric tower: . Integer -&gt; Rational -&gt; Float -&gt; Complex | . We want to be able to call a function raise on one of our types and get back the same numeric value, but represent by the next type in the tower. The function depends on the type of the argument so we can create another protocol: . (defprotocol PRaise (raise [x] &quot;raises a number type to the next type in the numeric tower with equivalent value&quot;)) (extend-protocol PRaise Int (raise [x] (rational (:n x) 1)) Rational (raise [x] (float (/ (numer x) (double (denom x))))) Float (raise [x] (complex-r (:n x) 0))) . Given a pair of types, e.g. [::integer ::float], we need a way to encode the fact that ::float is higher in the tower than ::integer, and so ::integer needs to be raised until it is a ::float. This could be done with a map associating the types to a rank number, but this is pretty inflexible if we add more types. . The numeric tower is a hierarchy so we are actually better off using Clojure’s ad-hoc hierarchies again. In Clojure there is a global hierarchy structure which we have used so far for the arithmetic, but you are free to create your own hierarchies with make-hierarchy. This exists independently of the global hierarchy we used earlier. This is handy because the numeric tower hierarchy is different from the arithmetic hierarchy. . (def numeric-tower (-&gt; (make-hierarchy) (derive ::complex ::complex-p) (derive ::complex ::complex-r) (derive ::float ::complex) (derive ::rational ::float) (derive ::integer ::rational))) . Using this we can create comparator functions for two keyword types. For example: . =&gt; (higher? ::float ::integer) true =&gt; (lower? ::rational ::complex-r) true . These are easy to implement using the functions for querying hierarchies, ancestors and descendents: . (defn higher? &quot;Is type 1 higher in the numeric tower than type 2?&quot; [t1 t2] (if (contains? (ancestors numeric-tower t2) t1) true false)) (defn lower? &quot;Is type 1 lower in the numeric tower than type 2?&quot; [t1 t2] (if (contains? (descendants numeric-tower t2) t1) true false)) . With these functions we can implement coerce-types: . (defn coerce-types &quot;Given two different number types raise the lesser type up to be the same as the greater type&quot; [x y] (let [t1 (kind x) t2 (kind y)] (cond (lower? t1 t2) (recur (raise x) y) (higher? t1 t2) (recur x (raise y)) :else [x y]))) . Trying this out in a REPL: . =&gt; (raise (integer 3)) #numbers.core.Rational{:n 3, :d 1} =&gt; (raise (float 4)) #numbers.core.Complex-r{:real 4.0, :imag 0} =&gt; (coerce-types (integer 4) (rational 5 6)) [#numbers.core.Rational{:n 4, :d 1} #numbers.core.Rational{:n 5, :d 6}] =&gt; (coerce-types (rational 5 6) (complex-r 7 8)) [#numbers.core.Complex-r{:real 0.8333333333333333, :imag 0} #numbers.core.Complex-r{:real 7, :imag 8}] . Final Product and Further Ideas . We have implemented a number system that can represent integers, floating point numbers, rational numbers, rectangular complex numbers, and polar complex numbers. It can perform basic binary arithmetic operations add, subtract, multiply, and divide on any combination of number types. . Let’s demonstrate the final product in the REPL: . (add (integer 4) (integer 6)) #numbers.core.Int{:n 10} (add (integer 4) (float 6)) #numbers.core.Float{:n 10.0} (add (rational 5 6) (float 6)) #numbers.core.Float{:n 6.833333333333333} (mul (float 6) (complex-p 10 0.5)) #numbers.core.Complex-p{:magnitude 60.0, :angle 0.5} (div (integer 5) (rational 7 5)) #numbers.core.Rational{:n 25, :d 7} (div (integer 5) (complex-r 7 5)) #numbers.core.Complex-p{:magnitude 0.5812381937190965, :angle -0.6202494859828215} . Seems to work fairly well! . I could continue building on this, but that would be beyond the scope of this blog post. There are a few things worth thinking about nonetheless. One aesthetic improvement would be implementing a pretty REPL representation of the custom number types. E.g. having 3/4 instead of Rational(:n 3 :d 4). Clojure has a single function that takes care of printing things: clojure.lang.print-method. This is simply another multimethod like we’ve already been using. Adding nicer printing is straight-forward: . ;; nicer printing for rationals (defmethod print-method Rational [x ^java.io.Writer writer] (.write writer (str (numer x) / (denom x)))) ;; and similarly for the other types... ;; ... . In SICP there is also an exercise to extend the number system to be implemented purely with its own types. So while Integer and Float types would be wrappers for Clojure’s own types, the Rational, Complex-r, and Complex-p types could themselves be composed of any of the other types. So you could have a Rational number whose demoninator and numerator were complex numbers. Or conversely a complex number whose real and imaginary parts were rational numbers. This would be achieved with some modification to the existing code. You would need to replace all uses of Clojure’s primitive arithmetic functions (+, *, etc.) with our multimethods (add, mult, etc), and also create our own implementations of sqrt, sin, cos, and atan that handled our number types. This could be done by adding new protocols. . Final Thoughts . This is a nice non-trivial program from SICP that demonstrates the ideas and challenges in polymorphism. Implementing this in Clojure required us to use just about every feature for polymorphism in Clojure’s core library. But I am satified with how Clojure could handle everything without any requiring any ‘hacks’ or major redesigns. . I think that this highlights one of the good aspects of Clojure’s design and philosophy. Namely, the decoupling (or ‘decomplecting’) of ideas in the language. In this demo there are the following concepts: . Data (in the form of records). | Functions. | Single and multiple dispatch (protocols and multimethods) to functions. | Hierarchies of types. | Functions and data are decoupled because records are just data and you can’t attach methods onto records in Clojure the way you can with class methods in OOP. You don’t need getter/setter methods because records are maps so you just the functions for maps. . Data and dispatch are decoupled. You do not need to know at design time which protocols or multimethods are going to use your record type. In Java where polymorphic single-dispatch is achieved via interfaces or abstract classes you need explicitly implement or extend your class when you write it. In Clojure you can add protocols or multimethods to any existing type when are where you want. . Dispatch in Clojure is simply a fancy wrapper for functions. For a user of a protocol, multimethod, or function it makes no difference how it is implemented - it looks exactly the same. This is a big win for extending or refactoring code without breaking things. For example, say we only had a complex type complex-r and implemented the methods real, imag, magnitude, and angle as functions via defn. But then later we required the complex-p type and it also needed the same methods. Refactoring the existing functions (real, imag, etc) into a protocol will make no difference to code already using these functions with the complex-r type - it looks and acts just like a function. Multimethods are the same. . Type hierarchies can be decoupled from types. In this example we built a hierarchy for the number types using namespaced keywords like ::complex-r. This exists independently from the records we defined. It is coupled to the record types via a one-to-one mapping of the records to a keyword, implemented by the protocol PNumberKind. This decoupling allowed us to create further abstractions such as ::number and ::complex. In Java you’d have to create abstract base classes retroactively and subclass the existing types, which would be a redesign. This decoupling was also useful later for the numeric tower where we actually required a completely different hierarchy of the types. This was made possible in Clojure because you can create multiple hierarchies ad-hoc - hierarchies are just data. You could even implement multimethods with different type hierarchies this way. In OOP this kind of polymorphic dispatch is strongly coupled to your class hierarchies, which you can’t just change. . Source Code . The complete source code of this tutorial can be found here: jimypbr/clojure-numbers. . References and Further Reading . SICP section 2.4 | Watch ‘Simple Made Easy’ by Rich Hickey. It’s a great talk that explains how Clojure aims to decouple key programming concepts from each other: Simple Made Easy, Rich Hickey | For more about multiple dispatch, Eli Bendersky’s series of blog posts are great: http://eli.thegreenplace.net/2016/a-polyglots-guide-to-multiple-dispatch | This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/23/polyclojure3.html",
            "relUrl": "/clojure/functional-programming/2016/11/23/polyclojure3.html",
            "date": " • Nov 23, 2016"
        }
        
    
  
    
        ,"post8": {
            "title": "How to do Polymorphism in Clojure (2/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . More Number Types . Let’s extend our number package with more number types: An Integer type, a Float type for real numbers, and a Rational type for fractions. Like before we create the records and constructors: . (defrecord Int [n]) (defrecord Rational [n d]) (defrecord Float [n]) ;; constructors (defn integer &quot;Make a new integer&quot; [n] {:pre [(number? n)]} (-&gt;Int (int n))) (defn float &quot;Make a new float&quot; [n] {:pre [(number? n)]} (-&gt;Float (double n))) (defn rational &quot;Make a new rational&quot; [n d] {:pre [(number? n) (number? d)]} (let [g (gcd n d)] (-&gt;Rational (/ n g) (/ d g)))) . Trying these out in the REPL: . =&gt; (float 3) #numbers.core.Float{:n 3.0} =&gt; (integer 6) #numbers.core.Int{:n 4} =&gt; (rational 6 3) #numbers.core.Rational{:n 2, :d 1} . Generic Arithmetic . We want to create an add function that can be called with either two integers, two rationals, two reals, or two complex types and do the right thing in every case. Protocols in Clojure allow for single dispatch only. Here we need to dispatch on the types of multiple arguments. . We could hack this with a mega-function that is just a big conditional statement: . (def mega-add &quot;one add to rule them all (don&#39;t do this)&quot; [n1 n2] (cond (and (= (type n1) Int) (= (type n2) Int)) (add-int n1 n2) (and (= (type n1) Float) (= (type n2) Float) (add-real n1 n2) ;; etc :else (throw &quot;unknown types&quot;))) . The problem with this solution is that it is closed for business. If a user of the our number library desired to extend the number system with a new type of number (e.g. a BigInt), they’d have to break in and edit this function directly. . Multi-Methods . Clojure’s core library provides multiple dispatch via multi-methods. While protocols in Clojure perform single-dispatch on just the type of the first argument, multi-methods are much more general and allow the programmer to define their own rules for dispatch using any number of arguments. You are not limited to dispatch with just the types of the arguments, but also their values. . Let’s throw out mega-add and do it properly with multi-methods. The multi-method is defined using the defmulti macro. It takes a docstring and a dispatch function as its arguments. For adding, the dispatch function will be mapped to the two numbers as arguments and so return a vector of the types of the arguments: . (defmulti add &quot;Generic add&quot; class) . So if we provided two Ints then the dispatch would return [Int Int]. With the dispatch machinery is in place, we now need to add the implementations for each of the types. This is done with defmethod, which defines a method for each valid output of the dispatch function: . (defmethod add [Int Int] &quot;Add two integers together&quot; [n1 n2] (integer (+ (:n n1) (:n n2)))) (defmethod add [Float Float] &quot;Add two floats together&quot; [n1 n2] (float (+ (:n n1) (:n n2)))) ;; etc . Trying this out in the repl: . =&gt; (add (integer 4) (integer 6)) #numbers.core.Int{:n 10} =&gt; (add (float 5) (float 10)) #numbers.core.Float{:n 15.0} . Neat! Multi-methods are easy to extend too. If I wanted to create a new number type (e.g. a BigInt), then all I need to do is add a new method with defmethod for the case of [BigInt BigInt]. . Similarly we can reimplement the add function defined previously for the two complex number types, using the new multi-method machinery: . (defmethod add [Complex-r Complex-r] &quot;Add two complex-r numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) (defmethod add [Complex-p Complex-p] &quot;Add two complex-p numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) . This works fine, but previously add for the two different complex number types was a single function, whereas now we have repetition. Moreover we can’t add a Complex-r to a Complex-p like we could before. . Multimethods have provided a lot of extensibility to new number types, but at the same time we have lost the polymorphic nature we had in the arithmetic functions of the two complex types. I will address this problem in the next section. . Keywords and Dispatch Hierarchies . We have an impression that Complex-r and Complex-p are subtypes of some imaginary abstract type Complex. However Clojure does not offer any notion of an ‘abstract type’ as we are used to in OOP. Instead Clojure provides an ad-hoc dynamic hierarchy system. The hierarchy system supports derivation relationships between names (either symbols or keywords), and relationships between classes and names. . The derive function creates these relationships, and the isa? function tests for their existence. We will use namespaced keywords (double colon) to represent the number types: . =&gt; (derive ::complex-r ::complex) =&gt; (derive ::complex-p ::complex) =&gt; (isa? ::complex-r ::complex) true =&gt; (isa? ::complex-p ::complex) true =&gt; (isa? ::complex-r ::complex-p) false =&gt; (ancestors ::complex-r) #{::complex} =&gt; (ancestors ::complex-p) #{::complex} . What we want to do is rewrite the arithmetic multi-methods to dispatch using these namespaced keywords in place of the number types. The complex add method could then be reduced to matching arguments that satisfy: [::complex ::complex]. To do this, we will require a one-to-one mapping of each type to its associated keyword: . Complex-r =&gt; ::complex-r | Complex-p =&gt; ::complex-p | Float =&gt; ::float | Rational =&gt; ::rational | Int =&gt; ::integer | . We could do this with a global lookup table or add the keywords to the record definitions, but these are cludgy solutions. The first requires maintaining some global data, and the second repeats information and forces us to rewrite the record definition, which would break existing code. A cleaner solution is just to create another protocol and extend our number types with it: . (defprotocol PNumberKind (kind [n] &quot;The keyword name for the kind of the number n&quot;)) (extend-protocol PNumberKind Complex-r (kind [z] ::complex-r) Complex-p (kind [z] ::complex-p) Float (kind [z] ::float) Rational (kind [z] ::rational) Int (kind [z] ::integer)) . In the REPL: . =&gt; (kind (integer 3)) :numbers.core/integer =&gt; (kind (complex-r 4 5)) :numbers.core/complex-r . We can now update the dispatch function used by the multimethod to dispatch using kind: . (defmulti add &quot;Generic add&quot; kind) . The methods can now be rewritten as: . (defmethod add [::integer ::integer] &quot;Add two integers together&quot; [n1 n2] (integer (+ (:n n1) (:n n2)))) (defmethod add [::float ::float] &quot;Add two reals together&quot; [n1 n2] (real (+ (:n n1) (:n n2)))) (defmethod add [::complex ::complex] &quot;Add two complex-p numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) . Since we added the rule (derive ::complex-r ::complex) to the hierarchy, the multimethod called with ::complex-r or ::complex-p implicitly satisfy the rule [::complex ::complex]. The hierarchy has therefore allowed us the implement a polymorphic add for adding different representations of complex numbers and their combinations. If we added more complex number representations, the generic add method for complex numbers would accomodate them automatically without modification. . Let’s try this in the REPL: . =&gt; (add (complex-r 2 3) (complex-r 6 7)) #numbers.core.Complex-r{:real 8, :imag 10} =&gt; (add (complex-p 4 5) (complex-p 1 0)) #numbers.core.Complex-r{:real 2.1346487418529048, :imag -3.835697098652554} =&gt; (add (complex-p 3 4) (complex-r 5 6)) #numbers.core.Complex-r{:real 3.039069137409164, :imag 3.7295925140762156} . This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/22/polyclojure2.html",
            "relUrl": "/clojure/functional-programming/2016/11/22/polyclojure2.html",
            "date": " • Nov 22, 2016"
        }
        
    
  
    
        ,"post9": {
            "title": "How to do Polymorphism in Clojure (1/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . Introduction . Through the years of use of conventional programming languages such as Python and C++, my thinking about programming was trapped within the Object Oriented model of these languages. When it came to solving a new problem my thinking would be dictated by how those languages wanted me to think and I was blind to any better way. It wasn’t until I came across the 1980s cult-classic “Structure and Interpretation of Computer Programs” (SICP) and the programming language Clojure that I started to see past thought models I had learned. One of the parts that was a real ‘Ah-ha!’ moment for me was the final section of Chapter 2 on “Data-directed programming”. This chapter was the clearest explanation of the problem of Polymorphism that I’d ever read… . The concern of this section is in implementing a system for calculating with different types of numbers (e.g. complex, integers, rationals, etc). Variants of this example are commonly used beginners books to Java and C++ in order to demonstrated how Object Oriented programming (OOP) can model data and abstractions. However SICP is not a language manual and doesn’t concern itself with showing how to make some feature of a language do what you want. It instead focuses on the problem of trying to model numbers itself, and then creates the necessary structures to solve this problem. SICP uses this problem to motivate the actual purpose of OOP and it then creates a basic implementation of Objects in scheme. The book then goes beyond OOP and implements multiple dispatch from scratch entirely in Scheme. . I found this to be such an enlightening exercise that I started thinking about how well other languages could solve this problem. In this blog post I want to implement the Numbers program in Clojure, a language that provides multiple dispatch. I think this is a non-trivial example that demonstrates every feature for polymorphism in Clojure’s core libraries. . Complex Numbers . A complex number, $z$ is a number expressed in the form $z=u+iv$, where $u$ and $v$ are real numbers and $i^2=-1$. $u$ is called the real part and $v$ is called the imaginary part. . We can represent a complex number as a pair $(u, v)$, called the rectangular form. . | An alternative representation is the polar form where the complex number is represented by the pair $(r, phi)$, where $r$ is the magntitude and $ phi$ is the angle. . | . Rectangular and polar forms are related via the following formulae: . [ begin{eqnarray} u &amp;=&amp; r cos phi v &amp;=&amp; r sin phi r &amp;=&amp; sqrt{u^2 + v^2} phi &amp;=&amp; tan^{-1}(v/u) end{eqnarray}] . In our Complex number package we want to support the following arithmetic operations on pairs of complex numbers: add, sub, mult, and div. . When adding or subtracting complex numbers it is natural to work with their rectangular coordinates: . [ begin{eqnarray} Re(z_1 + z_2) &amp;=&amp; Re(z_1) + Re(z_2) Im(z_1 + z_2) &amp;=&amp; Im(z_1) + Im(z_2) end{eqnarray}] . While when multiplying and dividing complex numbers it is more natural to work with the polar coordinates: . [ begin{eqnarray} Magnitude(z_1 cdot z_2) &amp;=&amp; Magnitude(z_1) cdot Magnitude(z_2) Angle(z_1 cdot z_2) &amp;=&amp; Angle(z_1) + Angle(z_2) end{eqnarray}] . The product is the vector obtained by stretching the length of $z_1$ by the length of $z_2$, and rotating the angle of $z_1$ by the angle of $z_2$. . So there are two different representations which are suitable for different operations. However we want to be able to do all the arithmetic operations on complex numbers regardless of which representation is used. . Rectangular Representation . How can we model this number pair using the tools in Clojure? Clojure allows us to create an object called a Record. A Record is basically a map with a name, a minimum set of keys that it is guaranteed to have, and a constructor. Here’s how we could create a rectangular complex number with records: . (defrecord Complex-r [real imag]) . You can create some Complex-r’s: . =&gt; (-&gt;Complex-r 2 3) #user.Complex-r{:real 2, :imag 3} =&gt; (-&gt;Complex-r -1 16) #user.Complex-r{:real -1, :imag 16} . With records it is good practice to create your own constructor to give you freedom to add post- and pre-conditions when a new record is created. This is just a wrapper function: . (defn complex-r &quot;create a new Complex-r&quot; [re im] {:pre [(number? re) (number? im)]} (-&gt;Complex-r re im)) . I provided some preconditions that assert that the parameters are a type of Clojure’s native number. We can access the real and imaginary parts of a Complex-r: . =&gt; (:real (complex-r 4 5)) 4 =&gt; (:imag (complex-r 4 5)) 5 . However, getting the real and imaginary parts of a complex number using the keywords seems to expose the implementation of Complex-r too much. Better practice would be to wrap those in some functions: . (defn real-r &quot;Get the real part of a complex-r number.&quot; [z] (:real z)) (defn imag-r &quot;Get the imaginary part of a complex-r number.&quot; [z] (:imag z)) . So now we have: . =&gt; (real-r (complex-r 4 5)) 4 =&gt; (imag-r (complex-r 4 5)) 5 . We can also view the magnitude and angle of a rectangular complex number using the formulae above: . (defn magnitude-r &quot;Magnitude of a complex-r number&quot; [z] (Math/sqrt (+ (square (real-r z)) (square (imag-r z))))) (defn angle-r &quot;Angle of a complex-r number&quot; [z] (Math/atan (/ (imag-r z) (real-r z)))) . In the REPL: . =&gt; (magnitude-r (complex-r 3 4)) 5.0 =&gt; (angle-r (complex-r 3 4)) 0.9272952180016121 . Polar Representation . Similarly, we can implement the Polar form of complex numbers as another record: . (defrecord Complex-p [magnitude angle]) (defn complex-p &quot;create a new Complex-p&quot; [magnitude angle] {:pre [(number? magnitude) (number? angle)]} (-&gt;Complex-p magnitude angle)) . Again for the Polar representation we need to write some functions that will give us real and imaginary parts, and the magnitude and angle of a polar number. . (defn real-p &quot;real part of a complex-p number&quot; [z] (* (:magnitude z) (Math/sin (:angle z)))) (defn imag-p &quot;imaginary part of a complex-p number&quot; [z] (* (:magnitude z) (Math/cos (:angle z)))) (defn magnitude-p &quot;magnitude of a complex-p number&quot; [z] (:magnitude z)) (defn angle-p &quot;angle of a complex-p number&quot; [z] (:angle z)) . In the REPL: . =&gt; (angle-p (complex-p 5 0.5)) 0.5 =&gt; (magnitude-p (complex-p 5 0.5)) 5 =&gt; (real-p (complex-p 5 0.5)) 4.387912809451864 =&gt; (imag-p (complex-p 5 0.5)) 2.397127693021015 . Single Dispatch with Protocols . At this point we have two different types of complex number representations and two sets of functions that are specialised to handle each type. This is obviously bad because a user of this numbers module has to pay attention at all times to whether they are using Complex-r or Complex-p types. They need to specialise whatever code they write with them. . Rather than having real-r and real-p functions we instead want to have a single function real that takes any type of complex number and performs dispatch at runtime based on the type of the argument it has received. I.e. dispatch based on the type of the first argument passed to the function. Dispatch based on a single argument is called single dispatch. . Clojure allows single dispatch through Protocols. A protocol is a named set of functions and their signatures, with no implementions. The functions dispatch on the type of their first argument, and thus must have at least one argument. Implementations of the protocol functions have to be written for each type implementing the protocol. They are very similar to Java interfaces, but with one important improvement: which protocols are implemented for a type is not a design time decesion by the code author, rather protocols can extend any type whenever and wherever you want. . We create a protocol for complex numbers using defprotocol: . (defprotocol PComplex (real [z] &quot;Real part of a complex number&quot;) (imag [z] &quot;Imaginary part of a complex number&quot;) (magnitude [z] &quot;Magnitude of a complex number&quot;) (angle [z] &quot;Angle of a complex number&quot;)) . Implement the PComplex protocol for each of our types: . (extend-protocol PComplex ;; implementation of methods for Complex-r type Complex-r (real [z] (:real z)) (imag [z] (:imag z)) (magnitude [z] (Math/sqrt (+ (square (real z)) (square (imag z)))) (angle [z] (Math/atan (/ (imag z) (real z)))) ;; implemention of methods for Complex-p type Complex-p (real [z] (* (:magnitude z) (Math/sin (:angle z)))) (imag [z] (* (:magnitude z) (Math/cos (:angle z)))) (magnitude [z] (:magnitude z)) (angle [z] (:angle z))) . Trying this out in a REPL: . =&gt; (def z1 (complex-r 5 6)) =&gt; (def z2 (complex-p 3 1)) =&gt; (real z1) 5 =&gt; (real z2) 1.6209069176044193 =&gt; (magnitude z1) 7.810249675906654 =&gt; (magnitude z2) 5 . With protocol we now have a generic set of functions for dealing with any type of complex number. If we created a new type of complex number then we’d simple make it implement the PComplex protocol. . Arithmetic With Complex Numbers . The PComplex protocol allows us to write code that works with complex numbers and does not need to worry whether whether they are rectangular or polar. We can now write single implementations the arithmetic functions add, sub, mult, and div using the formulas above. . (defn add &quot;Add two complex numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) (defn sub &quot;Subtract two complex numbers from each other&quot; [z1 z2] (complex-r (- (real z1) (real z2)) (- (imag z1) (imag z2)))) (defn mult &quot;Multiply two complex numbers together&quot; [z1 z2] (complex-p (* (magnitude z1) (magnitude z2)) (+ (angle z1) (angle z2)))) (defn div &quot;Divide two complex numbers by each other&quot; [z1 z2] (complex-p (/ (magnitude z1) (magnitude z2)) (- (angle z1) (angle z2)))) . Since the functions from PComplex work transparently for both representations of complex numbers we only need to write one function for add and it works not only for both types, but also all combinations of them for free! . Let’s try it all out in the REPL: . =&gt; (add (complex-r 5 5) (complex-r 6 6)) #numbers.core.Complex-r{:real 11, :imag 11} =&gt; (mul (complex-r 5 5) (complex-r 5 5)) #numbers.core.Complex-p{:magnitude 50.00000000000001, :angle 1.5707963267948966} =&gt; (add (complex-p 5 0.1) (complex-p 10 0)) #numbers.core.Complex-r{:real 14.975020826390129, :imag 0.4991670832341408} =&gt; (mul (complex-p 5 0.1) (complex-p 10 0)) #numbers.core.Complex-p{:magnitude 50, :angle 0.1} =&gt; (add (complex-r 5 5) (complex-p 10 0)) #numbers.core.Complex-r{:real 15.0, :imag 5.0} =&gt; (mul (complex-r 5 5) (complex-p 10 0)) #numbers.core.Complex-p{:magnitude 70.71067811865476, :angle 0.7853981633974483} . Without the polymorphism obtained from the protocol we would have to write 16 separate functions instead of just these 4. Moreover, if we wanted to create more complex number representations there would be a combinatorial explosion in the number of arithmetic functions we’d need to write. . This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/21/polyclojure1.html",
            "relUrl": "/clojure/functional-programming/2016/11/21/polyclojure1.html",
            "date": " • Nov 21, 2016"
        }
        
    
  
    
        ,"post10": {
            "title": "Experiments with Julia",
            "content": "SIMD Support . Since version 0.3 Julia has some vectorisation capabilities that can exploit SIMD instructions when executing loops. It seems to require some nudging though. There are macros that are a bit like the pragmas in OpenMP. . Example 1. SAXPY . function saxpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) for i = 1:n y[i] += a*x[i] end end . Sadly, in version 0.3.10 this obvious candidate does not auto-vectorise. You can inspect how this code has compiled nicely in Julia by using the macros @code_llvm to see the LLVM IR or @code_native to see the ASM. The ASM produced is: . Source line: 48 mov R8, QWORD PTR [RSI + 16] xor R11D, R11D xor ECX, ECX cmp RCX, R8 jae 65 cmp RCX, QWORD PTR [RDI + 16] jae 55 lea R10, QWORD PTR [4*R11] mov RAX, QWORD PTR [RSI + 8] sub RAX, R10 mov RDX, QWORD PTR [RDI + 8] sub RDX, R10 movss XMM1, DWORD PTR [RDX] mulss XMM1, XMM0 addss XMM1, DWORD PTR [RAX] movss DWORD PTR [RAX], XMM1 dec R11 inc RCX cmp R9, RCX jne -72 pop RBP ret movabs RAX, 140636405232096 mov RDI, QWORD PTR [RAX] movabs RAX, 140636390845696 mov ESI, 48 call RAX . Note the scalar instructions movss, mulss, and addss. . Example 2: SAXPY + SIMD . To make it generate vectorised instructions you have to use the explicit vectorisation macros @simd and @inbounds macros. @simd gives the compiler license to vectorise without checking the legality of the transformation. @inbounds is an optimisation that turns off subscript checking, because subscript checking might throw an exception and so isn’t vectorisable. . function axpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) @simd for i = 1:n @inbounds y[i] += a*x[i] end end . This now compiles with SIMD instructions: . ... Source line: 48 mov R8, QWORD PTR [RDI + 8] mov R9, QWORD PTR [RSI + 8] xor EDI, EDI mov RSI, RAX and RSI, -8 je 79 pshufd XMM1, XMM0, 0 # xmm1 = xmm0[0,0,0,0] xor EDI, EDI lea RCX, QWORD PTR [4*RDI] mov RDX, R8 sub RDX, RCX movups XMM3, XMMWORD PTR [RDX] movups XMM2, XMMWORD PTR [RDX + 16] mulps XMM3, XMM1 mov RDX, R9 sub RDX, RCX movups XMM5, XMMWORD PTR [RDX] movups XMM4, XMMWORD PTR [RDX + 16] addps XMM5, XMM3 movups XMMWORD PTR [RDX], XMM5 mulps XMM2, XMM1 addps XMM2, XMM4 movups XMMWORD PTR [RDX + 16], XMM2 add RDI, -8 mov RCX, RSI add RCX, RDI jne -69 mov RDI, RSI sub RAX, RDI je 41 ... . Note the instructions movups, addps, mulps… (English: move, unaligned, packed, single-precision). Packed =&gt; Vector. . We can also see from the LLVM IR: . define void @julia_axpy_21664(float, %jl_value_t*, %jl_value_t*) { ... br label %vector.body vector.body: ; preds = %vector.body, %vector.ph %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ] %25 = getelementptr float* %20, i64 %index, !dbg !4955 %26 = bitcast float* %25 to &lt;4 x float&gt;* %wide.load = load &lt;4 x float&gt;* %26, align 4 %.sum18 = or i64 %index, 4 %27 = getelementptr float* %20, i64 %.sum18 %28 = bitcast float* %27 to &lt;4 x float&gt;* %wide.load9 = load &lt;4 x float&gt;* %28, align 4 %29 = getelementptr float* %24, i64 %index, !dbg !4955 %30 = bitcast float* %29 to &lt;4 x float&gt;* %wide.load10 = load &lt;4 x float&gt;* %30, align 4 %31 = getelementptr float* %24, i64 %.sum18 %32 = bitcast float* %31 to &lt;4 x float&gt;* %wide.load11 = load &lt;4 x float&gt;* %32, align 4 %33 = fmul &lt;4 x float&gt; %wide.load10, %broadcast.splat13 %34 = fmul &lt;4 x float&gt; %wide.load11, %broadcast.splat13 %35 = fadd &lt;4 x float&gt; %wide.load, %33 %36 = fadd &lt;4 x float&gt; %wide.load9, %34 store &lt;4 x float&gt; %35, &lt;4 x float&gt;* %26, align 4 store &lt;4 x float&gt; %36, &lt;4 x float&gt;* %28, align 4 %index.next = add i64 %index, 8 %37 = icmp eq i64 %index.next, %n.vec br i1 %37, label %middle.block, label %vector.body middle.block: ; preds = %vector.body, %if %resume.val = phi i64 [ 0, %if ], [ %n.vec, %vector.body ] %cmp.n = icmp eq i64 %15, %resume.val br i1 %cmp.n, label %L7, label %L ... . Timing of a function is easy to do: . x = rand(Float32, 10000000) y = rand(Float32, 10000000) a = float32(0.1) @time axpy(a,x,y) . However, you probably want to run this more than once since the first time you call a function, Julia JITs it so the timing won’t be representative. Here are the timings of axpy with and without @simd with length(x) == 10000000: . @simd? Time (s) . yes | 0.006527373 | . no | 0.013172804 | . Setup: Hardware: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz (AVX) Julia version: 0.3.11 . Example 3: SAXPY + Implicit vectorisation . In my experiments, I found that it is sometimes possible to get implicit vectorisation by using just @inbounds to disable bounds checking: . function axpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) for i = 1:n @inbounds y[i] += a*x[i] end end . This generates the same ASM as Example 2. Other simple cases have required @simd as well, so explicit vectorisation seems to be the only option at the moment. . SIMD Limitations in Julia . No SIMD functions. Function calls have to be inlined. Julia can manage to inline short functions itself. | Code must be type-stable. This means that there isn’t implicit type conversion in the loop. This will prevent vectorisation and probably make it run slow serially too. | SIMD macro doesn’t have the bells and whistles of the OpenMP SIMD pragma. | Doesn’t appear to be any way to specify alignment of memory. | No outer loop vectorisation. | I can’t find any diagnostic information about how things were optimised. | Does handle any branching besides some ifelse function. | . More Info on Vectorisation . See Intel page Vectorization in Julia. . Case Study: Fitzhugh-Nagamo PDE . Experiment with a non-trivial example of a PDE solver using the reaction-diffusion system described by: . [ begin{eqnarray} frac{ partial u}{ partial t} &amp;=&amp; a nabla^2 u + u - u^3 - v - k tau frac{ partial v}{ partial t} &amp;=&amp; b nabla^2 v + u - v end{eqnarray}] . With Neumann boundary conditions, boundary of [1,1]^2, N=100^2, and T=[0.,10.]. This example is based on this online python example. I implement this in C++ and Julia and compare the performance and the vectorisation. $y=x^2$. . Here is the code in Julia: . Click to expand code… function fitzhugh_nagumo(size, T) # Define the constants const aa = 2.8e-4 const bb = 5.0e-3 const τ = 0.1 const κ = -0.005 const dx = 2.0 / size const dt = 0.9 * dx^2 / 2.0 const invdx2 = 1.0 / dx^2 const dt_τ = dt / τ # Random initial fields u_old = rand(Float64, (size,size)) v_old = rand(Float64, (size,size)) u_new = Array(Float64, (size,size)) v_new = Array(Float64, (size,size)) for t = 0.0 : dt : T for j = 2:size-1 for i = 2:size-1 Δu = invdx2 * (u_old[i+1,j] + u_old[i-1,j] + u_old[i,j+1] + u_old[i,j-1] - 4*u_old[i,j]) Δv = invdx2 * (v_old[i+1,j] + v_old[i-1,j] + v_old[i,j+1] + v_old[i,j-1] - 4*v_old[i,j]) u_new[i,j] = u_old[i,j] + dt * (aa*Δu + u_old[i,j] - u_old[i,j]*u_old[i,j]*u_old[i,j] - v_old[i,j] + κ) v_new[i,j] = v_old[i,j] + dt_τ * (bb*Δv + u_old[i,j] - v_old[i,j]) end end for i = 1:size u_new[i,1] = u_new[i,2] u_new[i,size] = u_new[i,size-1] v_new[i,1] = v_new[i,2] v_new[i,size] = v_new[i,size-1] end for j = 1:size u_new[1,j] = u_new[2,j] u_new[size,j] = u_new[size-1,j] v_new[1,j] = v_new[2,j] v_new[size,j] = v_new[size-1,j] end # swap new and old u_new, u_old = u_old, u_new v_new, v_old = v_old, v_new end return (u_old, v_old) end . Here is the code in C++. . Click to expand code… #include &lt;iostream&gt; #include &lt;random&gt; #include &lt;algorithm&gt; // swap #define N 100 std::random_device rd; std::mt19937 mt(rd()); double fitzhugh_nagumo(double T) { // Define the constants const double aa = 2.8e-4; const double bb = 5.0e-3; const double tau = 0.1; const double kappa = -0.005; const double dx = 2.0 / N; const double dt = 0.9 * dx*dx / 2.0; const double invdx2 = 1.0 / (dx*dx); const double dt_tau = dt / tau; // Random initial fields double u_old[N][N]; double v_old[N][N]; double u_new[N][N]; double v_new[N][N]; std::uniform_real_distribution&lt;double&gt; rng(0.,1.); for (int i = 0; i &lt; N; ++i) { for (int j = 0; j &lt; N; ++j) { u_old[i][j] = rng(mt); v_old[i][j] = rng(mt); } } // Solver int Nt = (int) (T / dt); std::cout &lt;&lt; &quot;Nt = &quot; &lt;&lt; Nt &lt;&lt; std::endl; for (int t = 0; t &lt; Nt; ++t) { // evolve inner coordinates for (int i = 1; i &lt; N-1; ++i) { for (int j = 1; j &lt; N-1; ++j) { double delta_u = invdx2 * (u_old[i+1][j] + u_old[i-1][j] + u_old[i][j+1] + u_old[i][j-1] - 4*u_old[i][j]); double delta_v = invdx2 * (v_old[i+1][j] + v_old[i-1][j] + v_old[i][j+1] + v_old[i][j-1] - 4*v_old[i][j]); u_new[i][j] = u_old[i][j] + dt * (aa*delta_u + u_old[i][j] - u_old[i][j]*u_old[i][j]*u_old[i][j] - v_old[i][j] + kappa); v_new[i]After i[j] = v_old[i][j] + dt_tau * (bb * delta_v + u_old[i][j] - v_old[i][j]); } } // neumann boundary conditions for (int i = 0; i &lt; N; ++i) { u_new[i][0] = u_new[i][1]; u_new[i][N-1] = u_new[i][N-2]; v_new[i][0] = v_new[i][1]; v_new[i][N-1] = v_new[i][N-2]; } for (int j = 0; j &lt; N; ++j) { u_new[0][j] = u_new[1][j]; u_new[N-1][j] = u_new[N-2][j]; v_new[0][j] = v_new[1][j]; v_new[N-1][j] = v_new[N-2][j]; } // Swap old and new std::swap(u_new, u_old); std::swap(v_new, v_old); } return u_old[0][0]; } . Results . Setup: . Hardware: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz (AVX) | Julia version: 0.3.10 | C++ Compiler: g++4.8 | . Language Notes Time (s) . Julia | None | 5.993 | . Julia | @inbounds | 3.105 | . Julia | @inbounds + @simd | 3.0603 | . C++ | -O0 -std=c++11 | 22.790 | . C++ | -Ofast -std=c++11 -fno-tree-vectorize -fno-tree-slp-vectorize | 3.2096 | . C++ | -Ofast -std=c++11 | 2.142 | . The best time in C++ is only 1.4x better than the best time in Julia. Inspecting the ASM of Julia + @inbounds + @simd shows that even with these macros Julia is still not generating vector instructions. :(. If I disable vectorisation in the C++ compiler, the times between Julia and C++ are much closer. This suggests that Julia could get even higher performance if it could generate vector instructions. I suppose that newer versions of Julia will improve this in the future. I find these results very impressive nonetheless. It will be worth trying with a newer LLVM version too. . Notes . The thing holding back performance in the Julia code was the use of u[i,j]^3 instead of u[i,j]*u[i,j]*u[i,j]. The former version compiles to a call of pow(double, double) from libm! The latter does what you expect. This is a known bug. Fixed when Julia is built against LLVM 3.6. Version I’m using is built with LLVM v3.4. .",
            "url": "https://jimypbr.github.io/blog/julia/c++/simd/2015/11/21/julia.html",
            "relUrl": "/julia/c++/simd/2015/11/21/julia.html",
            "date": " • Nov 21, 2015"
        }
        
    
  
    
        ,"post11": {
            "title": "Hello",
            "content": "Typical hello world first blog post… .",
            "url": "https://jimypbr.github.io/blog/2015/11/20/hello.html",
            "relUrl": "/2015/11/20/hello.html",
            "date": " • Nov 20, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jimypbr.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jimypbr.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}