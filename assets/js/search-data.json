{
  
    
        "post0": {
            "title": "Fast.ai v3 Lesson 10 Notes: Looking inside the model",
            "content": "Overview . This lesson covers a lot of material. It starts off with a review of some important foundations such as more advanced Python programming, variance, covariance, and standard deviation. It then goes into a short discussion on situation where Softmax loss is a bad idea in image classification tasks. My notes go deeper into this part on Multilabel classification than the original lecture does. The lesson then moves onto looking inside the model using PyTorch hooks. The last part of the lesson introduces Batch Normalization and studies the pros and cons of BatchNorm and shows some alternatives normalizations that are possible. Jeremy then develops a novel kind of normalization layer to overcome BatchNorm’s main problem, and compares it to previously published approaches, with some very encouraging results. . Lesson 10 lesson video. . Foundations . Notebook: 05a_foundations.ipynb . Mean: m = t.mean() . Variance: The average of how far away each data point is from the mean. Mean squared difference from the mean. Sensitive to outliers. . var = (t-m).pow(2).mean() | Better: var = (t*t).mean() - (m*m) | $ mbox{E}[X^2] - mbox{E}[X]^2$ | . Standard Deviation: Square root of the variance. . On same scale as the original data - easier to interpret. . | std = var.sqrt() . | . Mean Absolute Deviation: Mean absolute difference from the mean. It isn’t used nearly as much as it deserves to be. Less sensitive to outliers than variance. . (t-m).abs().mean() | . Covariance: A measure of how changes in one variable are associated with changes in a second variable. How linearly associated are two variables? . cov = ((t - t.mean()) * (v - v.mean())).mean() | $ operatorname{cov}(X,Y) = operatorname{E}{ big[(X - operatorname{E}[X])(Y - operatorname{E}[Y]) big]} = operatorname{E}[XY] - operatorname{E}[X] operatorname{E}[Y]$ | cov = (t*v).mean() - t.mean()*v.mean() | . Correlation: The strength and direction of the linear relationship between two variables. . Covariance divided by the standard deviations of X and Y. | $ rho_{X,Y}= frac{ operatorname{cov}(X,Y)}{ sigma_X sigma_Y}$ | cor = cov / (t.std() * v.std()) | . See this: 3 minute video on Correlation vs Covariance. . MultiLabel Classification (When Softmax is a Bad Idea) . Jump_to lesson 10 video . A common mistake many people make is using a Softmax where it isn’t appropriate. Recall the Softmax formula: . [p(z_i) = hbox{softmax(z)}{i} = frac{e^{z{i}}}{ sum_{0 leq j leq n-1} e^{z_{j}}}] . In the Excel screenshot below, two different network outputs can produce the same Softmax output. This is weird, how does it happen? . . The sums of the exponentials for the two images (12.70 and 3.00) are dividing each of the exponentials and it happens that they come out with the exact same proportions for each category for both images. . In Image 1 there is a large activation for category “fish” (2.07), but in image 2 the activation for “fish” is only 0.63. Image 1 likely contains a fish, but it’s possible that image 2 doesn’t contain any of the categories. Softmax has to pick something however so it takes the weak fish activation and makes it big. It’s also possible that image 1 contains a cat, fish, and a building. . Put another way: many images are in fact multilabel, so Softmax is often a dumb idea, unless every one of your items has definitely at least one example of the thing you care about in it, and no items that have multiple examples in it. If an image doesn’t even have cat, dog, plane, fish, or building in it, it still has to pick something! Even if it has more than just one of the categories in it, it will have to pick one of them. . (N.B multiclass means one valid label per image, while multilabel means multiple labels per image. I always confuse these. Read this for a refresher.) . What do you do if there could there could be no things, or there could be more than one of these things? Instead you use a binary function where the output for each category in is: . [B(z_i) = frac{e^{z_i}}{1+e^{z_i}}] . This treats every category independently. The network assigns each category with a probability between 0 and 1, corresponding to how likely it thinks the category is present in the input data. (Note: the binary function is AKA the sigmoid function or logistic function). . The output of a binary function with the same example would look like: . . See how each category gets its own probability and is independent from all the others. . For image recognition, probably most of the time you don’t want Softmax. This habit comes from the fact that we all grew up with the luxury of ImageNet where the images are curated so that there is only one of each class in the image. . What if you instead added an additional category like “null”, “doesn’t exist”, “background”? This has been tried by researchers, but they found that it doesn’t work. The reason it doesn’t work is that we’d have to look for some set of features that correspond to “not cat/dog/plane/fish/building”. However this class of all things that are not something isn’t a kind of object so there isn’t some vector that can represent this. . Creating a binary has/has-not for each class is much easier for the network to learn. According to Jeremy: lots of well regarded papers make this mistake, so look out for it. If you suspect something does this, try replicating it without Softmax and you may just get a better result. . Example where Softmax is a good idea: language modelling -&gt; predict the next word. There can be only one word next. . MultiLabel Predictions . Now that we understand the concept, what would this look like in code and how would we modify the loss function with the binary output layer? . Let’s first reproduce what Jeremy did in the Excel sheet in Python: . . Where the logistic function is what Jeremy calls ‘binary’ in his lecture. . How do we interpret the outputs of softmax and logistic to get predictions? For Softmax layer the predicted label is the label with the highest output value. In code this is simply: . . For the logistic output we need to threshold the values to filter in only the largest outputs. This threshold is user defined; 0.2 is used in fastai lesson 3 so let’s just go with that. Code: . . MultiLabel Loss Function . What about the loss function for a logistic output? Recall from the last lesson that Softmax outputs a categorical probability distribution. With the numbers from the example above this is: . . All the probabilities in a categorical distribution sum to 1 (I denote this property with the blue colour). Recall also from last lesson that the loss function used for a categorical distribution is the cross-entropy. . On the other hand, when we use the Binary/Logistic function the output isn’t a categorical distribution: . . The probabilities in this vector don’t all sum to 1 (denoted with red) because they are all independent of each other. These probabilities are each the probability that the label is present in the data, independent of all the other labels. If we take 1 minus these probabilities we’d get the probability of the label not being present in the data. We can think of each of these as a 2-state system of present / not present and expand the vector out to include the not present probability: . . Now we can see that each of the rows is itself a categorical distribution with two categories (AKA Bernoulli distribution). Therefore to get the loss we can individually apply the cross-entropy loss to each of these distributions using target data (binary vector of present / not present for each label), then take the average of them all. You do that for every sample in the batch and then take the averages of all those averages to get the loss for the batch. . We don’t have to literally expand the vector out in practice, and can instead create a special case of the cross-entropy for this binary case, binary_cross_entropy: . def binary_cross_entropy(pred, targ): return -targ * pred.log() - (1 - targ) * (1 - pred).log() . The loss would be: . def multilabel_loss(out, targ): return binary_cross_entropy(logistic(out), targ).mean(1).mean(0) . Example use: . &gt;&gt;&gt;out = torch.tensor([[0.02, -2.49, -1.75, 2.07, 1.25], [-1.42, -3.93, -3.19, 0.63, -0.19]]) &gt;&gt;&gt;targ = torch.tensor([[0., 0., 0., 1., 1.], [1., 0., 0., 1., 1.]]) &gt;&gt;&gt;multilabel_loss(out, targ) tensor(0.4230) . This is a naive implementation of the loss, but it shows how it works. For a production implementation we need it to be more numerically stable (as discussed in last lesson) and do it all in log-space. We put the logistic function in log-space and then simplify things by fusing that with binary_cross_entropy. You can derive that the binary cross entropy with logistic function simplifies to: . [l(x, y) = -yx + log(1 + e^x)] . Careful with the $e^x$, however, because it will overflow when $x$ isn’t even that large. To make things more numerically stable we employ the logsumexp trick again: . [l(x, y) = m - yx + log(e^{-m} + e^{x - m})] . Where $m = max(x, 0)$. As code, this is: . def binary_cross_entropy_with_logits(out, targ): max_val = out.clamp_min(0.) return max_val - out * targ + ((-max_val).exp() + (out - max_val).exp()).log() . The loss function is modified to: . def multilabel_loss(out, targ): return binary_cross_entropy_with_logits(out, targ).mean(1).mean(0) . We’ve now recreated the loss function BCEWithLogitsLoss from PyTorch, which we can now use. Test with same example: . &gt;&gt;&gt;out = torch.tensor([[0.02, -2.49, -1.75, 2.07, 1.25], [-1.42, -3.93, -3.19, 0.63, -0.19]]) &gt;&gt;&gt;targ = torch.tensor([[0., 0., 0., 1., 1.], [1., 0., 0., 1., 1.]]) &gt;&gt;&gt;loss = torch.nn.BCEWithLogitsLoss() &gt;&gt;&gt;loss(out, targ) tensor(0.4230) . (Implementation in PyTorch (C++): binary_cross_entropy_with_logits) . Build a Learning Rate Finder . Notebook: 05b_early_stopping.ipynb, Jump to lesson 10 video . Better Callback Cancellation . In the last implementation of the Callback and Runner classes, stopping the training prematurely (e.g. for early stopping) was handled by callbacks returning booleans or by a attribute called stop getting set and checked at some point. This is a bit inflexible and also not very readable. . We can instead make use of Exceptions as a kind of control flow technique rather than just an error handling technique. You can subclass Exception to give it your own informative name without even changing its behaviour, like so: . class CancelTrainException(Exception): pass class CancelEpochException(Exception): pass class CancelBatchException(Exception): pass . Callbacks are free to raise Exceptions. The training loop can catch these and change control. This is a super neat and readable way that someone writing a callback can stop any of the three levels in the training loop from happening. . Refactoring Callback and Runner . Refactor/redesign the Callback and Runner class from last time. The Callback class now contains the ‘message passing’ (e.g. self(&#39;begin_fit&#39;) ) logic from before. This means that callback writers can now have control to override __call__ themselves in special cases, for debugging etc. . Here’s what the base class looks like now, alongside the default Training/Validation callback which holds the logic for the training or validating parts of the loop: . class Callback(): _order, run = 0, None def set_runner(self, run): self.run=run def __getattr__(self, k): ## Get attributes from Runner object return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) ## Refactored from before def __call__(self, cb_name): f = getattr(self, cb_name, None) if f and f(): return True return False ## DEFAULT Callback for Training/Validation class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False . Notice how Callback and its subclasses can access attributes in Runner (set in the set_runner method) and even the getattr in Callback is overloaded to instead look in the Runner. . The __getattr__ overloading confused me for a while, until I realised how it actually works. Quote from this Stackoverflow question: . __getattr__ is only called as a last resort i.e. if there are no attributes in the instance that match the name. For instance, if you access foo.bar, then __getattr__ will only be called if foo has no attribute called bar. If the attribute is one you don’t want to handle, raise AttributeError . Python looks for the attribute in the Callback first, if it can’t find it then it looks in the attributes of Runner. . This kind of strong coupling / encapsulation breaking made me a bit nervous initially, but after thinking about it more I think its a special design that works well in this unique setting. Runner and Callback are kind of like ‘friend classes’ from C++, where two friend classes ‘share’ their attributes with each other, but are still separate classes. By doing it this way, callback writers can gain privileged access to internals of the training loop, and so can inject code into the loop as if they were directly editing the source code of Runner. . Here is a skeleton of the code for Runner: . class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.cbs = [TrainEvalCallback()] + cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): try: ## INNER LOOP CODE except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self, dl): try: ## EPOCH CODE except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def fit(self, epochs, learn): self.epochs, self.learn, self.loss = epochs, learn, tensor(0.) try: for cb in self.cbs: cb.set_runner(self) self(&#39;begin_fit&#39;) for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): # TRAIN with torch.no_grad(): if not self(&#39;begin_validate&#39;): # VALIDATE self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.learn = None def __call__(self, cb_name): res = False for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res return res . I removed all the business code from the snippet, to save space and also so it could be implemented as an exercise. . LR_Find Callback . The learning rate finder is the work horse from part 1 of the fastai course. Let’s look at how to implement it and code that up as a callback. . . LR_Find Algorithm Outline: . Define upper and lower bounds for the learning rate and a number of steps. Lower should be small like 1e-10 and the upper should be very layer like 1e+2. Numbers of steps should be something like 100. | Start training the network with a learning rate starting at the lower bound. | After every batch update, exponentially increase the learning rate and record the loss. | If the learning rate hits the upper bound, or the loss ‘explodes’ then stop the process. | After the finder has finished, plot the loss versus learning rate so we can eyeball the best learning rate. | To exponentially increase the learning rate using the formula: . [lr_i = lr_{min} left( frac{lr_{max}}{lr_{min}} right)^{i/i_{max}}] . ‘Exploding’ loss can be defined as some factor (e.g. 10) times the lowest loss value recorded. . The code for the LR_Find callback is: . class LR_Find(Callback): _order=1 def __init__(self, max_iter=100, min_lr=1e-6, max_lr=1): self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr self.best_loss = 1e9 def begin_batch(self): if not self.in_train: return pos = self.n_iter / self.max_iter lr = self.min_lr * (self.max_lr / self.min_lr) ** pos for pg in self.opt.param_groups: pg[&#39;lr&#39;] = lr def after_step(self): if self.n_iter &gt;= self.max_iter or self.loss &gt; self.best_loss*10: raise CancelTrainException() if self.loss &lt; self.best_loss: self.best_loss = self.loss . Plot of loss versus learning rate: . . This PyImageSearch blog post is an excellent resource for learning more about LR Find and also uses exponential smoothing in the loss recordings too. . Build a CNN (with Cuda!) . Notebook: 06_cuda_cnn_hooks_init.ipynb, Jump to lesson 10 video . Let’s build a CNN for doing the MNIST problem using PyTorch and CUDA. Our simple CNN is a sequential model that contains a bunch of stride-2 convolutions, an average pooling, flatten, then a linear layer. . def get_cnn_model(data): return nn.Sequential( Lambda(mnist_resize), # ni,nf,ksize nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), # 8x14x14 nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 16x7x7 nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 32x4x4 nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 32x2x2 nn.AdaptiveAvgPool2d(1), # 32x1 Lambda(flatten), # 32 nn.Linear(32,data.c) # 10 ) . The dimensions of the data as it flows through the model are provided in the comments. AdaptiveAvgPooling downsamples the data using an average. . See: What is adapative average pooling? | Also see: How Convolutions Work: A Mini-Review | . Original data is vectors of 784 so they need to be reshaped to 28x28 to go into the convolution layers. We need to write a function mnist_resize to do this: . def mnist_resize(x): # batchsize, num_channels, height, width return x.view(-1, 1, 28, 28) . In order to turn helper functions into ‘layers’ that we can pass into nn.Sequential, we can create simple wrapper layer class Lambda(nn.Module) that takes this function and calls it in its forward method. This is used in the code above for calling mnist_resize and flatten. . Training this for one epoch on my laptop CPU took 7.14 seconds. . We need to speed this up using the GPU! To get started we need to prepare PyTorch to use the GPU. First check that Cuda is available to use with torch.cuda.is_available(), which should return True. Then set the device in PyTorch: . device = torch.device(&#39;cuda&#39;, 0) # NB assumes only 1 GPU torch.cuda.set_device(device) . To run on the GPU we need to do two things: . Put the model on the GPU, i.e. the model’s parameters. | Put the inputs and the loss function on the GPU, i.e. the things that come out of the dataloaders. | We can implement this with a callback: . class CudaCallback(Callback): def begin_fit(self): self.model.cuda() def begin_batch(self): self.run.xb, self.run.yb = self.xb.cuda(), self.yb.cuda() . At the beginning of the fit, put the model on the GPU. Before each batch starts, put the batch data on the GPU. . Adding this in training for 3 epochs took 7.12 s on my laptop - a nice 3x speedup. :) . Some Refactoring . First we can regroup all the conv/ReLU in a single function because they are always called together. . Next to refactor is the batch resizing for MNIST. This is hardcoded in the model, but we need something more general that could be used on other datasets. Of course this can be implemented as a callback! Make a callback BatchTransformXCallback for doing ‘transformations’ to the data before it goes into the model. Resize is one such possible transformation. . class BatchTransformXCallback(Callback): _order = 2 def __init__(self, tfm): self.tfm = tfm # stores a transform def begin_batch(self): self.run.xb = self.tfm(self.xb) # transform the batch . So we have a resize or view transform to perform for each batch: . def view_tfm(*size): def _inner(x): return x.view(*((-1,)+size)) return _inner mnist_view = view_tfm(1,28,28) cbfs.append(partial(BatchTransformXCallback, mnist_view)) . Discussion on CNN Kernel Sizes . (Jump_to lesson 10 video) . First conv layer on imagenet networks typically have 7x7 or 5x5 size kernels, while the rest of the conv layers use 3x3 kernels. Why is that? . If we just focus on MNIST, the first layer of the MNIST-CNN we only have a single channel image. We need to be mindful of what’s going on when we apply a kernel to this. If we have 8 3x3 filters then for a single point in the image we are converting 9 pixels (from 3x3 kernel) into a vector of 8 numbers (from 8 filters). We aren’t gaining anything from that, it’s basically shuffling the numbers around. For the first conv layer when we just have 1 or 3 channels people use a larger kernel size such as 7x7 or 5x5 in order to capture more information. . 8 3x3 filters 1 channel =&gt; 9 -&gt; 8 | 8 3x3 filters 3 channels =&gt; 27 -&gt; 8 | 8 5x5 filters 1 channel =&gt; 25 -&gt; 8 | 8 5x5 filters 3 channels =&gt; 75 -&gt; 8 | 8 7x7 filters 1 channel =&gt; 49 -&gt; 8 | 8 7X7 filters 3 channels =&gt; 147 -&gt; 8 | . Later conv layers have more ‘channels’ so that isn’t an issue anymore. The deeper layers are typically 3x3. . Here are some useful discussions on this part of the lecture that helped me grok what Jeremy meant here: fastai forum, twitter. . Looking Inside the Model . Jump_to lesson 10 video . We want to look inside of the model while it is training and see how the parameters are changing over time. Are they behaving themselves? Are they actually learning anything? Are there vanishing or exploding gradients? . PyTorch Hooks . Hooks are PyTorch’s version of callbacks, which are called inside of the model, and can be added, or registered, to any nn.Module. Hooks allow you to inject a function into the model that that is executed in either the forward pass (forward hook) or backward pass (backward hook). With hooks you can inspect / modify the output and grad of a layer. The hook can be a forward hook or a backward book. . A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list. . class Hook(): def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self)) def remove(self): self.hook.remove() def __del__(self): self.remove() def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[],[]) means,stds,hists = hook.stats means.append(outp.data.mean().cpu()) stds .append(outp.data.std().cpu()) hists.append(outp.data.cpu().histc(40,0,10)) #histc isn&#39;t implemented on the GPU . It’s very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won’t be properly released when your model is deleted. . Hooks class that contains several hooks: . class Hooks(ListContainer): def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms]) def __enter__(self, *args): return self def __exit__ (self, *args): self.remove() def __del__(self): self.remove() def __delitem__(self, i): self[i].remove() super().__delitem__(i) def remove(self): for h in self: h.remove() . Having given an __enter__ and __exit__ method to our Hooks class, we can use it as a context manager. This makes sure that onces we are out of the with block, all the hooks have been removed and aren’t there to pollute our memory. . Current State of Affairs . Use the append_stats hook to look at the mean and std of the parameters in each of the layers. . The layer means: . . This looks awful. At the beginning of the training the values increase exponentially and then suddenly crash, repeatedly. It’s not training anything when this is happening. Eventually they settle down into some range and start to train. However are we sure that all the parameters are getting back to reasonable places after these ‘crashes’? Maybe the vast majority of them have zero gradients or are zero. Likely that this awful behaviour at the start of training is leaving the model in a really sad state. . The layer standard deviations: . . Subsequent layers standard deviations get closer and closer to 0. Later layers are basically getting 0 gradient. . Better Initialization . Use Kaiming init: . for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() . Here are the layer means and stds now: . . This is looking a lot better. No longer has the repeated exponential-crash pattern anymore. The standard deviations are all much closer to 1. . However these values are just aggregates of the layer parameters, so they don’t give us the full picture about how all the parameters are behaving. Rather than look at a single number we’d like to look at the distribution. To do that we can look at how the histogram of the parameters changes over time. . Here is a histogram of the activations, binned between 0 (relu) and 10 with 40 bins: . . What we find is that even with Kaiming init, with the high learning rate we still get the same exponential-crash behaviour. The biggest concern is the amount of mass at the bottom of the histogram at 0. . Here is a plot of the percentage of activations that are 0 or nearly 0: . . This is not good. In the last layer nearly 90% of the activations are actually 0. If you were training your model like this, it could appear like it was learning something, but you could be leaving a lot of performance on the table by wasting 90% of your activations. . Generalized ReLU . Let’s try to fix this so we can train a nice high learning rate and not have this happen. The main thing we will use to fix this is a GeneralRelu layer, where you can specify: . An amount to subtract from the ReLU. (In earlier lesson it seemed that subtracting 0.5 from the ReLU might be a good idea.) | Use leaky ReLU. | Also the option of a maximum value. | . Code for that: . class GeneralRelu(nn.Module): def __init__(self, leak=None, sub=None, maxv=None): super().__init__() self.leak,self.sub,self.maxv = leak,sub,maxv def forward(self, x): x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) if self.sub is not None: x.sub_(self.sub) if self.maxv is not None: x.clamp_max_(self.maxv) return x . Retrain just like before with Kaiming init, and a GeneralRelu with parameters: . leak=0.1 | sub=0.4 | maxv=6.0 | . The layer means and standard deviations over time: . . Looking better than before - means are around 0 and the stds are around 1 and are also a lot smoother looking. . Plot the histogram of the activations again, this time from -7 to 7 (leaky relu): . . This is way better! It’s using the full richness of the possible activations. There’s not crashing of values. . How many of the activations are at or around zero: . . The majority of the activations are not zero. . If we are careful about initialization, the ReLU, use one-cycle training, and a nice high learning rate of 0.9 we can achieve 98%-99% validation set accuracy after 8 epochs. . Normalization . Notebook: 07_batchnorm.ipynb . Batch Norm . Jump_to lesson 10 video . Up to this point we have learned how to initialize the values to get better results. To get even better results we need to use normalization. The most common form of normalization is Batch Normalization. This was covered in Lesson 6, but here we implement it from scratch. . Algorithm from the BatchNorm paper: . . It normalizes the batch and scales and shifts it by $ gamma$ and $ beta$, which are learnable parameters in the model. . Here is that as code: . class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() # NB: pytorch bn mom is opposite of what you&#39;d expect self.mom, self.eps = mom, eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;vars&#39;, torch.ones(1,nf,1,1)) self.register_buffer(&#39;means&#39;, torch.zeros(1,nf,1,1)) def update_stats(self, x): # x has dims (nb, nf, h, w) m = x.mean((0,2,3), keepdim=True) v = x.var ((0,2,3), keepdim=True) self.means.lerp_(m, self.mom) self.vars.lerp_ (v, self.mom) return m,v def forward(self, x): if self.training: with torch.no_grad(): m,v = self.update_stats(x) else: m,v = self.means,self.vars x = (x-m) / (v+self.eps).sqrt() return x*self.mults + self.adds . Let’s understand what this code is doing: . Instead of $ gamma$ and $ beta$, use descriptive names - mults and adds. There is a mult and an add for each filter coming into the BatchNorm. These are initialized to 1 and 0, respectively. . | At training time, it normalizes the batch data using the mean and variance of the batch. The mean calculation is: x.mean((0,2,3), ...). The dimensions of x are (nb, nf, h, w). So (0,2,3) tells it to take the mean over the batches, heights and widths, leaving nf numbers. Same thing with the variance. . | However, at inference time every batch needs to be normalized with the same means and variances. If we didn’t do this, then if we get a totally different kind of image then it would remove all the things that are interesting about it. . | While we are training, we keep an exponentially weighted moving average of the means and variances. The lerp_ method updates the moving average. These averages are what are used at inference time. . | These averages are stored in special way using: self.register_buffer. This comes from nn.Module. It works the same as a normal PyTorch tensor, except it moves the values to the GPU when the model is moved there. Also, we need to store these values the same way we store other parameters. This will save the numbers when the model is saved. We need to do this when we have ‘helper variables’ in a layer that aren’t parameters of the model. . | Another thing to note: if you use BatchNorm then the layer before doesn’t need to have a bias because BatchNorm has a bias already. . | . Exponentially Weighted Moving Average (EWMA) . The EWMA is a moving average that gives most weighting to recent values and exponentially decaying weight to older values. It allows you to keep a running average that is robust to outliers and requires that we keep track of only one number. The formula is: . [ mu_t = alpha x_t + (1 - alpha) mu_{t-1}] . Where $ alpha$ is called the momentum, which represents the degree of weight decrease. A higher value discounts older observations faster. . In PyTorch, EWMA is called ‘linear interpolation’ and uses the function means.lerp_(m, mom). In PyTorch the momentum in both lerp and in PyTorch’s BatchNorm uses opposite convention from everyone else, so you have to subtract value from 1 before you pass it. The default momentum in our code is 0.1. . (6 minute video with more info on EWMA) . Results . Training on MNIST with CNN, Kaiming init, BatchNorm, 1 epoch: . . Working well. Means are all around 0 and the variances are all around 1. . BatchNorm Deficiencies . BatchNorm works great in most places, but it can’t be applied to online learning tasks, where we learn after every item. The problem is that the variance of one data point is infinite. You could also get the same problem if a single batch of any size contained all the same values. BatchNorm doesn’t work well for small batch sizes (like 2). This prohibits people from exploring higher-capacity models that would be limited by memory. It also can’t be used with RNNs. . tl;dr We can’t use BatchNorm with small batchsizes or with RNNs. . Layer Norm . Jump_to lesson 10 video . LayerNorm is just like BatchNorm except instead of averaging over (0,2,3) we average over (1,2,3), and this doesn’t use the running average. Used in RNNs. It is not even nearly as good as BatchNorm, but for RNNs it is something we want to use because we can’t use BatchNorm. . From the LayerNorm paper: “batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small”. . The difference with BatchNorm is: . It doesn’t keep a moving average. | It doesn’t average over the batches dimension, but over the hidden/channel dimension, so it’s independent of the batch size. | Code: . class LayerNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, eps=1e-5): super().__init__() self.eps = eps self.mult = nn.Parameter(tensor(1.)) self.add = nn.Parameter(tensor(0.)) def forward(self, x): m = x.mean((1,2,3), keepdim=True) v = x.var ((1,2,3), keepdim=True) x = (x-m) / ((v+self.eps).sqrt()) return x*self.mult + self.add . Thought experiment: can this distinguish foggy days from sunny days (assuming you’re using it before the first conv)? . Foggy days are less bright and have less contrast (lower variance). | LayerNorm would normalize the foggy and sunny days to have the same mean and variance. | Answer: no you couldn’t. | . Instance Norm . Jump_to lesson 10 video . Instance Norm paper . The problem with LayerNorm is that it combines all channels into one. Instance Norm is a better version of LayerNorm where channels aren’t combined together. The key difference between instance and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones. . Code: . class InstanceNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, nf, eps=1e-0): super().__init__() self.eps = eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) def forward(self, x): m = x.mean((2,3), keepdim=True) v = x.var ((2,3), keepdim=True) res = (x-m) / ((v+self.eps).sqrt()) return res*self.mults + self.adds . Used for Style transfer, not for classification. It’s included here as another example of normalization. You need to understand what it is doing in available to understand is it something that might work. . Group Norm . Jump_to lesson 10 video . The Group Norm paper proposes a layer that divides channels into groups and normalizes the features within each group. GroupNorm is independent of batch sizes and it does not exploit the batch dimension, like how BatchNorm does. GroupNorm stays stable over a wide range of batch sizes. GroupNorm is supposed to solve the problem of BatchNorm with small batches. . It gets close to BatchNorm performance for ‘normal’ batch sizes in image classification, and beats BatchNorm with smaller batch sizes. GroupNorm works very well in large memory tasks such as: object detection, segmentation, and high resolution images. . It isn’t implemented in the lecture, but PyTorch has it already: . GroupNorm(num_groups, num_channels, eps=1e-5, affine=True) &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10) &gt;&gt;&gt; # Separate 6 channels into 3 groups &gt;&gt;&gt; m = nn.GroupNorm(3, 6) &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm) &gt;&gt;&gt; m = nn.GroupNorm(6, 6) &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm) &gt;&gt;&gt; m = nn.GroupNorm(1, 6) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input) . (See this blog post for more details. This blog post covers even more kinds of initialization.) . Summary of the Norms with One Picture . . (Source) . In this diagram the height and width dimensions are flattened to 1D, so a single image is a ‘column’ in this diagram. . Running Batch Norm: Fixing Small Batch Size Problem . Jump_to lesson 10 video . The normalizations above are attempts to work around the problem that you can’t use small batch sizes or RNNs with BatchNorm. But none of them are as good as BatchNorm. . Here Jeremy proposes a novel solution to solve the batch size problem, but not the RNN problem. This algorithm is called Running BatchNorm. . Algorithm idea: . In the forward function, don’t divide by the batch standard deviation or subtract the batch mean, but instead use the moving average statistics at training time as well, not just at inference time. | Why does this help? Let’s say you have batch size of 2. Then from time to time you may get a batch where the items are very similar and the variance is very close to 0. But that’s fine, because you are only taking 0.1 of that, and 0.9 of what you had before. | Code: . class RunningBatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;sums&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;sqrs&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;batch&#39;, tensor(0.)) self.register_buffer(&#39;count&#39;, tensor(0.)) self.register_buffer(&#39;step&#39;, tensor(0.)) self.register_buffer(&#39;dbias&#39;, tensor(0.)) def update_stats(self, x): bs,nc,*_ = x.shape self.sums.detach_() self.sqrs.detach_() dims = (0,2,3) s = x.sum(dims, keepdim=True) ss = (x*x).sum(dims, keepdim=True) c = self.count.new_tensor(x.numel()/nc) mom1 = 1 - (1-self.mom)/math.sqrt(bs-1) self.mom1 = self.dbias.new_tensor(mom1) self.sums.lerp_(s, self.mom1) self.sqrs.lerp_(ss, self.mom1) self.count.lerp_(c, self.mom1) self.dbias = self.dbias*(1-self.mom1) + self.mom1 self.batch += bs self.step += 1 def forward(self, x): if self.training: self.update_stats(x) sums = self.sums sqrs = self.sqrs c = self.count if self.step&lt;100: sums = sums / self.dbias sqrs = sqrs / self.dbias c = c / self.dbias means = sums/c vars = (sqrs/c).sub_(means*means) if bool(self.batch &lt; 20): vars.clamp_min_(0.01) x = (x-means).div_((vars.add_(self.eps)).sqrt()) return x.mul_(self.mults).add_(self.adds) . Let’s work through this code. . In normal BatchNorm we take the running average of the variance, but this doesn’t make sense - you can’t just average a bunch of variances. Particularly if the batch size isn’t constant. The way we want to calculate the variance is like this: | . [ mbox{E}[X^2] - mbox{E}[X]^2] . Let’s instead keep track of the sums sums and the sums of the squares sqrs, that store the EWMA of them. From the above formula - to get the means and variances we need to divide them by the count (running average of H*W*BS), which we also store as an EWMA. This accounts for the possibility of different batch sizes. . | We need to do something called Debiasing (aka bias correction). We want to make sure that no observation is weighted too highly. Normal way of doing EWMA gives the first point far too much weight. These first points are all zero, so the running averages are all biased low. Add a correction factor dbias: $x_i = x_i/(1 - alpha^i)$. When $i$ is large this correction factor tends to 1 - it only pushes up the initial values. (See this post). . | Lastly, to avoid the unlucky case of having a first mini-batch where the variance is close to zero, we clamp the variance to 0.01 for the first 20 batches. . | . Results . With a batchsize of 2 and learning rate of 0.4, it totally nails it with just 1 epoch: . . Links and References . Lesson 10 lesson video. | Lesson 10 notebooks: 05a_foundations.ipynb, 05b_early_stopping.ipynb, 06_cuda_cnn_hooks_init.ipynb, 07_batchnorm.ipynb. . | Laniken Lesson 10 notes: https://medium.com/@lankinen/fast-ai-lesson-10-notes-part-2-v3-aa733216b70d | Interpreting the colorful histograms used in this lesson | Lecture on Bag-of-tricks for CNNs. Loads of state-of-the-art tricks for training CNNs for image problems, which would be a great exercise to reimplement as callbacks. | Papers to read: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | Layer Normalization | Instance Normalization: The Missing Ingredient for Fast Stylization | Group Normalization | Revisiting Small Batch Training for Deep Neural Networks | . | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/03/24/fast-ai-lesson-10-notes-looking-inside-the-model.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/03/24/fast-ai-lesson-10-notes-looking-inside-the-model.html",
            "date": " • Mar 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jimypbr.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fast.ai v3 Lesson 9 Notes: Training in Depth",
            "content": "Overview . This lesson continues with the development of the MNIST model from the last lesson. It introduces and implements a Cross-entropy loss for MNIST, then takes a deep dive refactoring the model and the training loop, where it builds the equivalent classes from PyTorch from scratch, which provides a great foundation for understanding the main PyTorch classes. In the second half, the lesson moves onto the implementation of Callbacks and how they are integrated into the training loop in the FastAI library. Then it shows how to implement one-cycle training using the callback infrastructure that was built. . Lesson 9 lecture video . I found the second half of this lesson hard to make notes for because it is so code heavy. I didn’t want to just reproduce the jupyter notebooks here. I instead opted to provide a companion to the notebooks, providing extra explanation and also motivation for the design decisions. I tried to write it such that they could be used as guide for implementing the main parts yourself from scratch, which is how I practice this course. Enjoy! . Classification Loss Function . From the last lesson the model so far is: . class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)] def __call__(self, x): for l in self.layers: x = l(x) return x . Recall we were using the MSE as the loss function, which doesn’t make sense for a multi-classification problem, but was convenient as a teaching tool. Let’s continue with this and use an appropriate loss function. . This follows the notebook: 03_minibatch_training.ipynb . Cross-Entropy Loss . We need a proper loss function for MNIST. This is a multi-class classification problem so we use Cross-entropy loss. Cross-entropy loss is calculated using a function called the Softmax function: . [p(z_i) = hbox{softmax(z)}{i} = frac{e^{z{i}}}{ sum_{0 leq j leq n-1} e^{z_{j}}}] . Where $z_i$ are the real-valued outputs of the model. Softmax takes in a vector of $K$ real numbers, and normalizes it into a probability distribution of $K$ probabilities proportional to the exponentials of the input numbers. These collectively sum to 1, and each have values between 0 and 1 (this is also called a Categorical distribution). . We now have a probability vector (length 10), $p(z_i)$, that the model thinks that a given input has label $i$ (i.e. 0-9). This could look like: . pz = [0.05, 0.05, 0.05, 0.05, 0.1, 0.6, 0.025, 0.025, 0.025, 0.025] . When training know what the target value is. If this is represented as a categorical distribution like $z$, we would get the vector $x$: . x = [0., 0., 0., 0., 0., 1.0, 0., 0., 0., 0.] . We know for certain what the target value is, so the probability for that label is 1 and the rest are 0. So we could think of this as a distribution, or just as a one-hot encoded vector for the target label. . Cross-entropy is a function commonly used to quantify the difference between two probability distributions, this is why we can use it as our loss function. If we have the ‘true’ distribution, $x_i$, and the estimated distribution, $p(z_i)$, the cross-entropy loss is defined as: . [L =- sum_i x_i log p(z_i)] . This has a minimal value when the estimated distribution matches the true distribution. You can see this in the plot of the cross entropy with varying $p(z)$: . . Another name for cross entropy is the negative log likelihood. . Since $x$ is a one-hot encoded vector, all the 0 entries will be masked out leaving the cross entropy as just: . [L = - log p(z_i) = - log ( mbox{softmax}( mathbf{z})_i)] . Where $i$ is the index of the target label. We can therefore code the cross-entropy loss for multi-class as an array lookup. The code for the cross-entropy, or negative log likelihood, is therefore: . def nll(input, target): # input is log(softmax(z)) # x is 1-hot encoded target, so this simplifies to array lookup. return -input[range(target.shape[0]), target].mean() . The total loss is just the average of the negative log likelihood’s of all the training examples (in a batch). Next we need to implement a log-Softmax function to calculate the input to nll. . Log-Softmax Layer: Naive Implementation . First implementation: let’s code up the formula for Softmax then take the log of it: . def log_softmax(x): # naive implementation return (x.exp() / x.exp().sum(-1, keepdim=True)).log() . On paper, the maths works out and we can just convert the formula to code like above. However, this implementation has several big problems that mean this code will not work in practice. . Exponentials, Logs, and Floating Point Hell… . Working with exponentials on a computer requires care - these numbers can get very big or very small, fast. Floating point numbers are finite approximation of real numbers; for most of the time we can pretend that they behave like real numbers, but when we start to get into extreme values this thinking breaks down and we are confronted with the limitations of floats. . If a float gets too big it will overflow, that is it will go to INF: . np.exp(1) -&gt; 2.718281828459045 np.exp(10) -&gt; 22026.465794806718 np.exp(100) -&gt; 2.6881171418161356e+43 np.exp(500) -&gt; 1.4035922178528375e+217 np.exp(1000) -&gt; inf # oops... . On the other hand, if a float gets too small it will underflow, that is it will go to zero: . np.exp(-1) -&gt; 0.36787944117144233 np.exp(-10) -&gt; 4.5399929762484854e-05 np.exp(-100) -&gt; 3.720075976020836e-44 np.exp(-500) -&gt; 7.124576406741286e-218 np.exp(-1000) -&gt; 0.0 # oops... . The input to exponential doesn’t even have to that big to get under/overflow. Therefore we can’t really trust the naive softmax not to break because of this. . Another less obvious issue is that when doing operations on floats with extreme values, arithmetic can stop working: . np.exp(-10) + np.exp(-100) == np.exp(-10) # wut np.exp(10) + np.exp(100) == np.exp(100) # wut? . Operations between floats are performed and then rounded. The difference in value between the numbers here is so massive that the smaller one gets rounded away and disappears - loss of precision. This is a big problem for the sum of exponentials in the denominator of the softmax formula. . The solution to dealing with extreme numbers is to transform everything into log space, where things are more stable. A lot of numerical code is implemented in log space and there are many formulae/tricks for transforming operations into log space. The easy ones are: . [ begin{align} log e^x &amp;= x log b^a &amp;= a log b log (ab) &amp;= log a + log b log left ( frac{a}{b} right ) &amp;= log(a) - log(b) end{align}] . How to transform the sum of exponentials in softmax? There is no nice formula for the log of a sum, so we’d have to leave log space, compute the sum, and then take the log of it. Leaving log space would give us all the headaches described above. However there is trick to computing the log of a sum stably called the LogSumExp trick. The idea is to use the following formula: . [ log left ( sum_{j=1}^{n} e^{x_{j}} right ) = log left ( e^{m} sum_{j=1}^{n} e^{x_{j}-m} right ) = m + log left ( sum_{j=1}^{n} e^{x_{j}-m} right )] . Where $m$ is the maximum of the $x_{j}$. The subtraction of $m$ is to bring the numbers down to a size that’s safe to leave log land to perform the sum. . (Nerdy extras: even if a float isn’t so small that it underflows, if it gets small enough it becomes ‘denormalized’. Denormal numbers extend floats to get some extra values very close to zero. They are handled differently from normal floats by the CPU and their performance is terrible, slowing your code right down. See this classic stackoverflow question for more on this). . Log-Softmax Layer: Better Implementation . Implement LogSumExp in Python: . def logsumexp(x): m = x.max(dim=-1)[0] return m + (x - m.unsqueeze(-1)).exp().sum(dim=-1).log() . PyTorch already has this: x.logsumexp(). . We can now implement log_softmax and cross_entropy_loss: . def log_softmax(x): # return x - x.logsumexp(-1,keepdim=True) # pytorch version return x - logsumexp(x).unsqueeze(-1) def cross_entropy_loss(output): return nll(log_softmax(output), target) . Now we’ve implemented cross entropy from scratch we may use PyTorch’s versions of the functions: . import torch.nn.functional as F test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss) test_near(F.cross_entropy(pred, y_train), loss) . Mini-Batch Training . Basic Training Loop . Now we have the loss function done, next we need a performance metric. For a classification problem we can use accuracy: . def accuracy(out, targ): return (torch.argmax(out, dim=1) == targ).float().mean() . Now we built a training loop. (Recall the training loop from Fast.ai part 1). . The basic training loop repeats over the following: . Get the output of model on a batch of inputs | Compare the output with the target and compute the loss | Calculate the gradients of the loss wrt every parameter of the model | Update the parameters using those gradients to make them a little bit better | In Python with our current model this is: . for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad(): for l in model.layers: if hasattr(l, &#39;weight&#39;): l.weight -= l.weight.grad * lr l.bias -= l.bias.grad * lr l.weight.grad.zero_() l.bias .grad.zero_() . What it does: . loss.backward() computes the gradient of the loss wrt the parameters of the model using Pytorch’s autograd. | The updating of the parameters is done inside of torch.no_grad() because this is not part of the gradient calculation, it’s the result of it. | It loops through the layers and checks if they have attribute ‘weight’. | After updating the parameters it zeros the gradients so that the old values don’t persist into the next iteration. | . The next part of the lesson works on refactoring this loop until we end up with an implementation equivalent to the one in PyTorch. I think it’s a good exercise to try and reproduce this yourself after watching this part of the lecture. Rather than just copy the notebook, I will structure this section as hints/descriptions of what you need to do, followed by the solution code from the notebook. . Refactoring 1 . Currently when we update the parameters we have to loop through the layers and then check to see if they have parameter ‘weight’ and then update the weight and bias of that layer. This is long winded and it exposes the implementation too much. . We want instead to be able to loop through all the parameters in the model in a cleaner way: . ... loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . Hint: Our model already is a subclass of nn.Module, which has a special way of handling its attributes (__setattr__) that we can take advantage of if we change the way the layers are declared. Doing it this way will enable the use of nn.Module methods .parameters and .zero_grad… . . Solution: . Click to reveal code… class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.l1 = nn.Linear(n_in,nh) self.l2 = nn.ReLU() self.l3 = nn.Linear(nh,n_out) def __call__(self, x): return self.l3(self.l2(self.l1(x))) . Set the layers as attributes rather than storing a list of them. Doing things this way enables nn.Module to do some magic in the background. Look at the string representation of our model now: . &gt;&gt;&gt; model Model( (l1): Linear(in_features=784, out_features=50, bias=True) (l2): ReLU() (l3): Linear(in_features=50, out_features=10, bias=True) ) . It somehow knows about the layers we set as attributes. Looping through .parameters now returns the weight and bias matrices of the layers too. . What’s actually going on is nn.Module class overrides __setattr__, so every time we set an attribute that’s a PyTorch layer it registers that to an internal list. Methods like .parameters and .zero_grad then iterate through that list. . This internal list is stored as self._modules, we can take a peek at it: . &gt;&gt;&gt; model._modules OrderedDict([(&#39;l1&#39;, Linear(in_features=784, out_features=50, bias=True)), (&#39;l2&#39;, ReLU()), (&#39;l3&#39;, Linear(in_features=50, out_features=10, bias=True))]) . Refactoring 2 . It’s more convenient now, but it’s not convenient enough. It’s not nice having to write attributes for every layer - what if we had 50 layers? The forward pass is also inconvenient to write, it was better when we could just loop through the layers. . It would be nice if we could make the old implementation that had a list of layers work while getting the __setattr__ goodness too. . Hint: checkout nn.ModuleList . . Solution: . Click to reveal code… class SequentialModel(nn.Module): def __init__(self, layers): super().__init__() self.layers = nn.ModuleList(layers) def __call__(self, x): for l in self.layers: x = l(x) return x . ``nn.ModuleList` gives us the list model, but also registers the layers in the list so we retain the nice features from before: . &gt;&gt;&gt; model SequentialModel( (layers): ModuleList( (0): Linear(in_features=784, out_features=50, bias=True) (1): ReLU() (2): Linear(in_features=50, out_features=10, bias=True) ) ) . We have implemented the equivalent to nn.Sequential, which we now may use. . model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, nout)) . Checkout the source code for this and see how similar the code is to our version: nn.Sequential??. . Refactoring 3 . That’s the model refactored. What about the optimization step? Let’s replace our previous manually coded optimization step: . with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . and instead use just: . opt.step() opt.zero_grad() . This abstracts away the optimization algorithm and implementation and lets us swap things out in future. . Hint: Let’s create a class Optimizer to do this. It should take the parameters and the learning rate and implement the step and zero_grad methods. . . Solution: . Click to reveal code… class Optimizer(): def __init__(self, params, lr=0.05): self.params = list(params) self.lr = lr def step(self): with torch.no_grad(): for p in self.params: p -= p.grad * self.lr def zero_grad(self): for p in self.params: p.grad.zero_() . Training loop is now: . opt = Optimizer(model.parameters()) for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . We now implemented an equivalent of PyTorch’s optim.SGD, which we may now use: . from torch import optim opt = optim.SGD(model.parameters(), lr=0.05) . Refactoring 4 - Dataset . Let’s refactor how the data is retrieved and grouped into batches. . It’s clunky to iterate through minibatches of x and y values separately: . xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] . Instead, let’s do these two steps together, by introducing a Dataset class: . xb, yb = train_ds[i*bs : i*bs+bs] . Hint: your class needs to override __getitem__. . . Solution: . Click to reveal code… class Dataset(): def __init__(self, x, y): self.x, self.y = x, y def __len__(self): return len(x) def __getitem__(self): return self.x[i], self.y[i] . Use: . train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) . Refactoring 5 - DataLoader . Previously, our loop iterated over batches (xb, yb) like this: . for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] ... . Let’s make our loop much cleaner, using a data loader: . for xb,yb in train_dl: ... . Hint: you need to override __iter__ and use yield. . . Solution: . Click to reveal code… class DataLoader(): def __init__(self, ds, bs): self.ds,self.bs = ds,bs def __iter__(self): for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] . Use, training and validation data loaders: . train_dl = DataLoader(train_ds, bs) valid_dl = DataLoader(valid_ds, bs) . After all this refactoring the training loop now looks like: . def fit(): for epoch in range(epochs): for xb,yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . Much smaller and very readable. . Random Sampling . We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized. . As we did with DataLoader we can implement this as a class that takes a Dataset and batch size, then overrides _iter__ so that it yields the indices of the dataset in a random order. . class Sampler(): def __init__(self, ds, bs, shuffle=False): self.n,self.bs,self.shuffle = len(ds),bs,shuffle def __iter__(self): self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n) for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] . Use: . s = Sampler(small_ds,3,True) [o for o in s] . We can then update our DataLoader class so that it takes a Sampler and can return items in a random order. . def collate(b): xs,ys = zip(*b) return torch.stack(xs),torch.stack(ys) class DataLoader(): def __init__(self, ds, sampler, collate_fn=collate): self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn def __iter__(self): for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s]) . The collate function is for gathering up the data in the batch. In this case [self.ds[i] for i in s] returns a list of (x,y) tuples. We want these to be instead be two tensors xs and ys, which is what the function collate does. . Use: . train_samp = Sampler(train_ds, bs, shuffle=True) valid_samp = Sampler(valid_ds, bs, shuffle=False) train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate) valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate) . Training Loop Implemented with PyTorch Classes . At this point we have implemented the equivalents of the PyTorch classes: DataLoader, SequentialSampler, and RandomSampler, so we may use them from now on. . The PyTorch code that does everything we have implemented so far would be: . from torch.utils.data import DataLoader import torch.functional as F from torch import optim train_ds = Dataset(x_train, y_train) valid_ds = Dataset(x_valid, y_valid) train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True) valid_dl = DataLoader(valid_ds, bs, shuffle=False) loss_func = F.cross_entropy def get_model(): model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)) opt = optim.SGD(model.parameters(), lr=0.05) return model, opt def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): # train model.train() for xb, yb in train_dl: loss = loss_func(model(xb), yb) loss.backward() opt.step() opt.zero_grad() # validate model.eval() with torch.no_grad(): total_loss, total_acc = 0.0, 0.0 for xb, yb in valid_dl: pred = model(xb) total_loss += loss_func(pred, yb) total_acc += accuracy(pred, yb) nv = len(valid_dl) # NB these averages are incorrect if the # batch size varies... print(epoch, total_loss/nv, total_acc/nv) fit(3, model, loss_func, opt, train_dl, valid_dl) . This training loop also includes validation. We calculate and print the validation loss at the end of each epoch. . Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases. . Infinitely Customizable Training Loop . (Time in Lesson 9) . Our train loop so far is in the function fit above. We need a code design where users can infinitely customize this loop to add whatever they want, like fancy progress bars, different optimizers, tensorboard integration, regularization etc. The library design would need to be open and flexible enough to handle any unforeseen extensions. There is a good way to build something that can handle this - Callbacks. . . FastAI’s callbacks not only let you look at, but fully customize every single part of the training loop. The training loop contains all the parts of the code we wrote above, but in between these parts are slots for callbacks. Like on_epoch_begin, on_batch_begin, on_batch_end, on_loss_begin… and so on. Screen grab from lecture: . . These updates can be new values, or flags that skip steps or stop the training. . With this we can create all kinds of useful stuff in FastAI like learning rate schedulers, early stopping, parallelism, or gradient clipping. You can also mix them all together. . This next part of the lesson builds the framework for handling callbacks. It’s hard to write as notes because it is very code heavy. I will make some general descriptions of the design decisions. Then I will move onto the implementations of Callbacks used within this framework. I recommend just watching the lesson and working through the notebook. . Training Loop Landmarks . The training loop has several key points or landmarks just before or just after important parts of the training loop and we may want to inject some functionality/code into those points. In running order these are: . The start of the training: begin_fit | The end of the training: after_fit | The start of each epoch: begin_epoch | The start of a batch: begin_batch | After the loss is calculated: after_loss | After the backward pass is performed: after_backward | After the optimizer has performed a step: after_step | After all the batches and before validation: begin_validate | The end of each epoch: after_epoch | The end of the training: after_fit | Also after every batch or epoch we may want to halt everything: do_stop | Callback Class + Callback Handler (Version 1) . A sensible design option when faced with this would be to define an abstract base class that has methods corresponding to all the landmarks (+ method names) above. Every one of these methods should return True or False to indicate success/failure or some other stopping condition. At each of the landmarks in the training loop these booleans will be checked to see if the training loop should continue or not. . What the Callback base class could look like: . . We want to be able to pass multiple callbacks to the training loop so we’d need an addition class to handle collections of callbacks called CallbackHandler. It would have a collection of Callback objects and the same methods as Callback except it loops through all of its callback objects and return a boolean indicated if all the callbacks were successful or if any failed. . Here is a snippet of a potential CallbackHandler class: . . Alternative Design: Runner Class . The last design could lead to some code smell as seen here: . . Callbacks cb are passed as the argument of every function in the training loop. This suggests that these functions should be part of a class and cb should be an instance attribute in that class. . We create a new class Runner (I won’t list here), which contains one_batch, all_batches, and fit methods from the training loop, takes a list of Callback objects in the constructor, while also integrating the logic of the the previous CallbackHandler class. . It has some clever refactoring so that the looping through the callbacks is handled by overriding __call__, finding all the callbacks in its collection that have the required method name (e.g. ‘begin_epoch’) and calling them. The boolean logic of starting and stopping is handled by this method too, which means the Callback subclasses no longer need to return booleans - they can just do their job without needing to know the context within which they are used. Here is an example of a Callback in this implementation: . class ChattyCallback(Callback): def begin_epoch(self): print(&#39;begin_epoch...&#39;) def after_epoch(self): print(&#39;after epoch...&#39;) def begin_fit(self): print(&#39;begin_fit...&#39;) def begin_validate(self): print(&#39;begin_validate...&#39;) . &gt;&gt;&gt; run = Runner(cbs=[ChattyCallback()]) &gt;&gt;&gt; run.fit(2, learn) begin_fit... begin_epoch... begin_validate... after epoch... begin_epoch... begin_validate... after epoch... . The Runner design decouples the training loop from the callbacks such that even the different logic required for training and validation parts of the training loop can be implemented as a Callback which is hard coded into the Runner class: . class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False . (IMHO: The Runner code is quite hard to understand, but it’s not important in the rest of the course. This is an experimental class and it doesn’t end up even in the FastAI2 library. Looking at the state of the library (2/2020), ideas from this class do appear in the new Learner class. It’s better just to know what you need to write callbacks). . Things to note for all the Callbacks implemented in the next section: . They assume the existence of self.in_train, denoting if we are in training or validation. This variable is set by TrainEvalCallback. | They also have access to variables in the Runner class such as: self.opt, self.model, self.loss_func, self.data, self.n_epochs, and self.epochs. | . Callbacks Applied: Annealing . (Time in lesson 9 video) . Rather than spend too much time on understanding Runner, let’s move onto doing something useful - implementing some callbacks. . Let’s implement callbacks to do one-cycle training. If you can train the first batches well, then the whole training will be better, and you can get super-convergence. Good annealing is critical to doing the first few batches well. . First let’s make a callback Recorder that records the learning rate and loss after every batch. This calls will need two lists for the learning rates and the losses that are initialized at the being of the training loop, and it will need to append to these lists after every batch. . Recorder: . class Recorder(Callback): def begin_fit(self): self.lrs,self.losses = [],[] def after_batch(self): if not self.in_train: return self.lrs.append(self.opt.param_groups[-1][&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) # methods for plotting results def plot_lr (self): plt.plot(self.lrs) def plot_loss(self): plt.plot(self.losses) . Next we need a callback class that can update the parameters of the optimizer opt according to some schedule function based on how many epochs have elapsed. . ParamScheduler: . class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_func): self.pname, self.sched_func = pname, sched_func def set_param(self): for pg in self.opt.param_groups: pg[self.pname] = self.sched_func(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() . Next we want to define some annealing functions for raising and lowering the learning rate as shown in these plots: . .   |   | . These annealers should take a start and end value and a position between 0 and 1 denoting the relative position in the schedule. Rather than writing a function that takes all 3 of these arguments, when 2 of them are constant, we could either implement the annealing functions as an abstract base class or just use partial functions. Here partial functions are used: . def annealer(f): def _inner(start, end): return partial(f, start, end) return _inner @annealer def sched_lin(start, end, pos): return start + pos*(end-start) @annealer def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2 @annealer def sched_no(start, end, pos): return start @annealer def sched_exp(start, end, pos): return start * (end/start) ** pos def cos_1cycle_anneal(start, high, end): return [sched_cos(start, high), sched_cos(high, end)] . annearler is a decorator function. Decorators take a function and return another function and have the fancy @decorator syntax in Python. . We want to combine raising and lowering schedules in a single function alongside a list of positions for when the different schedules start. This is the combine_scheds function: . def combine_scheds(pcts, scheds): assert sum(pcts) == 1. pcts = tensor([0] + listify(pcts)) assert torch.all(pcts &gt;= 0) pcts = torch.cumsum(pcts, 0) def _inner(pos): idx = (pos &gt;= pcts).nonzero().max() actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx]) return scheds[idx](actual_pos) return _inner sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) . Which gives the following schedule: . . Now we can make our list of callbacks and run the training loop: . cbs = [Recorder(), AvgStatsCallback(accuracy), ParamScheduler(&#39;lr&#39;, sched)] learn = create_learner(get_model_func(0.3), loss_func, data) run = Runner(cbs=cbs) run.fit(3, learn) . We can then check the Recorder plots to see if it worked: . .   |   | . Super! . Q &amp; A . Why do we have to zero out our gradients in PyTorch? . In models, Parameters often have lots of sources of gradients. The grad stored by the parameters in PyTorch is a running sum - it is updated with +=, not =. If we didn’t zero the gradients after every update then these old values from previous batches would accumulate. . | Why does the optimizer separate step and zero_grad? . If we merged the two, we remove the ability to not zero the gradients here. There are cases where we may want that control. For example, what if we are dealing with super resolution 4K images and we can only fit a batch size of 2 into RAM. The stability you get from this batch size is poor and you need a larger batch size. We could instead not zero the grads every time, rather do it ever other batch. Our effective batch size would have then doubled. That’s called gradient accumulation. . | What’s the difference between FastAI callbacks and PyTorch Hooks? . PyTorch hooks allow you to hook into the internals of your model. So if you want to look at the forward pass of layer 2 of you model, FastAI callbacks couldn’t do that because they are operating at a higher level. All FastAI sees is the forward and backward passes of your model. What goes on within them is PyTorch’s domain. . | . Links and References . Lecture video: Lesson9 | Course notebooks: 04_callbacks.ipynb, 05_anneal.ipynb | Lesson notes by @Lankinen are great transcriptions of the lecture. | An even deeper dive into PyTorch’s classes, written by the FastAI team: What is torch.nn really? | Sylvain’s talk, An Infinitely Customizable Training Loop (from the NYC PyTorch meetup) and the slides that go with it . | Softmax vs Sigmoid? tl;dr sigmoid is a special case of softmax. | Some other cool Log tricks: Exp-normalize trick, Gumbel-max trick | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/16/fast-ai-lesson-9-notes-training-in-depth.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/16/fast-ai-lesson-9-notes-training-in-depth.html",
            "date": " • Feb 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fast.ai v3 Lesson 8 Notes: Backprop from the Foundations",
            "content": "Overview . Part 2 of FastAI 2019 is ‘bottom-up’ - building the core of the FastAI library from scratch using PyTorch. . This lesson implements matrix multiplication in pure Python, then refactors and optimizes it using broadcasting and einstein summation. Then this lesson starts to look at the initialization of neural networks. Finally the lesson covers handcoding the forward and backwards passes of a simple model with linear layers and ReLU, before refactoring the code to be more flexible and concise so that you can understand how PyTorch’s work. . Lesson 8 lecture video. . Different Matrix Multiplication Implementations . Naive Matmul . A baseline naive implementation in pure python code: . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . Time: 3.26s . Doing loops in pure python and updating array elements one at a time is the bane of performance in python. There is almost always another way that gives better performance. (Though admittedly in some cases the faster way isn’t always obvious or more readable IMHO). . Elementwise Matmul . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): # Any trailing &quot;,:&quot; can be removed c[i,j] = (a[i,:] * b[:,j]).sum() return c . Time: 4.84ms . The loop over k is replaced with a sum() over the elements of row slice in a times the column slice in b. This operation is outsourced to library calls in numpy which are likely compiled code written in C or Fortran, which gives the near 1000x speed-up. . Broadcasting matmul . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) # c[i] = (a[i, :, None] * b).sum(dim=0) alternative return c . Time: 1.11ms . WTH is this? As is almost always the case, optimizing code comes at the expense of code readability. Let’s work through this to convince ourselves that this is indeed doing a matmul. . Aside: Proof of Broadcasting Matmul . Matmul is just a bunch of dot products between the rows of one matrix and the columns of another: i.e. c[i,j] is the dot product of row a[i, :] and column b[:, j]. . Let’s consider the case of 3x3 matrices. a is: . tensor([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.]], dtype=torch.float64) . b is: . tensor([[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]], dtype=torch.float64) . Let’s derive the code above looking purely through modifying the shape of a. . a has shape (3,3) | a[0], first row of a, has shape (3,) and val [1, 1, 1] | a[i, :, None] (or a[i].unsqueeze(-1)) has shape (3,1) and val [[1], [1], [1]] | Now multiplying the result of 3 by the matrix b is represented by the expression (I have put brackets in to denote the array dimensions): . [ left( begin{matrix}(1) (1) (1) end{matrix} right) times left( begin{matrix}(0&amp;1&amp;2) (3&amp;4&amp;5) (6&amp;7&amp;8) end{matrix} right)] . From the rules of broadcasting, the $(1)$s on the left array are expanded to match the size of the rows on the right array (size 3). As such, the full expression computed effectively becomes: . [ left( begin{matrix}(1&amp;1&amp;1) (1&amp;1&amp;1) (1&amp;1&amp;1) end{matrix} right) times left( begin{matrix}(0&amp;1&amp;2) (3&amp;4&amp;5) (6&amp;7&amp;8) end{matrix} right) = left( begin{matrix}(0&amp;1&amp;2) (3&amp;4&amp;5) (6&amp;7&amp;8) end{matrix} right)] . The final step is to sum(dim=0), which sums up all the rows leaving a vector of shape (3,), value: [ 9., 12., 15.] . That completes the dot product and forms the first row of matrix c. Simply repeat that for the remaining 2 rows of a and you get the final result of the matmul: . tensor([[ 9., 12., 15.], [18., 24., 30.], [27., 36., 45.]], dtype=torch.float64) . Einstein Summation Matmul . This will be familiar to anyone who studied Physics, like me! Einstein summation (einsum) is a compact representation for combining products and sums in a general way. From the numpy docs: . “The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so np.einsum(&#39;i,i&#39;, a, b) is equivalent to np.inner(a,b). If a label appears only once, it is not summed, so np.einsum(&#39;i&#39;, a) produces a view of a with no changes.” . def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . Time: 172µs . This is super concise with great performance, but also kind of gross. It’s a bit weird that einsum is a mini-language that we pass as a Python string. We get no linting or tab completion benefits that you would get if it were somehow a first class citizen in the language. I think einsum could certainly be great and quite readable in cases where you are doing summations on tensors with lots of dimensions. . PyTorch Matmul Intrinsic . Matmul is already provided by PyTorch (or Numpy) using the @ operator: . def matmul(a, b): return a@b . Time: 31.2µs . The best performance is, unsuprisingly, provided by the library implementation. This operation will drop down to an ultra optimized library like BLAS or cuBLAS, written by low-level coding warrior-monks working at Intel or Nvidia who have have spent years hand optimizing linear algebra code in C and assembly. (The matrix multiply algorithm is actually a very complicated topic, and no one knows what the fastest possible algorithm for it is. See this wikipedia page for more). So basically in the real world, you should probably avoid writing your own matmal! . Single Layer Network: Forward Pass . Work through the Jupyter notebook: 02_fully_connected.ipynb . Create simple network for MNIST. One hidden layer and one output layer, parameters: . n = 50000 m = 784 nout = 1 # just for teaching purposes here, should be 10 nh = 50 . The model will look like this: . [X rightarrow mbox{Lin}(W_1, b_1) rightarrow mbox{ReLU} rightarrow mbox{Lin2}(W_2, b_2) rightarrow mbox{MSE} rightarrow L] . Linear activation function: . def lin(x, w, b): return x@w + b . ReLU activation function: . def relu(x): return x.clamp_min(0.) . Loss function we’ll use here is the Mean Squared Error (MSE). This doesn’t quite fit for a classification task, but it’s used as a pedgogical tool for teaching the concepts of loss and backpropagation. . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . Forward Pass of model: . def model(xb): l1 = lin(xb, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 preds = model(x_train) loss = mse(preds, y_train) . Let’s go over the tensor dimensions to review how the forward pass works: . Input $X$ is a batch of vectors of size 784, shape=[N, 784] | Hidden layer is of size 50 and has an input of shape=[N, 784] =&gt; $W_1$: shape=[784, 50], $b_1$: shape=[50], output: shape=[N, 50] | Output layer has size 1 and input of shape=[N, 50] =&gt; $W_2$: shape=[50, 1], $b_2$: shape=[1], output: shape=[N, 1] | . Initialisation . Recent research shows that weight initialisation in NNs is actually super important. If the network isn’t initialised well, then after one pass through the network the output can sometimes become vanishingly small or even explode, which doesn’t bode well for when we do backpropagation. . A rule of thumb to prevent this is: . The mean of the activations should be zero | The variance of the activations should stay close to 1 across every layer. | Let’s try Normal initialisation with a linear layer: . w1 = torch.randn(m,nh) b1 = torch.zeros(nh) w2 = torch.randn(nh,1) b2 = torch.zeros(1) . x_valid.mean(),x_valid.std() &gt;&gt;&gt; (tensor(-0.0059), tensor(0.9924)) . t = lin(x_valid, w1, b1) t.mean(),t.std() &gt;&gt;&gt; (tensor(-1.7731), tensor(27.4169)) . After one layer, it’s already in the rough. . A better initialisation is Kaiming/He initialisation (paper). For a linear activation you simply divide by the square root of the number of inputs to the layer.: . w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . Test: . t = lin(x_valid, w1, b1) t.mean(),t.std() &gt;&gt;&gt; (tensor(-0.0589), tensor(1.0277)) . The initialisation used depends on the activation function used. If we instead use a ReLU layer then we have to do something different from the linear. . If you have a normal distribution with mean 0 with std 1, but then clamp it at 0, then obviously the resulting distribution will no longer have mean 0 and std 1. . From pytorch docs: a: the negative slope of the rectifier used after this layer (0 for ReLU by default) . [ text{std} = sqrt{ frac{2}{(1 + a^2) times text{fan_in}}} ] This was introduced in the paper that described the Imagenet-winning approach from He et al: Delving Deep into Rectifiers, which was also the first paper that claimed “super-human performance” on Imagenet (and, most importantly, it introduced resnets!) . w1 = torch.randn(m,nh)*math.sqrt(2/m) . Test: . t = relu(lin(x_valid, w1, b1)) t.mean(),t.std() &gt;&gt;&gt; (tensor(0.5854), tensor(0.8706)) . The function that does this in the Pytorch library is: . from torch.nn import init w1 = torch.zeros(m,nh) init.kaiming_normal_(w1, mode=&#39;fan_out&#39;) . &#39;fan_out&#39; means that we divide by m, while &#39;fan_in&#39; would mean we divide by nh. This bit here is confusing because we are using the opposite convention to PyTorch has. PyTorch Linear layer stores the matrix as (nh, m), where our implementation is (m, nh). Looking inside the forward pass of linear in PyTorch the weight matrix is transposed before being multiplied. This means that for this special case here we swap ‘fan_out’ and ‘fan_in’. If we were using PyTorch’s linear layer we’d initialize with ‘fan_in’. . Let’s get a better view of the means and standard deviations of the model with Kaiming initialization by running the forward pass a few thousand times and looking at the distributions. . (Update, 8/2/20: Old plots were buggy. Fixed plots, added code, and added plots with Linear-ReLU model). . Linear-ReLU Model, Kaiming Init . def model_dist(x): w1 = torch.randn(m, nh) * math.sqrt(2/m) b1 = torch.zeros(nh) l1 = lin(x, w1, b1) l2 = relu(l1) l2 = l2.detach().numpy() return l2.mean(), l2.std() data = np.array([model_dist(x_train) for _ in range(3000)]) means, stds = data[:, 0], data[:, 1] . Mean and standard deviations of the outputs with Kaiming Initialization: . . . The means and standard deviations of the output have Gaussian distributions. The mean of the means is 0.55 and the mean of the standard deviations is 0.82. The mean is shifted to be positive because the ReLU has set all the negative values to 0. The typical standard deviation we get with Kaiming initialization is quite close to 1, which is what we want. . Full Model, Kaiming Init . def model_dist(x): w1 = torch.randn(m, nh) * math.sqrt(2/m) b1 = torch.zeros(nh) w2 = torch.randn(nh, nout) / math.sqrt(nh) b2 = torch.zeros(nout) l1 = lin(x, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) l3 = l3.detach().numpy() return l3.mean(), l3.std() data = np.array([model_dist(x_train) for _ in range(3000)]) means, stds = data[:, 0], data[:, 1] . . . The means have a clearly Gaussian distribution with mean value 0.01. The standard deviations have a slightly skewed distribution, but the mean value is 0.71. We see empirically that the expected output values of the model after Kaiming initialisation are approximately mean 0, standard deviation near to 1, so it seems to be working well. . Aside: Init in Pytorch - sqrt(5)?? . In torch.nn.modules.conv._ConvNd.reset_parameters: . def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) . A few differences here: . Uses Uniform distribution instead of a Normal distribution. This just seems to be convention the PyTorch authors have chosen to use. Not an issue and it is centred around zero anyway. | The sqrt(5) is probably a bug, according to Jeremy. | The initialization for the linear layer is similar. . From the documentation on parameter a: . a: the negative slope of the rectifier used after this layer (0 for ReLU by default) . For ReLU it should be 0, but here it is hard-coded to sqrt(5). So for ReLU activations in Conv layers, the initialization of some layers in PyTorch is suboptimal by default. . (Update 8/2/20). We can look at the distribution of the outputs of our model using PyTorch’s default init: . def model_dist(x, n_in, n_out): layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)] for l in layers: x = l(x) x = x.detach().numpy() return x.mean(), x.std() . . . Mean value is approximately 0.0 and the standard deviation is 0.16. This isn’t great - we have lost so much variation after just two layers. The course investigates this more in the notebook: 02a_why_sqrt5.ipynb. . (Update: Here is a link to the issue in PyTorch, still open (2020-2-13), https://github.com/pytorch/pytorch/issues/18182) . Gradients and Backpropagation . To understand backpropagation we need to first understand the chain rule from calculus. The model looks like this: . [x rightarrow mbox{Lin1} rightarrow mbox{ReLU} rightarrow mbox{Lin2} rightarrow mbox{MSE} rightarrow L] . Where $L$ denotes the loss. We can also write this as: . [L = mbox{MSE}( mbox{Lin2}( mbox{ReLU}( mbox{Lin1(x)})), y)] . Or fully expanded: . [ begin{align} L &amp;= frac{1}{N} sum_n^N left(( mbox{max}(0, X_nW^{(1)} + b^{(1)})W^{(2)} + b^{(2)}) - y_n right)^2 end{align}] . In order to update the parameters of the model, we need to know what is the gradient of $L$ with respect to (wrt) the parameters of the model. What are the parameters of this model? They are: $W_{ij}^{(1)}$, $W^{(2)}_{ij}$, $b^{(1)}_i$, $b^{(2)}_i$ (including indices to remind you of the tensor rank of the parameters). The partial derivatives of the parameters we want to calculate are: . [ frac{ partial L}{ partial W^{(1)}{ij}}, frac{ partial L}{ partial W^{(2)}{ij}}, frac{ partial L}{ partial b^{(1)}{i}}, ; mbox{and} ; frac{ partial L}{ partial b^{(2)}{i}}] . On first sight, looking at the highly nested function of $L$ finding the derivative of it wrt to matrices and vectors looks like a brutal task. However the cognitive burden is greatly decreased thanks to the chain rule. . When you have a nested function, such as: . [f(x,y,z) = q(x, y)z q(x,y) = x+y] . The chain rule tells you that the derivative of $f$ wrt to $x$ is: . [ frac{ partial f}{ partial x} = frac{ partial f}{ partial q} frac{ partial q}{ partial x} = (z) cdot(1) = z] . A helpful mnemonic is to picture the $ partial q$’s ‘cancelling out’. . Backpropagation: Graph Model . How does this fit into backpropagation? Things become clearer when the model is represented as a computational graph, instead of as equations. . Imagine some neuron $f$ in the middle of a bigger network. In the forward pass, data $x$ and $y$ flows from left to right through the neuron $f$, outputting $z$, then calculating the loss $L$. Then we want the gradients of all the variables wrt the loss. Here is a diagram taken from CS231 course : . . (Source: brilliant CS231 course from Stanford. This lecture made backpropagation ‘click’ for me: video, notes). . Calculate the gradients of the variables backwards from right to left. We have the gradient $ frac{ partial L}{ partial z}$ coming from ‘upstream’. To calculate $ frac{ partial L}{ partial x}$, we use the chain rule: . [ frac{ partial L}{ partial x} = frac{ partial L}{ partial z} frac{ partial z}{ partial x}] . The gradient = upstream gradient $ times$ local gradient. This relation recurses back through the rest of the network, so a neuron directly before $x$ would receive the upstream gradient $ frac{ partial L}{ partial x}$. The beauty of the chain rule is that it enables us to break up the model into its constituent operations/layers, compute their local gradients, then multiply by the gradient coming from upstream, then propagate the gradient backwards, repeating the process. . Coming back to our model - $ mbox{MSE}( mbox{Lin2}( mbox{ReLU}( mbox{Lin1(x)})), y)$ - to compute the backward pass we just need to compute the expressions for the derivatives of MSE, Linear layer, and ReLU layer. . Gradients of Vectors or Matrices . What happens when $z$, $x$, and $y$ aren’t scalar, but are vectors or matrices? Nothing changes with how backpropagation works - just the maths for computing the local gradients gets a bit hairier. . If the loss $L$ is a scalar and $ mathbf{z}$ is a vector then the derivative would be vector: . [ frac{ partial L}{ partial mathbf{z}} = left( frac{ partial L}{ partial z_1}, frac{ partial L}{ partial z_2}, …, frac{ partial L}{ partial z_n}, right)] . Think: “For each element of $ mathbf{z}$, if it changes by a small amount how much will $L$ change?” . If $ mathbf{x}$ and $ mathbf{z}$ are both vectors then the derivative would be a Jacobian matrix: . [ mathbf{ frac{ partial mathbf{z}}{ partial mathbf{x}}} = left[ begin{array}{ccc} frac{ partial z_1}{ partial x_1} &amp; frac{ partial z_1}{ partial x_2} &amp; … &amp; frac{ partial z_1}{ partial x_m} frac{ partial z_2}{ partial x_1} &amp; frac{ partial z_2}{ partial x_2} &amp; … &amp; frac{ partial z_2}{ partial x_m} … &amp; … &amp; … &amp; … frac{ partial z_n}{ partial x_1} &amp; frac{ partial z_n}{ partial x_2} &amp; … &amp; frac{ partial z_n}{ partial x_m} end{array} right]] . Think: “For each element of $ mathbf{x}$”, if it changes by a small amount then how much will each element of $ mathbf{y}$ change? . Summary, again taken from CS231n: . . More info: a full tutorial on matrix calculus is provided here: Matrix Calculus You Need For Deep Learning. . Gradient of MSE . The mean squared error: . [L = frac{1}{N} sum_i^N (z_i - y_i)^2] . Where $N$ is the batch size, $z_i$ is the output of the model for data point $i$, and $y_i$ is the target value of $i$. The loss is the average of the squared error in a batch. $ mathbf{z}$ is a vector here. The derivative of scalar $L$ wrt a vector will be vector. . [ begin{align} frac{ partial L}{ partial z_i} &amp;= frac{ partial}{ partial z_i} left( frac{1}{N} sum_j^N (z_j - y_j)^2 right) &amp;= frac{ partial}{ partial z_i} frac{1}{N} (z_i - y_i)^2 &amp;= frac{2}{N}(z_i - y_i) end{align}] . All the other terms in the sum go to zero because they don’t depend on $z_i$. Notice also how $L$ doesn’t appear in the gradient - we don’t actually need the value of the loss in the backwards step! . In Python code: . def mse_grad(inp, targ): # inp from last layer of model, shape=(N,1) # targ targets, shape=(N) # want: grad of MSE wrt inp, shape=(N, 1) grad = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / inp.shape[0] inp.g = grad . Gradient of Linear Layer . Linear layer: . [Y = XW + b] . Need to know: . [ frac{ partial L}{ partial X}, frac{ partial L}{ partial W}, frac{ partial L}{ partial b}] . Where $X$, and $W$ are matrices and $b$ is a vector. We already know $ frac{ partial L}{ partial Y}$ - it’s the upstream gradient (remember it’s a tensor, not necessarily a single number). . Here is where the maths gets a bit hairier. It’s not worth redoing the derivations of the gradients here, which can be found in these two sources: matrix calculus for deep learning, linear backpropagation. . The results: . [ frac{ partial L}{ partial X} = frac{ partial L}{ partial Y}W^T frac{ partial L}{ partial W} = X^T frac{ partial L}{ partial Y} frac{ partial L}{ partial b_i} = sum_j^M frac{ partial L}{ partial y_{ij}}] . In Python: . def lin_grad(inp, out, w, b): # inp - incoming data (x) # out - upstream data # w - weight matrix # b - bias inp.g = out.g @ w.t() w.g = inp.t() @ out.g b.g = out.g.sum(dim=0) . Gradient of ReLU . Gradient of ReLU is easy. For the local gradient - if the input is less than 0, gradient is 0, otherwise it’s 1. In Python . def relu_grad(inp, out): # inp - input (x) # out - upstream data inp.g = (inp&gt;0).float() * out.g . Putting it together: forwards and backwards . def forwards_and_backwards(inp, targ): # forward pass l1 = lin(inp, w1, b1) l2 = relu(l1) out = lin(l2, w2, b2) loss = mse(out, targ) # backward pass mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . Check the Dimensions. How does batchsize affect things? . (Added 17-03-2020) . What do the dimensions of the gradients look like? The loss $L$ is a scalar and the parameters are tensors so remembering the rules above the derivative of $L$ wrt any parameter will have the same dimensionality as that parameter. The gradients of the parameters have the same shape as the parameters, which makes intuitive sense. . w1.g.shape =&gt; [784, 50] | b1.g.shape =&gt; [50] | w2.g.shape =&gt; [50, 1] | b2.g.shape =&gt; [1] | loss.shape =&gt; [] (scalar) | . Notice how the batch size doesn’t appear in the gradients. That’s not to say it doesn’t matter - the batch size is there behind the scenes in the gradient calculation: the loss is an average of the individual losses in a batch, and also as a dimension multiplied out in the matrix multiplies of the gradient calculations. . To be even more explicit with the dimensions: . inp.g = out.g @ self.w.t() # [N, 784] = [N, 50] @ [50, 784] self.w.g = inp.t() @ out.g # [784, 50] = [784, N] @ [N, 50] self.b.g = out.g.sum(0) # [50] = [N, 50].sum(0) inp.g = out.g @ self.w.t() # [N, 50] = [N, 1] @ [1, 50] self.w.g = inp.t() @ out.g # [50, 1] = [50, N] @ [N, 1] self.b.g = out.g.sum(0) # [1] = [N, 1].sum(0) . With bigger batch size you are accumulating more gradients because it is basically doing more dot products. If you could hack the loss so its gradient is constant and increase the batch size then these gradients would get correspondingly larger (in absolute size). . In reality this is cancelled out because the larger the batch size the smaller the gradient. You can see this by look at the gradient calculation for MSE: it is divided by the batch size. . Let’s vary the batchsize and plot the average gradients of the parameters W1 and W2, alongside the loss and loss gradient: . . The average gradient of the loss gets smaller with increasing batchsize, while the other gradients and the loss pretty much settle towards some value. . Refactoring . (Updated 17-03-2020) . The rest of the notebook - 02_fully_connected.ipynb - is spent refactoring this code using classes so we understand how PyTorch’s classes are constructed. I won’t reproduce it all here. If you want to reproduce it yourself you need to create a base Module that all your layer inherit from, which remembers the inputs it was called with (so it can do gradient calculations): . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . The different layers (linear, ReLU, MSE) need to subclass Module and implement forward and bwd methods. . The end result of this gives an equivalent implementation of PyTorch’s nn.Module. The equivalent with PyTorch classes, which we can now use, is: . from torch import nn class Model(nn.Module): def __init__(self, n_in, n_out): super().__init__() self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)] self.loss = mse def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x.squeeze(), targ) . Now that we understand how backprop works, we luckily don’t have to derive anymore derivatives of tensors, we can instead from now on harness PyTorch’s autograd to do all the work for us! . model = Model(m, nh, 1) loss = model(x_train, y_train) loss.backward() # do the backward pass! . Links and References . Lesson 8 lecture video. . | Lesson notes from Laniken provide a transcription of the lesson. . | Broadcasting tutorial from Jake Vanderplas: Computation on Arrays: Broadcasting. . | Deeplearning.ai notes on initialisation with nice demos of different initialisations and their effects: deeplearning.ai . | Kaiming He paper on initialization with ReLu activations (assignment: read section 2.2 of this paper): Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification . | Fixup Initialization: paper where they trained a 10,000 layer NN with no normalization layers through careful initialization. . | Things that made Backprop ‘click’ for me: . CS231: backpropagation explained using the a circuit model: http://cs231n.github.io/optimization-2/ | CS231: backpropagation lecture (Andrej Karpathy), slides. | Blog post with worked examples of backpropagation on simple calculations. | Calculus on Computational Graphs, Chris Olah. | . | StackExchange: Tradeoff batch size vs. number of iterations to train a neural network - worth reading about somewhat unintuitive effect batchsize has on training performance and speed. . | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/06/fast-ai-lesson-8-notes-backprop-from-the-foundations.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/02/06/fast-ai-lesson-8-notes-backprop-from-the-foundations.html",
            "date": " • Feb 6, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jimypbr.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Galaxy Zoo Kaggle Competition Redux with Fast.ai",
            "content": "Overview . . The Galaxy Zoo kaggle competition is something that I feel a special connection with. Not only because my own background is in astrophysics, but this competition first introduced me to Kaggle and data science. While I didn’t have the time or knowledge to compete at the time, I followed it and I knew even then that the winning solutions would be based on convolutional neural networks. . I’ve been learning deep learning on and off since 2017, but it wasn’t until fastai that I actually managed to get from zero to one. After finishing the first course in fastai, going back and doing the Galaxy Zoo challenge was actually pretty undaunting! It’s basically a quirky image multi-label classification classification posed as a regression problem. Once I got my head around this, I was very fast in getting a CNN running in fastai. . Problem . From the Galaxy Zoo challenge description: . Understanding how and why we are here is one of the fundamental questions for the human race. Part of the answer to this question lies in the origins of galaxies, such as our own Milky Way. Yet questions remain about how the Milky Way (or any of the other ~100 billion galaxies in our Universe) was formed and has evolved. Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Understanding the distribution, location and types of galaxies as a function of shape, size, and color are critical pieces for solving this puzzle. . … . . (Image Credit: ESA/Hubble &amp; NASA) . Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies. That’s where you come in. . This competition asks you to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class. Can you write an algorithm that behaves as well as the crowd does? . Rather than being an image classification or image multi-label classification problem, that one might initially assume, this problem is actually a regression problem. The task is actually to predict the distribution of how the users would label an image of a galaxy. The UX presented to the user is that of a descision tree of questions: . . The distribution of user’s answers to these questions is represented as a 37D vector of floats with values between 0 and 1. These values are weighted according to the description here. . The task is therefore to train an algorithm that takes an image of a galaxy as an input and outputs a 37D vector of floats; it is a multi-dimensional regression problem. . Preprocessing . Let’s look at a batch of the images: . . The target galaxy is always in the centre of the image. There is a lot of redundant space round the outside of the galaxies. Each image is 424x424. I cropped the images down to 224x224. This saved on computation without throwing out resolution. . Transforms . The transforms I used: . tfms = get_transforms(flip_vert=False, max_warp=0.0, max_rotate=360, max_lighting=0.0, max_zoom=1.05) . Here they are as a grid: . . I also tried with adjusting the brightness and contrast, but I found that that didn’t improve anything. . DataBlock . The image file names and accompanying classification vectors are stored in a CSV file training_solutions_rev1. . I modified the fastai class FloatList to GalaxyFloatList, which is the same except it uses GalaxyFloatItem instead of FloatItem. GalaxyFloat has the show method subclassed so that the 37D float vector is converted to a string using a function I wrote called vec2labels. . class GalaxyFloatItem(FloatItem): def show(self, ax:plt.Axes, **kwargs): &quot;Translate the GalaxyZoo vector into a list of features&quot; ax.set_title(vec2labels(self.data)) class GalaxyFloatList(ItemList): def __init__(self, items:Iterator, log:bool=False, classes:Collection=None, **kwargs): super().__init__(np.array(items, dtype=np.float32), **kwargs) self.log = log self.copy_new.append(&#39;log&#39;) self.c = self.items.shape[1] if len(self.items.shape) &gt; 1 else 1 self.loss_func = MSELossFlat() def get(self, i): o = super().get(i) return GalaxyFloatItem(np.log(o) if self.log else o) def reconstruct(self,t): return GalaxyFloatItem(t.numpy()) src = (ImageList.from_csv(path, &#39;training_solutions_rev1.csv&#39;, folder=&#39;images_training_rev1_cropped&#39;, suffix=&#39;.jpg&#39;, cols=0) .split_by_rand_pct(0.2) .label_from_df(cols=list(range(1, 38)), label_cls=GalaxyFloatList)) data = (src.transform(tfms, size=112, resize_method=ResizeMethod.SQUISH, padding_mode=&#39;reflection&#39;).databunch()).normalize(imagenet_stats) . Model . For my model I used a ResNet50 CNN pretrained on Imagenet. . learner = cnn_learner(data, models.resnet50, metrics=rmse, ps=0.1, wd=1e-4,) learner.model[-1] = nn.Sequential(*learner.model[-1], GalaxyOutput()) . At the end of the network I tacked on a simple layer that correctly normalises the probability vector outputed by the network so that the output obeys the rules of probability. I got this idea after looking at the winning submission. Training without normalising the output also worked quite well, I found, however it does produce ill-formed results such as small negative numbers so normalising is good idea to get a small performance boost. . class GalaxyOutput(nn.Module): def __init__(self): super().__init__() def forward(self, x): return answer_probability(x) . Here is the code that normalises the probability vectors: . task_sectors = { 1: slice(0, 3), 2: slice(3, 5), 3: slice(5, 7), 4: slice(7, 9), 5: slice(9, 13), 6: slice(13, 15), 7: slice(15, 18), 8: slice(18, 25), 9: slice(25, 28), 10: slice(28, 31), 11: slice(31, 37), } def normalize(q): return q / (q.sum(dim=1) + eps)[:, None] def answer_probability(x): # Source: http://benanne.github.io/2014/04/05/galaxy-zoo.html # clip probabilities nb = x.shape[0] x = x.clamp_min(0.) # normalize q1 = normalize(x[:, task_sectors[1]]) q2 = normalize(x[:, task_sectors[2]]) q3 = normalize(x[:, task_sectors[3]]) q4 = normalize(x[:, task_sectors[4]]) q5 = normalize(x[:, task_sectors[5]]) q6 = normalize(x[:, task_sectors[6]]) q7 = normalize(x[:, task_sectors[7]]) q8 = normalize(x[:, task_sectors[8]]) q9 = normalize(x[:, task_sectors[9]]) q10 = normalize(x[:, task_sectors[10]]) q11 = normalize(x[:, task_sectors[11]]) # reweight w1 = 1.0 w2 = q1[:, 1] * w1 w3 = q2[:, 1] * w2 w4 = w3 w5 = w4 w6 = 1.0 w7 = q1[:, 0] * w1 w8 = q6[:, 0] * w6 w9 = q2[:, 0] * w2 w10 = q4[:, 0] * w4 w11 = w10 wq1 = w1*q1 wq2 = w2[:, np.newaxis]*q2 wq3 = w3[:, np.newaxis]*q3 wq4 = w4[:, np.newaxis]*q4 wq5 = w5[:, np.newaxis]*q5 wq6 = w6*q6 wq7 = w7[:, np.newaxis]*q7 wq8 = w8[:, np.newaxis]*q8 wq9 = w9[:, np.newaxis]*q9 wq10 = w10[:, np.newaxis]*q10 wq11 = w11[:, np.newaxis]*q11 return torch.cat([wq1, wq2, wq3, wq4, wq5, wq6, wq7, wq8, wq9, wq10, wq11], dim=1) . This code is pretty yuck so I will explain. task_sectors are the slices of the probability vector corresponding to the answers of each of the questions (tasks) in the decision tree. These sectors are all normalised individually, so they are &gt;=0 and sum to 1. . Training . One of the things I stuggled a lot in remedying in this problem was underfitting. With the default settings of CNNs in fastai (ps=0.5 and wd=1e-2) the validation loss was consistently lower than the training loss even after training for many epochs. The loss was also not improving over subsequent cycles. Here is an example of the loss plot for this case: . . According to Jeremy in lesson 2 of fastai, underfitting can be remedied with reducing regularization. After many experiments I settled on the following values: . ps=0.1 | wd=1e-4 | . I then trained the network using the freeze/unfreeze protocol taught in the fastai course and used progressive resizing to get the drive down the error further. To overcome underfitting I had to run many cycles until the validation error stopped being less than the training error. . Training Programme . Train head 2 epochs lr=5e-2 . | Unfreeze all layers . | Train 10 epochs lr=1e-4. Validation error here is ~0.081. . | Resize to 224x224 . | data = (src.transform(tfms, padding_mode=&#39;reflection&#39;) .databunch().normalize(imagenet_stats)) learner.data = data data.train_ds[0][0].shape . | Freeze all layers . | Train head 2 epochs lr=1e-2 . | Unfreeze . | learner.fit_one_cycle(6, 1e-4) . | learner.fit_one_cycle(6, 1e-4) . | learner.fit_one_cycle(6, 1e-4) . | learner.fit_one_cycle(8, 1e-4/5) . | Change dropout: ps=0.25 . | learner.fit_one_cycle(6, slice(1e-6, 1e-5/2)) . | The final training epochs: . epoch train_loss valid_loss root_mean_squared_error time . 0 | 0.005536 | 0.005953 | 0.077037 | 05:08 | . 1 | 0.005683 | 0.005960 | 0.077083 | 05:09 | . 2 | 0.005581 | 0.005979 | 0.077199 | 05:11 | . 3 | 0.005662 | 0.005976 | 0.077189 | 05:09 | . 4 | 0.005648 | 0.005932 | 0.076905 | 05:10 | . 5 | 0.005611 | 0.005942 | 0.076965 | 05:11 | . 6 | 0.005511 | 0.005919 | 0.076818 | 05:09 | . 7 | 0.005534 | 0.005906 | 0.076728 | 05:10 | . | . Total training time: ~200 minutes . Final Validation RMSE: 0.076728 . Summary and Further Thoughts . This competition was required a lot more work that I thought it would be, even with all the convenience of fastai. Before I started I expect this project to be a multi-label classification problem, but it’s actually a regression problem. Writing the normalization layer was tricky to figure out and in the end I learned a lot about PyTorch and fastai by writing this and appending it onto a pretrained network. . I believe my main issue was underfitting in this problem. I remedied this by reducing the regularisation and running for more epochs. In the future I will do further experiments to see if it can be fixed in another way with a larger network or different learning rate schedules. . The final validation RMSE is about the equal to what Dielemann achieved for a single model: http://benanne.github.io/2014/04/05/galaxy-zoo.html. I’m a bit disappointed that I couldn’t do better than the result from 5 years ago, but on the other hand the amount of code required to do this today compared to what Dielemann wrote is tiny. His final score was down at 0.074 after bagging the results of many CNNs. This is a huge amount of effort and was necessary to win the Kaggle competition at the time, however I feel this isn’t worth trying to reproduce this. . My Jupyter notebooks for this are on github: https://github.com/jimypbr/galaxyzoo-fastai .",
            "url": "https://jimypbr.github.io/blog/2019/09/20/galaxy-zoo-kaggle-competition-redux-with-fast-ai.html",
            "relUrl": "/2019/09/20/galaxy-zoo-kaggle-competition-redux-with-fast-ai.html",
            "date": " • Sep 20, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Fast.ai v3 Lesson 6 Notes: CNN Deep Dive; Ethics",
            "content": "Overview of the Lesson . This lesson starts with teaching the powerful techniques to avoid overfitting and decrease training time.: . Dropout: remove activations at random during training in order to regularize the model | Data augmentation: modify model inputs during training in order to effectively increase data size | Batch normalization: adjust the parameterization of a model in order to make the loss surface smoother. | . Next the lesson teaches convolutions, which can be thought of as a variant of matrix multiplication with tied weights, and are the operation at the heart of modern computer vision models (and, increasingly, other types of models too). . This knowledge is then used to create a class activated map, which is a heat-map that shows which parts of an image were most important in making a prediction. . Finally, the lesson ends with data ethics. . Platform.ai - Assisted Image Labeling . Jeremy showed in his TED talk in 2015 a cool demo where you can ‘collaborate’ with a pretrained neural network to label an unlabeled image dataset. | Basically it is a UI where the images are projected from the network into a 2D space (via T-SNE or similar). If the model is trained well then there will be good separation between the images in this space. | It is an iterative process where the user labels a few images, the network trains with these labels, the network then guesses the labels, and the user can correct these and label more images. Repeat. | Platform.ai is a product brought out by Jeremy that lets you do with your own image dataset that you upload. | . . This is similar to active learning. | . Tabular Data: Deep Dive . We want to understand every line of the code of TabularModel: . class TabularModel(Module): &quot;Basic model for tabular data.&quot; def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False): super().__init__() ps = ifnone(ps, [0]*len(layers)) ps = listify(ps, layers) self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs]) self.emb_drop = nn.Dropout(emb_drop) self.bn_cont = nn.BatchNorm1d(n_cont) n_emb = sum(e.embedding_dim for e in self.embeds) self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range sizes = self.get_sizes(layers, out_sz) actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None] layers = [] for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)): layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act) if bn_final: layers.append(nn.BatchNorm1d(sizes[-1])) self.layers = nn.Sequential(*layers) def get_sizes(self, layers, out_sz): return [self.n_emb + self.n_cont] + layers + [out_sz] def forward(self, x_cat:Tensor, x_cont:Tensor) -&gt; Tensor: if self.n_emb != 0: x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] x = torch.cat(x, 1) x = self.emb_drop(x) if self.n_cont != 0: x_cont = self.bn_cont(x_cont) x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont x = self.layers(x) if self.y_range is not None: x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0] return x . Model . The model for a tabular learner in fastai is like this one (source): . . In this model there is a categorical variable: words in real estate ad and there are two continuous variables: latitude and longitude. . The words in a real estate ad can be represented as a sparse vector of the word counts in the text. The network learns a lower dimensional embedding for these words as shown as the green layer in the diagram. . In pink is the actual ML model: it’s a simple multi-layer perceptron. After the categorical variables have been encoded by their embedding layers, these vectors are catted together along with the continuous variables to make one big vector; this is the input to the MLP. That’s all there is to the tabular learner. . In the fastai the code to create the tabular learner is: . learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=exp_rmspe) . What do these parameters mean? . layers is a list of ints, which specify the size of each of the layers in the MLP. Here it has two layers of size 1000 and 500 respectively. . | Now the intermediate weight matrix is going to have to go from a 1000 activation input to a 500 activation output, which means it’s going to have to be 500,000 elements in that weight matrix. That’s an awful lot for a data set with only a few hundred thousand rows. So this is going to overfit, and we need to make sure it doesn’t. The way to make sure it doesn’t is to use regularization; not to reduce the number of parameters. . | One way to do that will be to use weight decay which fast.ai will use automatically, and you can vary it to something other than the default if you wish. It turns out in this case, we’re going to want more regularization. . | The parameter ps provides something called dropout. . | Also the parameter emb_drop provides dropout to just the embeddings. . | . Dropout . Dropout is a kind of regularization. This is the dropout paper. . The diagram from the paper illustrates perfectly what is going on: . . For dropout, we throw that away. At random, we throw away some percentage of the activations. . N.B. it doesn’t zero the weights/parameters. (Remember, there’s only two types of layer in a neural net - parameters and activations). . We throw each one away with a probability p. A common value of p is 0.5. . It means that no one activation can memorize some part of the input because that’s what happens if we over fit. If we over fit, some part of the model is basically learning to recognize a particular image rather than a feature in general or a particular item. This forces the network to use more neurons to determine the outcome and so the network is more likely to learn the actual patterns in the data, rather than trying to short-circuit the problem by memorizing the data. . During backpropagation, the gradients for the zeroed out neurons are also zero. . Check out this quote from one of the creators, Geoffry Hinton: . I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting. . Hinton: Reddit AMA . Dropout stops your neural network from conspiring against you! Dropout is a technique that works really well, and has become standard practice in training neural networks. . In fastai nearly every learner has a parameter ps for defining how much dropout to use (number between 0 and 1). . Dropout: Training versus test time: . There is an interesting feature of dropout regarding training time and test time (AKA inference time). Training time is when we’re actually updating the weights - doing backpropagation etc. During training time, dropout works the way we just saw. . At test time however we turn off dropout. We’re not going to do dropout anymore because we want it to be as accurate as possible. It’s not updating any weights at test time so overfitting obviously isn’t an issue. But there is a small issue here. If previously p was set to 0.5, then half the activations were being removed. Which means when we turn them all back on again, now our overall activation level is twice what it used to be. Therefore, in the paper, they suggest multiplying all of the weights affect by dropout at test time by p. . You could alternatively scale things at training time instead, except you would scale the activations and gradients of the non-zeroed neurons by $ frac{1}{1-p}$ . (Source). . Dropout in Tabular Learner . Looking again at the tabular learner: . learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=exp_rmspe) . In this case: . Dropout of 0.001 on layer 1. | Dropout of 0.01 on layer 2. | Then some dropout of 0.04 on the embedding layers. | . What is the embedding dropout actually doing? Look at the source code of the forward method again specifically at the embedding part: . def forward(self, x_cat:Tensor, x_cont:Tensor) -&gt; Tensor: if self.n_emb != 0: x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] x = torch.cat(x, 1) x = self.emb_drop(x) ... . It calls each embedding | It concatenates the embeddings in a single matrix (batch of vectors) | It calls dropout on that | . The output of an embedding layers is basically a big vector so we can think of it as just another layer in the neural network and so just call dropout on that like we normally would. . Here is the TabularModel for the Rossmann dataset: . TabularModel( (embeds): ModuleList( (0): Embedding(1116, 50) (1): Embedding(8, 5) (2): Embedding(4, 3) (3): Embedding(13, 7) (4): Embedding(32, 17) (5): Embedding(3, 2) (6): Embedding(26, 14) (7): Embedding(27, 14) (8): Embedding(5, 3) (9): Embedding(4, 3) (10): Embedding(4, 3) (11): Embedding(24, 13) (12): Embedding(9, 5) (13): Embedding(13, 7) (14): Embedding(53, 27) (15): Embedding(22, 12) (16): Embedding(7, 4) (17): Embedding(7, 4) (18): Embedding(4, 3) (19): Embedding(4, 3) (20): Embedding(9, 5) (21): Embedding(9, 5) (22): Embedding(3, 2) (23): Embedding(3, 2) ) (emb_drop): Dropout(p=0.04) (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=229, out_features=1000, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.001) (4): Linear(in_features=1000, out_features=500, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.01) (8): Linear(in_features=500, out_features=1, bias=True) ) ) . There are 24 categorical variables and so 24 embedding layers. Embedding(53, 27) means that there are 52+1 possible values (+1 is #na#) and the size of the embedding is 27D. . There are also these extra layers in there BatchNorm1d too. These are batch normalization, another standard regularization technique. . Batch Normalization . Batch norm is a very high impact training technique that was published in 2015. . . Showing the current then state of the art ImageNet model Inception. This is how long it took them to get a pretty good result, and then they tried the same thing with batch norm, and it was a lot faster. . From the abstract of the original paper: . Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates… . Batch Normalization layer adjusts the distribution of the output of a layer by controlling the the first two moments of the layer distributions (mean and standard deviation). This allows networks to be trained with a higher learning rate (so they train faster) and with more layers. . The algorithm: . . You have the activations from the layer $x$ going into the batch norm layer. . The first thing we do is we find the mean with those activations - sum divided by the count that is just the mean. | The second thing we do is we find the variance of those activations - a difference squared divided by the mean is the variance. | Then we normalize - the values minus the mean divided by the standard deviation is the normalized version. It turns out that bit is actually not that important. We used to think it was - it turns out it’s not. The really important bit is the next bit. | We take those values and we add a vector of biases (they call it beta here). We’ve seen that before. We’ve used a bias term before. So we’re just going to add a bias term as per usual. Then we’re going to use another thing that’s a lot like a bias term, but rather than adding it, we’re going to multiply by it. These are the parameters gamma $ gamma$ and beta $ beta$ which are learnable parameters. | Basically $ gamma$ and $ beta$ are biases. $ beta$ is just a normal bias layer and $ gamma$ is a multiplicative/scale bias layer. They are parameters and so they are learned with gradient descent. . Roughly speaking, this works by scaling a layer’s output to the size and location it needs to be in (like between 0 and 5 for a movie review). This is harder to do with just stacks of non-linear functions because of all the intricate interactions between them. Navigating that complex landscape is hard and there will be many bumps in the road. . There is one more aspect to batch norm - momentum: . BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) . This has nothing to do with momentum in optimization. This is momentum as in exponentially weighted moving average. Specifically this mean and standard deviation (in batch norm algorithm), we don’t actually use a different mean and standard deviation for every mini batch. If we did, it would vary so much that it be very hard to train. So instead, we take an exponentially weighted moving average of the mean and standard deviation. . Why Batch Normalization works is actually still a debated topic. . In the original paper they said it accelerates training by reducing something they call ‘internal covariate shift’. This is one of those things where researchers came up with some intuition and some idea about this thing they wanted to try and found that it worked well. They then look for an explanation after the fact. So the original explanation for why it works may well be wrong. In this paper - How Does Batch Normalization Help Optimization? - they have an alternative explanation: . . The above is from this paper. The plot represents the ‘loss landscape’ of the network during training. The red line is what happens whn you train without Batch Norm - very very bumpy. The blue line is training with batch norm - a lot smoother. If the loss landscape is very bumpy then your model can get trapped in some awful region of parameter space that it can’t escape from. If it is smoother then you can train with a higher learning rate and hence converge faster. . Other points of view: . An influential twitter thread on how Batch Norm works that vindicates the Internal Covariate Shift explanation: Twitter thread. | Blog post with analysis of the different points of view: https://arthurdouillard.com/post/normalization/ | . Why it works is still debatable and I need to read more into this, but this aside, it seems that the output distribution of the hidden layers in the network is very important for training networks more quickly and with more layers. We already know that these internal distributions are very important for training because of all the research done into the art of initializing neural networks when training from scratch. Getting this wrong can prevent the network from training at all by making gradients vanish or explode. So with this in mind, it makes sense that adjusting these distributions as data flows through the network could improve training. . Data Augmentation . One of the most effective and least studied forms of regularization is data augmentation. . Here is a link to the notebook that explores data augmentation in computer vision: lesson6-pets-more.ipynb. . I recommend reading the fastai documentation on data augmentation for computer vision: vision.transform. . In particular, read the list of transforms. . The data augmentation you pick should be realistic of what you expect in the dataset and problem domain. | . How do convolutions work? . Convolutions are like a special kind of matrix multiply. . Checkout the website: http://setosa.io/ev/image-kernels/: . . Post from Matthew Kleinsmith: CNNs from Different Viewpoints. This is a wonderfully concise explanation with great diagrams and hardly any text. The following diagrams are from that post. . Sliding window view: . . You can alternatively think of it as a a set of linear equations. . . You can also think of it as a fully connected neural network. In the following the colour of the links stand for their weight, and the gray links are 0. . . You can also interpret it as a matrix multiply: . . Banded matrix multiply where the colours again stand for the weights. $b$ is a bias term. . We have to also consider padding: . . This diagram uses zero padding, but it could be reflection padding or whatever. . So a single convolution kernel is a small matrix of weights (typical sized 3 to 7) and a bias. In a convolutional layer the same convolution is applied to every channel of input. If you take example of a colour image there the image is 3x224x224 in size. The 2D convolutional kernal will be applied to all 3 channels simultaneously and the results from all 3 is summed to produce a single number for each pixel. . . If you have multiple convolutions then you have multiple different outputs. We stack these together to make another tensor: . . This output can also be fed into another convolution layer, and so on. . In order to avoid our memory going out of control, from time to time we create a convolution where we don’t step over every single set of 3x3, but instead we skip over two at a time. We would start with a 3x3 centered at (2, 2) and then we’d jump over to (2, 4), (2, 6), (2, 8), and so forth. That’s called a stride 2 convolution. What that does is, it looks exactly the same, it’s still just a bunch of kernels, but we’re just jumping over 2 at a time. We’re skipping every alternate input pixel. So the output from that will be H/2 by W/2. When we do that, we generally create twice as many kernels, so we can now have 32 activations in each of those spots. That’s what modern convolutional neural networks tend to look like. . The learn.summary() of a resnet it looks like this: . ====================================================================== Layer (type) Output Shape Param # Trainable ====================================================================== Conv2d [64, 176, 176] 9,408 False ______________________________________________________________________ BatchNorm2d [64, 176, 176] 128 True ______________________________________________________________________ ReLU [64, 176, 176] 0 False ______________________________________________________________________ MaxPool2d [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ ReLU [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ ReLU [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ ReLU [64, 88, 88] 0 False ______________________________________________________________________ Conv2d [64, 88, 88] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 88, 88] 128 True ______________________________________________________________________ Conv2d [128, 44, 44] 73,728 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 8,192 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ ReLU [128, 44, 44] 0 False ______________________________________________________________________ Conv2d [128, 44, 44] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 44, 44] 256 True ______________________________________________________________________ Conv2d [256, 22, 22] 294,912 False ______________________________________________________________________ BatchNorm2d [256, 22, 22] 512 True . There are stacks of convolutional layers and every so often it halves the grid size and doubles the number of channels. . Manual Convolution . Input image: . . Convolve with tensor: . k = tensor([ [0. ,-5/3,1], [-5/3,-5/3,1], [1. ,1 ,1], ]).expand(1,3,3,3)/6 . The images has 3 channels so we need to expand the tensor to replicate the kernel 3 times. We also add in an additional unit dimension because PyTorch expects to work with mini-batches always so this way it has the right tensor rank. . You then take that image t and convolve it using PyTorch: . edge = F.conv2d(t[None], k) . . CNN Heatmap Example . . This is covered in the notebook: lesson6-pets-more.ipynb. . Ethics and Data Science . I won’t cover this here. Instead just watch the video, it’s great: ethics and data science (YouTube) Also here is the transcription of this section by @hiromi . Jeremy Says… . Not an explicit “do this” but it feels like it fits here. “One of the big opportunities for research is to figure out how to do data augmentation for different domains. Almost nobody is looking at that and to me it is one of the biggest opportunities that could let you decrease data requirements by 5-10x.” Lesson 6: Data augmentation on inputs that aren’t images 26 | In context of data augmentation: reflection mode padding almost always works best. | If you take your time going through the convolution kernel section and the heatmap section of this notebook, running those lines of code and changing them around a bit. The most important thing to remember is shape (rank and dimensions of tensor). Try to think “why?”. Try going back to the printout of the summary, the list of the actual layers, the picture we drew and think about what’s going on. Lesson 6: Go through the convolution kernel and heatmap notebook 11 | (Source: Robert Bracco) . Q &amp; A . In what proportion would you use dropout vs. other regularization errors, like, weight decay, L2 norms, etc.? [54:49]: . So remember that L2 regularization and weight decay are kind of two ways of doing the same thing? We should always use the weight decay version, not the L2 regularization version. So there’s weight decay. There’s batch norm which kind of has a regularizing effect. There’s data augmentation which we’ll see soon, and there’s dropout. So batch norm, we pretty much always want. So that’s easy. Data augmentation, we’ll see in a moment. So then it’s really between dropout versus weight decay. I have no idea. I don’t think I’ve seen anybody to provide a compelling study of how to combine those two things. Can you always use one instead of the other? Why? Why not? I don’t think anybody has figured that out. I think in practice, it seems that you generally want a bit of both. You pretty much always want some weight decay, but you often also want a bit of dropout. But honestly, I don’t know why. I’ve not seen anybody really explain why or how to decide. So this is one of these things you have to try out and kind of get a feel for what tends to work for your kinds of problems. I think the defaults that we provide in most of our learners should work pretty well in most situations. But yeah, definitely play around with it. . | . Links and References . Link to Lesson 6 lecture. | Parts of my notes were copied from the excellent lecture transcriptions made by @hiromi: Detailed lesson notes. | Homework notebooks: lesson6-rossmann.ipynb | rossman_data_clean.ipynb | lesson6-pets-more.ipynb | . | CNNs from Different Viewpoints | Blog post about different kinds of Normalization. | Cross-entropy loss | Lecture on BackProp going deeper into how it works from A. Karpathy: https://www.youtube.com/watch?v=i94OvYb6noo | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/09/01/fast-ai-lesson-6-notes-cnn-deep-dive-ethics.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/09/01/fast-ai-lesson-6-notes-cnn-deep-dive-ethics.html",
            "date": " • Sep 1, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Fast.ai v3 Lesson 5 Notes: Backpropagation; Neural Network From Scratch",
            "content": "Overview of the Lesson . This lesson looks at the fundament components of deep learning - parameters, activations, backpropagation, transfer learning, and discriminative learning rates. . Components of Deep Learning . Roughly speaking, this is the bunch of concepts that we need to learn about - . Inputs | Weights/parameters | Random | Activations | Activation functions / nonlinearities | Output | Loss | Metric | Cross-entropy | Softmax | Fine tuning | Layer deletion and random weights | Freezing &amp; unfreezing | . Diagram of a neural network: . . There are three types of layer in a NN: . Input. . | Weights/Parameters: These are layers that contain parameters or weights. These are things like matrices or convolutions. Parameters are used by multiplying them by input activations doing a matrix product. The yellow things in the above diagram are our weight matrices / weight tensors. Parameters are the things that your model learns in train via gradient descent: . weights = weights - learning_rate * weights.grad . | Activations: These are layers that contain activations, also called as non-linear layers which are stateless. For example, ReLu, softmax, or sigmoid. . | Here is the process of input, weight multiplication, and activation up close (image credit): . . The parameters/weights are the matrix $ mathbf{W}$, the input is the vector $x$, and there is also the bias vector $b$. This can be expressed mathematically as: ( mathbf{a} = g( mathbf{W^T} mathbf{x} + mathbf{b}) ) Let’s get an intuition for how the dimension of the data changes as it flows through the network. In the diagram above there is an input vector of size 4 and a weight matrix of size 4x3. The matrix vector product in terms of just the dimensions is: $(4, 3)^T cdot (4) = (3, 4) cdot (4) = (3)$. . In summation notation this is $(W_{ji})^Tx_j = W_{ij} x_j = a_i$. The $j$ terms are summed out and we are left with $i$ dimension only. . The activation function is an element-wise function. It’s a function that is applied to each element of the input, activations in turn and creates one activation for each input element. If it starts with a twenty long vector it creates a twenty long vector by looking at each one of those, in turn, doing one thing to it and spitting out the answer, so an element-wise function. These days the activation function most often used is ReLu. . Backpropagation . After the loss has been calculated from the different between the output and the ground truth, how are millions of parameters in the network then updated? This is done by a clever algorithm called backpropogation. This algorithm calculates the partial derivatives of the loss with respect to every parameter in the network. It does this using the chain-rule from calculus. The best explanation of this I’ve seen is from Chris Olah’s blog. . In PyTorch, these derivatives are calculated automatically for you (aka autograd) and the gradient of any PyTorch variable is stored in its .grad attribute. . Mathematically the weights are updated like so: (w_t = w_{t-1} - lr times frac{ partial L}{ partial w_{t-1}} ) . How is Transfer Learning Done? . What happens when we take a resnet34 trained on ImageNet and we do transfer learning? How can a network that is trained to identify 1000 different everyday objects be repurposed for, say, identifying galaxies? . Let’s look at some examples we’ve seen already. Here are is the last layer group of the resnet34 used in the dog/cat breed example: . (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5) (8): Linear(in_features=512, out_features=37, bias=True) ) . And here is the same layer group from the head pose example: . (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5) (8): Linear(in_features=512, out_features=2, bias=True) ) . The two layers that change here are (4) and (8). . Layer (4) is a matrix with some default size that is the same in both cases. It anyway needs to be relearned for a new problem. | Layer (8) is different in out_features. This is the dimensionality of the output. There are 37 cat/dog breeds in the first example, and there are x and y coordinates in the second. Layer (8) is a Linear layer – i.e. it’s a matrix! | . So to do transfer learning we just need to swap out the last matrix with a new one. . . For an imagenet network this matrix would have shape (512, 1000), for the cat/dog (512, 37), and for the head pose (512, 2). Every row in this matrix represents a category you are predicting. . Fastai library figures out the right size for this matrix by looking at the data bunch you passed to it. . Freezing Layers . . The new head of the network has two randomly initialized matrices. They need to be trained because they are random, however the rest of the network is quite well trained on imagenet already - they are vastly better than random even though they weren’t trained on the task at hand. . So we freeze all the layers before the head, which means we don’t update their parameters during training. It’ll be a little bit faster because there are fewer calculations to do, and also it will save some memory because there are fewer gradients to store. . After training a few epochs with just the head unfrozen, we are ready to train the whole network. So we unfreeze everything. . Discriminative Learning Rates . Now we’re going to train the whole thing but we still have a pretty good sense that these new layers we added to the end probably need more training and these ones right at the start that might just be like diagonal edges probably don’t need much training at all. . Let’s review what the different layers in a CNN learn to do first. The first layer visualized looks like this: . . This layer is good at finding diagonal lines in different directions. . . In layer 2 some of the filters were good at spotting corners in the bottom right corner. . . In layer 3, one of the filters found repeating patterns or round orange things or fluffy or floral textures. . . As we go deeper, they’re becoming more sophisticated but also more specific. By layer 5, It could find bird eyeballs or dog faces. . If you’re wanting to transfer and learn to something for galaxy morphology there’s probably going to be no eyeballs in that dataset. So the later layers are no good to you but there will certainly be some repeating patterns or some diagonal edges. The earlier a layers is in the pretrained model the more likely it is that you want those weights to stay as they are. . We can implement this by splitting the model into a few sections and giving the earlier sections slower learning rates. Earlier layers of the model we might give a learning rate of 1e - 5 and newly added layers of the model we might give a learning rate of 1e - 3. What’s gonna happen now is that we can keep training the entire network. But because the learning rate for the early layers is smaller it’s going to move them around less because we think they’re already pretty good. If it’s already pretty good to the optimal value if you used a higher learning rate it could kick it out. It could actually make it worse which we really don’t want to happen. . In fastai this is done with any of the following lines of code: . learn.fit_one_cycle(5, 1e-3) learn.fit_one_cycle(5, slice(1e-3)) learn.fit_one_cycle(5, slice(1e-5, 1e-3)) . These mean: . A single number like 1e-3: Just using a single number means every layer gets the same learning rate so you’re not using discriminative learning rates. | A slice with a single number slice(1e-3):If you pass a single number to slice it means the final layers get a learning rate of 1e-3 and then all the other layers get the same learning rate which is that divided by 3. All of the other layers will be (1e-3)/3 and the last layers will be 1e-3. | A slice with two numbers, slice(1e-5, 1e-3) In the last case, the final layers the these randomly hidden added layers will still be again 1e-3. The first layers will get 1e-5. The other layers will get learning rates that are equally spread between those two. Multiplicatively equal. If there were three layers there would be 1e-5, 1e-4 and 1e-3. Multiplied by the same factor between layers each time. | This divided by 3 thing that is a little weird and we won’t talk about why that is until part two of the course. it is specific quirk around batch normalization. . Collaborative Filtering: Deep Dive . Recall from the previous lesson the movie recommendation problem. . . For the movielens dataset the model looks like: . $&gt; learn.model EmbeddingDotBias( (u_weight): Embedding(944, 40) (i_weight): Embedding(1654, 40) (u_bias): Embedding(944, 1) (i_bias): Embedding(1654, 1) ) . We already know what u_weight and i_weight are: the user and movie embedding matrices, respectively. What are the additional components of the model: u_bias and i_bias? . Interpreting Bias . These are the 1D bias terms for the movies and films. In the model diagram above these are the two edge nodes on the left and right that connect to the ADD node. . Every user and every movie has its own bias term. . You can interpret user bias as how much that user likes movies. . | You can interpret movie bias as how well liked a movie is in general. (Ironically, the bias is the unbiased movie score). . | . Here are the movies with the highest bias alongside their actual movie rating: . movie_bias = learn.bias(top_movies, is_item=True) . Top 10: . [(tensor(0.6105), &quot;Schindler&#39;s List (1993)&quot;, 4.466442953020135), (tensor(0.5817), &#39;Titanic (1997)&#39;, 4.2457142857142856), (tensor(0.5685), &#39;Shawshank Redemption, The (1994)&#39;, 4.445229681978798), (tensor(0.5451), &#39;L.A. Confidential (1997)&#39;, 4.161616161616162), (tensor(0.5350), &#39;Rear Window (1954)&#39;, 4.3875598086124405), (tensor(0.5341), &#39;Silence of the Lambs, The (1991)&#39;, 4.28974358974359), (tensor(0.5330), &#39;Star Wars (1977)&#39;, 4.3584905660377355), (tensor(0.5227), &#39;Good Will Hunting (1997)&#39;, 4.262626262626263), (tensor(0.5114), &#39;As Good As It Gets (1997)&#39;, 4.196428571428571), (tensor(0.4800), &#39;Casablanca (1942)&#39;, 4.45679012345679), (tensor(0.4698), &#39;Boot, Das (1981)&#39;, 4.203980099502488), (tensor(0.4589), &#39;Close Shave, A (1995)&#39;, 4.491071428571429), (tensor(0.4567), &#39;Apt Pupil (1998)&#39;, 4.1), (tensor(0.4566), &#39;Vertigo (1958)&#39;, 4.251396648044692), (tensor(0.4542), &#39;Godfather, The (1972)&#39;, 4.283292978208232)] . Bottom 10: . [(tensor(-0.4076), &#39;Children of the Corn: The Gathering (1996)&#39;, 1.3157894736842106), (tensor(-0.3053), &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, 1.7142857142857142), (tensor(-0.2892), &#39;Cable Guy, The (1996)&#39;, 2.339622641509434), (tensor(-0.2856), &#39;Mortal Kombat: Annihilation (1997)&#39;, 1.9534883720930232), (tensor(-0.2530), &#39;Striptease (1996)&#39;, 2.2388059701492535), (tensor(-0.2405), &#39;Free Willy 3: The Rescue (1997)&#39;, 1.7407407407407407), (tensor(-0.2361), &#39;Showgirls (1995)&#39;, 1.9565217391304348), (tensor(-0.2332), &#39;Bio-Dome (1996)&#39;, 1.903225806451613), (tensor(-0.2279), &#39;Crow: City of Angels, The (1996)&#39;, 1.9487179487179487), (tensor(-0.2273), &#39;Barb Wire (1996)&#39;, 1.9333333333333333), (tensor(-0.2246), &quot;McHale&#39;s Navy (1997)&quot;, 2.1884057971014492), (tensor(-0.2197), &#39;Beverly Hills Ninja (1997)&#39;, 2.3125), (tensor(-0.2178), &quot;Joe&#39;s Apartment (1996)&quot;, 2.2444444444444445), (tensor(-0.2080), &#39;Island of Dr. Moreau, The (1996)&#39;, 2.1578947368421053), (tensor(-0.2064), &#39;Tales from the Hood (1995)&#39;, 2.037037037037037)] . Having seen many of the films in both these lists, I’m not not surprise by what I see here! . Interpreting Weights (the Embeddings) . Grab the weights: . $&gt; movie_w = learn.weight(top_movies, is_item=True) $&gt; movie_w.shape torch.Size([1000, 40]) . There are 40 factors in the model. Looking at the 40 latent factors isn’t intuitive or necessarily meaningful so it’s better to squish the 40 factors down to 3 - using Principle Component Analysis (PCA). . $&gt; movie_pca = movie_w.pca(3) $&gt; movie_pca.shape torch.Size([1000, 3]) . We can now rank each of the films along these 3 dimensions, knowing film, interpret what these dimensions mean. . Factor 0 . fac0,fac1,fac2 = movie_pca.t() movie_comp = [(f, i) for f,i in zip(fac0, top_movies)] . Top 10: . [(tensor(1.1000), &#39;Wrong Trousers, The (1993)&#39;), (tensor(1.0800), &#39;Close Shave, A (1995)&#39;), (tensor(1.0705), &#39;Casablanca (1942)&#39;), (tensor(1.0304), &#39;Lawrence of Arabia (1962)&#39;), (tensor(0.9957), &#39;Citizen Kane (1941)&#39;), (tensor(0.9792), &#39;Some Folks Call It a Sling Blade (1993)&#39;), (tensor(0.9778), &#39;Persuasion (1995)&#39;), (tensor(0.9752), &#39;North by Northwest (1959)&#39;), (tensor(0.9706), &#39;Wallace &amp; Gromit: The Best of Aardman Animation (1996)&#39;), (tensor(0.9703), &#39;Chinatown (1974)&#39;)] . Bottom 10: . [(tensor(-1.2963), &#39;Home Alone 3 (1997)&#39;), (tensor(-1.2210), &quot;McHale&#39;s Navy (1997)&quot;), (tensor(-1.2199), &#39;Leave It to Beaver (1997)&#39;), (tensor(-1.1918), &#39;Jungle2Jungle (1997)&#39;), (tensor(-1.1209), &#39;D3: The Mighty Ducks (1996)&#39;), (tensor(-1.0980), &#39;Free Willy 3: The Rescue (1997)&#39;), (tensor(-1.0890), &#39;Children of the Corn: The Gathering (1996)&#39;), (tensor(-1.0873), &#39;Bio-Dome (1996)&#39;), (tensor(-1.0436), &#39;Mortal Kombat: Annihilation (1997)&#39;), (tensor(-1.0409), &#39;Grease 2 (1982)&#39;)] . Interpretation: This dimension is best described as ‘connoisseur movies’. . Factor 1 . Top 10: . [(tensor(1.1052), &#39;Braveheart (1995)&#39;), (tensor(1.0759), &#39;Titanic (1997)&#39;), (tensor(1.0202), &#39;Raiders of the Lost Ark (1981)&#39;), (tensor(0.9324), &#39;Forrest Gump (1994)&#39;), (tensor(0.8627), &#39;Lion King, The (1994)&#39;), (tensor(0.8600), &quot;It&#39;s a Wonderful Life (1946)&quot;), (tensor(0.8306), &#39;Pretty Woman (1990)&#39;), (tensor(0.8271), &#39;Return of the Jedi (1983)&#39;), (tensor(0.8211), &quot;Mr. Holland&#39;s Opus (1995)&quot;), (tensor(0.8205), &#39;Field of Dreams (1989)&#39;)] . Bottom 10: . [(tensor(-0.8085), &#39;Nosferatu (Nosferatu, eine Symphonie des Grauens) (1922)&#39;), (tensor(-0.8007), &#39;Brazil (1985)&#39;), (tensor(-0.7866), &#39;Trainspotting (1996)&#39;), (tensor(-0.7703), &#39;Ready to Wear (Pret-A-Porter) (1994)&#39;), (tensor(-0.7173), &#39;Beavis and Butt-head Do America (1996)&#39;), (tensor(-0.7150), &#39;Serial Mom (1994)&#39;), (tensor(-0.7144), &#39;Exotica (1994)&#39;), (tensor(-0.7129), &#39;Lost Highway (1997)&#39;), (tensor(-0.7094), &#39;Keys to Tulsa (1997)&#39;), (tensor(-0.7083), &#39;Jude (1996)&#39;)] . Interpretation: This dimension I think can be best described as ‘blockbuster’, or ‘family’. . Factor 2 . Top 10: . [(tensor(1.0152), &#39;Beavis and Butt-head Do America (1996)&#39;), (tensor(0.8703), &#39;Reservoir Dogs (1992)&#39;), (tensor(0.8640), &#39;Pulp Fiction (1994)&#39;), (tensor(0.8582), &#39;Terminator, The (1984)&#39;), (tensor(0.8572), &#39;Scream (1996)&#39;), (tensor(0.8058), &#39;Terminator 2: Judgment Day (1991)&#39;), (tensor(0.7728), &#39;Seven (Se7en) (1995)&#39;), (tensor(0.7457), &#39;Starship Troopers (1997)&#39;), (tensor(0.7243), &#39;Clerks (1994)&#39;), (tensor(0.7227), &#39;Die Hard (1988)&#39;)] . Bottom 10: . [(tensor(-0.6912), &#39;Lone Star (1996)&#39;), (tensor(-0.6720), &#39;Jane Eyre (1996)&#39;), (tensor(-0.6613), &#39;Steel (1997)&#39;), (tensor(-0.6227), &#39;Piano, The (1993)&#39;), (tensor(-0.6183), &#39;My Fair Lady (1964)&#39;), (tensor(-0.5946), &#39;Evita (1996)&#39;), (tensor(-0.5827), &#39;Home for the Holidays (1995)&#39;), (tensor(-0.5669), &#39;Cinema Paradiso (1988)&#39;), (tensor(-0.5668), &#39;All About Eve (1950)&#39;), (tensor(-0.5586), &#39;Sound of Music, The (1965)&#39;)] . Regularization: Weight Decay . . On the left the model isn’t complex enough. One the right the model has more parameters and is too complex. The middle model has just the right level of complexity. . In Jeremy’s opinion, to reduce model complexity: . In statistics: they reduce the number of parameters. Jeremy thinks this is wrong. | In machine learning: they increase the number of parameters, but increase the amount of regularization. | . The regularization used in fastai is weight decay (aka L2 regularization). This is where the loss function is modified with an extra component that penalizes extreme values of the weights. . Unregularized loss: | . [ begin{align} L(x, w) &amp;= mse( hat{y}, y) nonumber &amp;= mse(m(x, w), y) . end{align}] . L2 regularized loss: | . [L(x, w) = mse(m(x,w), y) + lambda sum w^2] . The weight decay parameter $ lambda$ corresponds to the fastai parameter to the Learner model: wd . This is set by default to 0.01. . Higher values of wd give more regularlization. Looking at the diagram of line fits above, more regularlization makes the line ‘stiffer’ - harder to force through every point. . | Lower values of wd, conversely, make the line ‘slacker’ - it’s easier to force it through every point. . | It’s worth experimenting with wd if your model is overfitting or underfitting. . | . With weight decay the update formula for the weights in gradient descent becomes: . [w_t = w_{t-1} - lr times frac{ partial L}{ partial w_{t-1}} - lr times lambda w_{t-1}] . (Source) . Jeremy Says… . The answer to the question “Should I try blah?” is to try blah and see, that’s how you become a good practitioner. Lesson 5: Should I try blah? | If you want to play around, try to create your own nn.linear class. You could create something called My_Linear and it will take you, depending on your PyTorch experience, an hour or two. We don’t want any of this to be magic and you know everything necessary to create this now. These are the things you should be doing for assignments this week, not so much new applications but trying to write more of these things from scratch and get them to work. Learn how to debug them and check them to see what’s going in and coming out. Lesson 5 Assignment: Create your own version of nn.linear | A great assignment would be to take Lesson 2 SGD and try to add momentum to it. Or even the new notebook we have for MNIST, get rid of the Optim.SGD and write your own update function with momentum Lesson 5: Another suggested assignment | It’s definitely worth knowing that taking layers of neural nets and chucking them through PCA is very often a good idea. Because very often you have way more activations than you want in a layer, and there’s all kinds of reasons you would might want to play with it. For example, Francisco who’s sitting next to me today has been working on something to do with image similarity. And for image similarity, a nice way to do that is to compare activations from a model, but often those activations will be huge and therefore your thing could be really slow and unwieldy. So people often, for something like image similarity, will chuck it through a PCA first and that’s kind of cool. | (Source - Robert Bracco) . Q &amp; A . When we load a pre-trained model, can we explore the activation grids to see what they might be good at recognizing? [36:11] . Yes, you can. And we will learn how to (should be) in the next lesson. . | Can we have an explanation of what the first argument in fit_one_cycle actually represents? Is it equivalent to an epoch? . Yes, the first argument to fit_one_cycle or fit is number of epochs. In other words, an epoch is looking at every input once. If you do 10 epochs, you’re looking at every input ten times. So there’s a chance you might start overfitting if you’ve got lots of lots of parameters and a high learning rate. If you only do one epoch, it’s impossible to overfit, and so that’s why it’s kind of useful to remember how many epochs you’re doing. . | What is an affine function? . An affine function is a linear function. I don’t know if we need much more detail than that. If you’re multiplying things together and adding them up, it’s an affine function. I’m not going to bother with the exact mathematical definition, partly because I’m a terrible mathematician and partly because it doesn’t matter. But if you just remember that you’re multiplying things together and then adding them up, that’s the most important thing. It’s linear. And therefore if you put an affine function on top of an affine function, that’s just another affine function. You haven’t won anything at all. That’s a total waste of time. So you need to sandwich it with any kind of non-linearity pretty much works - including replacing the negatives with zeros which we call ReLU. So if you do affine, ReLU, affine, ReLU, affine, ReLU, you have a deep neural network. . | Why am I sometimes getting negative loss when training? [59:49] . You shouldn’t be. So you’re doing something wrong. Particularly since people are uploading this, I guess other people have seen it too, so put it on the forum. We’re going to be learning about cross entropy and negative log likelihood after the break today. They are loss functions that have very specific expectations about what your input looks like. And if your input doesn’t look like that, then they’re going to give very weird answers, so probably you press the wrong buttons. So don’t do that. . | . Links and References . Link to Lesson 5 lecture | Parts of my notes were copied from the excellent lecture transcriptions made by @PoonamV: Lecture notes | Homework notebooks: Notebook 1: SGD and MNIST | Notebook 2: Rossmann (tabular data) | . | Netflix and Chill: Building a Recommendation System in Excel - Latent Factor Visualization in Excel blog post | An overview of gradient descent optimization algorithms - Sebastian Ruder | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/29/fast-ai-lesson-5-notes-backpropagation-neural-network-from-scratch.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/29/fast-ai-lesson-5-notes-backpropagation-neural-network-from-scratch.html",
            "date": " • Aug 29, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Fast.ai v3 Lesson 4 Notes: NLP; Tabular Data; Recommenders",
            "content": "Overview of the Lesson . The first part of this lesson dives into natural language processing (NLP), using the IMDB movie review dataset. We train a classifier that categorises if a review is negative or positive. This is called sentiment analysis. This is done via a state-of-the-art NLP algorithm called ULMFiT. . Next the lesson shows how to use deep learning with tabular data using fastai. . Lastly the lesson shows how collaborative filtering models (aka recommender systems) can be built using similar ideas to those for tabular data, but with some special tricks to get both higher accuracy and more informative model interpretation. . __Table of Contents__ * TOC {:toc} Natural Language Processing (NLP) . We want to build a NLP classifier. | Task: IMDB movie reviews - postive or negative? | Using neural networks for NLP classification hasn’t been successful until a break through made in 2018 – ULMFit. This is what FastAI is using now. | . . Just as we have seen already in imaging problems, we can get good performance by using transfer learning. | In NLP transfer learning means taking a language model which has be pretrained on some large corpus of text and then fine tuning that for our current problem using its own text corpus. | . Language Model . The language model in this case is a special type of neural network called an RNN (recurrent neural network) and what it does is predict the next word given a sequence of prior words. So in the diagram above you have the sentences: “I’d like to eat a hot [ ]” : the language model should predict “dog” | “It was a hot [ ]” : the language model should predict “day” | . | This takes 2-3 days to train on a decent GPU, so not much point in you doing it. You may as well start with ours. Even if you’ve got a big corpus of like medical documents or legal documents, you should still start with Wikitext 103. There’s just no reason to start with random weights. It’s always good to use transfer learning if you can. | Once you have trained your language model you can stick it on the internet (e.g. github) for others to download and use for their own NLP problems. fastai provides a pretrain language model trained on text from Wikipedia. | This kind of learning is what Yann Lecun calls “Self-supervised Learning”. You don’t give the dataset labels, rather the labels are built into the data itself. | . Fine Tuning the Language Model . Starting from the pretrained Wikitext language model you can fine tune the language model with your own target corpus. Every domain that you work in will have its own domain specific language that it uses. . | For the case of movie reviews it may learn about actor’s names or certain vocabulary will be more important. For example: “My favourite actor is Tom ___ (Cruise)” | “I thought the photography was fantastic but I wasn’t really so happy about the _____ (director).” | . | Fine tuning your language model will take a long time. However this is basically a one-time cost. You only have to train the language model once and then you can use that model for training classifiers or whatever, which won’t take a long time to train. | This transfer learning approach works very well and gives state of the art performance on the IMDB dataset. | . IMDB Sentiment Classification . The data loading process for text was covered in the previous lesson. Here is a short review: . Load data using a data bunch or the data block API | The data is tokenized: this means that text is split into raw words or ‘tokens’. Special tokens denote puncuation, unknown words etc. | The tokenized data is then numericalized: every token is assigned its own unique number. A text document becomes a list of numbers, which can be processed by a neural network. | This data loading and transforming is achieved in fastai with the data block API: . data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) .split_from_df(col=2) .label_from_df(cols=0) .databunch()) . Training the Language Model . No point training the Wikitext 103 model from scratch just download the pretrained one from fastai. Instead we want to start with that a fine tune it with the IMDB corpus. First we load the IMDB data for language model learning: . bs=48 data_lm = (TextList.from_folder(path) #Inputs: all the text files in path .filter_by_folder(include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) #We may have other temp folders that contain text files so we only keep what&#39;s in train and test .split_by_rand_pct(0.1) #We randomly split and keep 10% (10,000 reviews) for validation .label_for_lm() #We want to do a language model so we label accordingly .databunch(bs=bs)) data_lm.save(&#39;data_lm.pkl&#39;) . We can say: . It’s a list of text files﹣the full IMDB actually is not in a CSV. Each document is a separate text file. | Say where it is﹣in this case we have to make sure we just to include the train and test folders. | We randomly split it by 0.1. | . This data looks like: . . You then train the language model, not using a CNN rather a Recursive Neural Network (RNN). In fastai the code is: . learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3) . The pretrained language model that comes from fastai is AWD_LSTM: link. . You then do usual routine for training: . Run LRFind | Train the network head (1-2 epochs) | Unfreeze | Run LRFind again | Train the whole network (5+ epochs). | Save the encoder: learn.save_encoder(&#39;fine_tuned_enc&#39;) | . Predicting Text with the Language Model . With the trained language model we can have some fun by making it finish sentences. . TEXT = &quot;I liked this movie because&quot; N_WORDS = 40 N_SENTENCES = 2 print(&quot; n&quot;.join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES))) . The output of this: . I liked this movie because of the cool scenery and the high level of xxmaj british hunting . xxmaj the only thing this movie has going for it is the horrible acting and no script . xxmaj the movie was a big disappointment . xxmaj I liked this movie because it was one of the few movies that made me laugh so hard i did n’t like it . xxmaj it was a hilarious film and it was very entertaining . xxmaj the acting was great , i ‘m . Text Classifier . Load the data: . data_clas = (TextList.from_folder(path, vocab=data_lm.vocab) #grab all the text files in path .split_by_folder(valid=&#39;test&#39;) #split by train and valid folder (that only keeps &#39;train&#39; and &#39;test&#39; so no need to filter) .label_from_folder(classes=[&#39;neg&#39;, &#39;pos&#39;]) #label them all with their folders .databunch(bs=bs)) . Create a text classifer and give it the language model we trained: . learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) learn.load_encoder(&#39;fine_tuned_enc&#39;) # load language model . Tabular Data . Tabular data is one of the most common problems that data scientists work on day-to-day. This are things like spreadsheets, relational databases, or financial reports. People used to be sceptical about using neural networks for tabular data - everybody knows you should be using XGBoost! However not only does it work well, it can do things that even XGBoost can’t do. . fastai has created the module fastai.tabular for using NNs with tabular data. . Loading the Data . Import the fastai modules: . from fastai import * from fastai.tabular import * . The data input is assumed to be a pandas dataframe. Here is the Adult dataset, which is a classic dataset where you have to predict somebody’s salary given a number of variables like age, education, occupation etc: . path = untar_data(URLs.ADULT_SAMPLE) df = pd.read_csv(path/&#39;adult.csv&#39;) . For fastai’s tabular models you need to tell it about your columns: . Which column is the target variable? | Which columns have continuous variables? | Which columns have categorical variables? | What preprocessing do you want to do to the columns? | In code these variables look like: . dep_var = &#39;salary&#39; cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] procs = [FillMissing, Categorify, Normalize] . Using these we can then load the data using data block API: . data = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs) .split_by_idx(list(range(800,1000))) .label_from_df(cols=dep_var) .add_test(test, label=0) .databunch()) . There are a number of processors in the fastai library. The ones we’re going to use this time are: . FillMissing: Look for missing values and deal with them some way (e.g. mean, median…). | Categorify: Find categorical variables and turn them into Pandas categories | Normalize : Do a normalization ahead of time which is to take continuous variables and subtract their mean and divide by their standard deviation so they are zero-one variables. | . For the full list of transforms available see the documentation. . Training the Model . learn = tabular_learner(data, layers=[200,100], metrics=accuracy) . learn.fit(1, 1e-2) Total time: 00:03 epoch train_loss valid_loss accuracy 1 0.362837 0.413169 0.785000 (00:03) . This creates a tabular_learner network with the parameter layers=[200, 100]. What is this exactly? If you look at model in pytorch, learn.model: . TabularModel( (embeds): ModuleList( (0): Embedding(10, 6) (1): Embedding(17, 8) (2): Embedding(8, 5) (3): Embedding(16, 8) (4): Embedding(7, 5) (5): Embedding(6, 4) (6): Embedding(3, 3) ) (emb_drop): Dropout(p=0.0) (bn_cont): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): Linear(in_features=42, out_features=200, bias=True) (1): ReLU(inplace) (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Linear(in_features=200, out_features=100, bias=True) (4): ReLU(inplace) (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (6): Linear(in_features=100, out_features=2, bias=True) ) ) . The tabular learner is a just a multi-layer perceptron (MLP) (the layers group) with some funny input bolted onto the front of it. In the layers group you can see the first two Linear layers have an output size of 200 and 100, respectively. These are the sizes we put into the layers parameter in the model. So it’s a two layer MLP with layer sizes of 200 and 100. . The input layer consists of a bunch of Embedding layers. We’ll explain these later, but basically for each of the categorical features in the data (there are 6 here) there is an embedding layer. An embedding maps the total number of unique values of a categorical variable to a lower dimensional continuous vector space. If you take the zeroth embedding layer as an example: . (0): Embedding(10, 6) . This variable has 9 unique values + 1 null value added by the fastai processors. Its output is 6 dimensional. . All of the outputs of the embedding layers are concatenated together along with the 3 continuous features to create a 42 dimensional vector that is the input to the MLP part of the network. . Collaborative Filtering . Collaborative filtering is where you have many users and many items and you want to predict how much a certain user is going to like a certain item. You have historical information about who bought what, who liked which item etc. You then want to predict what a particular user would like that they haven’t seen before. . The most basic version would be a table with userId, movieId, and rating: .   userId movieId rating timestamp . 0 | 73 | 1097 | 4.0 | 1255504951 | . 1 | 561 | 924 | 3.5 | 1172695223 | . 2 | 157 | 260 | 3.5 | 1291598691 | . 3 | 358 | 1210 | 5.0 | 957481884 | . 4 | 130 | 316 | 2.0 | 1138999234 | . The data is sparse - no single user has rated even a decent fraction of the films and many films haven’t been rated. . To achieve these aims, the problem is posed as a Matrix Factorisation problem. That is you suppose that there is some matrix that describes all the users $U$, and a matrix that describes all the movies $M$, and that the ratings of all the movies by all the users is the matrix product of these two matrices: (UM = R ) The matrices $U$ and $M$ are called the Embedding matrices. The idea is that every row of the matrix $U$ is some $D_u$ dimensional vector that represents a single user, and likewise every row of the matrix $M$ is some $D_m$ dimensional vector that represents a single movie. These vectors are such that, if I take the dot product of a user vector and movie vector it will predict the rating the user would assign that movie. . The embeddings here are the same as what we saw earlier in the categorical variables for tabular data. It’s worth taking a deeper dive into what these are. . Embeddings . Given that users and movies are only categorical variables, how do we determine how ‘far apart’ they are from each other. How similar is one film to another. How similar is one user to another? . | Often in machine learning categorical variables are represented using One-hot Encoding. . | This is where categorical variables are represented as a sparse vector, with a dimension for every unique value. For example, consider three items: . &#39;Twix&#39;: [1, 0, 0] &#39;Kit-kat&#39;: [0, 1, 0] &#39;Vodka&#39;: [0, 0, 1] . | This is often sufficient for categorical variables in machine learning algorithms. However there is a lack of meaning in these vectors. For example, all the vectors are equidistant, but I know that ‘Twix’ and ‘Kit-kat’ are both chocolate bars and so are ‘nearer’ to each other than they are to ‘Vodka’. One-hot encoding does not encode these semantics. . | This is what embeddings can do for us. An embedding is a matrix of weights. They map these one-hot vectors to a continuous vector space that encodes some meaning about the categories. In the example above this could be some 2D space of ‘foody’ things and ‘drinky’ things: . &#39;Twix&#39;: [0.98, 0] &#39;Kit-kat&#39;: [0.97, 0] &#39;Vodka&#39;: [0.1, 0.95] . | But the meaning is context dependent. The embedded space dimensions could represent anything like whether the item is expensive or whether it is more likely to be consumed at night. . | Embeddings have to be trained with supervised learning. They are initialized with random weights and then learned in collaborative filtering and in the tabular network with gradient-descent. . | . Embedding Layer as a Look-up Table . What does an embedding layer look like under the hood? text mining - How does Keras ‘Embedding’ layer work? - Cross Validated . It’s pretty much a lookup table of vectors. You have an input size of 5000 and an embedding size of 100 then you will have a list of 5000 100d vectors. You could represent this as a spare-vector dense matrix multiply, but that would be inefficient. | . Embedding Layer as matrix multiplication . The lookup, multiplication, and addition procedure we’ve just described is equivalent to matrix multiplication. Given a $1 times N$ sparse representation $S$ and an $N times M$ embedding table $E$, the matrix multiplication $S times E$ gives you the $1 times M$ dense vector. . . Different Uses of Embeddings . Embeddings map items (e.g. movies, text…) to a low dimensional dense eal vectors such that similar items are close to each other. | Embeddings can also be applied to dense data (e.g. audio) to create a meaningful similarity metric. | Jointly embedding diverse data types (e.g. text, images, audio…) can define a similarity metric between them. | . Example: Movie Lens Dataset . Link to notebook here. . First you choose some number of factors $N_f$. This is the size of the embedding. There are $N_u$ users and $N_m$ movies. You then create emedding matrices for users and movies: . User embedding matrix of size $(N_u, N_f)$ | Movie embedding matrix of size $(N_m, N_f)$ | . Note that the sizes of the embeddings for the users and movies, $N_f$, have to be the same because we are taking a dot product of them. . You can also add biases. Maybe some users just really like movies a lot more than other users. Maybe there are certain movies that everybody just likes. So in addition to the matrices you can add a single movie for how much a user likes movies, and a single number for how popular a movie is. . So the prediction of how a user would rate a movie would be the dot product of the vector from the user embedding matrix with the vector from the movie embedding matrix, plus the bias for the user and the bias for the movie. This intuitively makes sense - you have the embedded model of how users like different movies (embedding model), and then the individual characteristics of that particular user and that particular film (bias). . The rating of the movie is then calculated using a sigmoid function with a range of 0 to 5 stars. . . In fastai the code to do this is: . ratings = pd.read_csv(path/&#39;ratings.csv&#39;) data = CollabDataBunch.from_df(ratings, seed=42) y_range = [0,5.5] learn = collab_learner(data, n_factors=50, y_range=y_range) learn.fit_one_cycle(3, 5e-3) . What does the model look like? . $&gt; learn.model EmbeddingDotBias( (u_weight): Embedding(101, 50) (i_weight): Embedding(101, 50) (u_bias): Embedding(101, 1) (i_bias): Embedding(101, 1) ) . In this dataset there are 100 movies and 100 users. The inputs to the embedding layers are of size 101. This is because fastai adds in a ‘null’ category - #na#. You can see this in the CollabDataBunch object: . $&gt; data.train_ds.x.classes OrderedDict([(&#39;userId&#39;, array([&#39;#na#&#39;, &#39;15&#39;, &#39;17&#39;, &#39;19&#39;, ..., &#39;652&#39;, &#39;654&#39;, &#39;664&#39;, &#39;665&#39;], dtype=&#39;&lt;U21&#39;)), (&#39;movieId&#39;, array([&#39;#na#&#39;, &#39;1&#39;, &#39;10&#39;, &#39;32&#39;, ..., &#39;6539&#39;, &#39;7153&#39;, &#39;8961&#39;, &#39;58559&#39;], dtype=&#39;&lt;U21&#39;))]) . Cold Start Problem . If you don’t have any data on your user’s preferences then you can’t recommend them anything. There isn’t an easy solution to this; likely the only way is to have a second model which is not a collaborative filtering model but a metadata driven model for new users or new movies. A few possible approaches to tackle this problem: . Ask the user in the UX. For example Netflix proposes films and tv series to a user and asks them which ones they like so that it can bootstrap collaborative filtering. | You could use metadata about the user and the products and handcraft a crude recommendation system that way. | Jeremy Says… . If you’re doing NLP stuff, make sure you use all of the text you have (including unlabeled validation set) to train your language model, because there’s no reason not to. In Kaggle competions they don’t give you the labels for the test set, but you can still use the test data for self-supervised learning. Lesson 4: A little NLP trick | Jeremy used to use random forests / xgboost with tabular data 99% of the time. Today he uses neural networks 90% of the time. It’s his goto method he tries first. | (Source: Robert Bracco) . Q &amp; A . Does the language model approach works for text in forums that are informal English, misspelled words or slangs or shortforms like s6 instead of Samsung S 6? [12:47] . Yes, absolutely it does. Particularly if you start with your wikitext model and then fine-tune it with your “target” corpus. Corpus is just a bunch of documents (emails, tweets, medical reports, or whatever). You could fine-tune it so it can learn a bit about the specifics of the slang , abbreviations, or whatever that didn’t appear in the full corpus. So interestingly, this is one of the big things that people were surprised about when we did this research last year. People thought that learning from something like Wikipedia wouldn’t be that helpful because it’s not that representative of how people tend to write. But it turns out it’s extremely helpful because there’s a much a difference between Wikipedia and random words than there is between like Wikipedia and reddit. So it kind of gets you 99% of the way there. . So language models themselves can be quite powerful. For example there was a blog post from SwiftKey (the folks that do the mobile-phone predictive text keyboard) and they describe how they kind of rewrote their underlying model to use neural nets. This was a year or two ago. Now most phone keyboards seem to do this. You’ll be typing away on your mobile phone, and in the prediction there will be something telling you what word you might want next. So that’s a language model in your phone. . Another example was the researcher Andrej Karpathy who now runs all this stuff at Tesla, back when he was a PhD student, he created a language model of text in LaTeX documents and created these automatic generation of LaTeX documents that then became these automatically generated papers. That’s pretty cute. . We’re not really that interested in the output of the language model ourselves. We’re just interested in it because it’s helpful with this process. . | How to combine NLP (tokenized) data with meta data (tabular data) with Fastai? For instance, for IMBb classification, how to use information like who the actors are, year made, genre, etc. [49:14] . Yeah, we’re not quite up to that yet. So we need to learn a little bit more about how neural net architectures work as well. But conceptually, it’s kind of the same as the way we combine categorical variables and continuous variables. Basically in the neural network, you can have two different sets of inputs merging together into some layer. It could go into an early layer or into a later layer, it kind of depends. If it’s like text and an image and some metadata, you probably want the text going into an RNN, the image going into a CNN, the metadata going into some kind of tabular model like this. And then you’d have them basically all concatenated together, and then go through some fully connected layers and train them end to end. We will probably largely get into that in part two. In fact we might entirely get into that in part two. I’m not sure if we have time to cover it in part one. But conceptually, it’s a fairly simple extension of what we’ll be learning in the next three weeks. . | Where does the magic number of in the learning rate come from? [33:38] . learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7)) . Good question. So the learning rate is various things divided by 2.6 to the fourth. The reason it’s to the fourth, you will learn about at the end of today. So let’s focus on the 2.6. Why 2.6? Basically, as we’re going to see in more detail later today, this number, the difference between the bottom of the slice and the top of the slice is basically what’s the difference between how quickly the lowest layer of the model learns versus the highest layer of the model learns. So this is called discriminative learning rates. So really the question is as you go from layer to layer, how much do I decrease the learning rate by? And we found out that for NLP RNNs, the answer is 2.6. . How do we find out that it’s 2.6? I ran lots and lots of different models using lots of different sets of hyper parameters of various types (dropout, learning rates, and discriminative learning rate and so forth), and then I created something called a random forest which is a kind of model where I attempted to predict how accurate my NLP classifier would be based on the hyper parameters. And then I used random forest interpretation methods to basically figure out what the optimal parameter settings were, and I found out that the answer for this number was 2.6. So that’s actually not something I’ve published or I don’t think I’ve even talked about it before, so there’s a new piece of information. Actually, a few months after I did this, Stephen Merity and somebody else did publish a paper describing a similar approach, so the basic idea may be out there already. . Some of that idea comes from a researcher named Frank Hutter and one of his collaborators. They did some interesting work showing how you can use random forests to actually find optimal hyperparameters. . | How does the language model trained in this manner perform on code switched data (Hindi written in English words), or text with a lot of emojis?: . Text with emojis, it’ll be fine. There’s not many emojis in Wikipedia and where they are at Wikipedia it’s more like a Wikipedia page about the emoji rather than the emoji being used in a sensible place. But you can (and should) do this language model fine-tuning where you take a corpus of text where people are using emojis in usual ways, and so you fine-tune the Wikitext language model to your reddit or Twitter or whatever language model. And there aren’t that many emojis if you think about it. There are hundreds of thousands of possible words that people can be using, but a small number of possible emojis. So it’ll very quickly learn how those emojis are being used. So that’s a piece of cake. . I’m not really familiar with Hindi, but I’ll take an example I’m very familiar with which is Mandarin. In Mandarin, you could have a model that’s trained with Chinese characters. There are about five or six thousand Chinese characters in common use, but there’s also a romanization of those characters called pinyin. It’s a bit tricky because although there’s a nearly direct mapping from the character to the pinyin (I mean there is a direct mapping but that pronunciations are not exactly direct), there isn’t direct mapping from the pinyin to the character because one pinyin corresponds to multiple characters. . So the first thing to note is that if you’re going to use this approach for Chinese, you would need to start with a Chinese language model. . Actually fastai has something called Language Model Zoo where we’re adding more and more language models for different languages, and also increasingly for different domain areas like English medical texts or even language models for things other than NLP like genome sequences, molecular data, musical MIDI notes, and so forth. So you would you obviously start there. . To then convert that (in either simplified or traditional Chinese) into pinyin, you could either map the vocab directly, or as you’ll learn, these multi-layer models﹣it’s only the first layer that basically converts the tokens into a set of vectors, you can actually throw that away and fine-tune just the first layer of the model. So that second part is going to require a few more weeks of learning before you exactly understand how to do that and so forth, but if this is something you’re interested in doing, we can talk about it on the forum because it’s a nice test of understanding. . | Regarding using NN for Tabular data: What are the 10% of cases where you would not default to neural nets? [40:41]: . Good question. I guess I still tend to give them a try. But yeah, I don’t know. It’s kind of like as you do things for a while, you start to get a sense of the areas where things don’t quite work as well. I have to think about that during the week. I don’t think I have a rule of thumb. But I would say, you may as well try both. I would say try a random forest and try a neural net. They’re both pretty quick and easy to run, and see how it looks. If they’re roughly similar, I might dig into each and see if I can make them better. But if the random forest is doing way better, I’d probably just stick with that. Use whatever works. . | Do you think that things like scikit-learn and xgboost will eventually become outdated? Will everyone will use deep learning tools in the future? Except for maybe small datasets?[50:36] . I have no idea. I’m not good at making predictions. I’m not a machine learning model. I mean xgboost is a really nice piece of software. There’s quite a few really nice pieces of software for gradient boosting in particular. Actually, random forests in particular has some really nice features for interpretation which I’m sure we’ll find similar versions for neural nets, but they don’t necessarily exist yet. So I don’t know. For now, they’re both useful tools. scikit-learn is a library that’s often used for pre-processing and running models. Again, it’s hard to predict where things will end up. In some ways, it’s more focused on some older approaches to modeling, but I don’t know. They keep on adding new things, so we’ll see. I keep trying to incorporate more scikit-learn stuff into fastai and then I keep finding ways I think I can do it better and I throw it away again, so that’s why there’s still no scikit-learn dependencies in fastai. I keep finding other ways to do stuff. . | What about time series on tabular data? is there any RNN model involved in tabular.models? [1:05:09]: . We’re going to look at time series tabular data next week, but the short answer is generally speaking you don’t use a RNN for time series tabular data but instead, you extract a bunch of columns for things like day of week, is it a weekend, is it a holiday, was the store open, stuff like that. It turns out that adding those extra columns which you can do somewhat automatically basically gives you state-of-the-art results. There are some good uses of RNNs for time series, but not really for these kind of tabular style time series (like retail store logistics databases, etc). . | . Links and References . Link to Lesson 4 lecture | Homework notebooks: Notebook 1: lesson4-collab.ipynb | Notebook 2: lesson4-tabular.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson4 Detailed Notes. | Link to ULMFiT paper: https://arxiv.org/abs/1801.06146 | Fastai blog post on tabular data An Introduction to Deep Learning for Tabular Data · fast.ai | Medium post on recommenders with NN: Collaborative Embeddings for Lipstick Recommendations | Lecture on Embeddings: Embeddings, Machine Learning Crash Course, Google Developers | Word2Vec: Vector Representations of Words | Paper Review: Neural Collaborative Filtering Explanation &amp; Implementation | Blog post from Twitter on Embeddings: Embeddings@Twitter | Video: Embeddings for Everything: Search in the Neural Network Era | Applying the four step “Embed, Encode, Attend, Predict” framework to predict document similarity | Mini-course on Recommendation Systems, Google: Introduction to Recommendation Systems, Google Developers | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/18/fast-ai-lesson-4-notes-nlp-tabular-data-recommenders.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/08/18/fast-ai-lesson-4-notes-nlp-tabular-data-recommenders.html",
            "date": " • Aug 18, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Fast.ai v3 Lesson 3 Notes: Multi-label, Segmentation",
            "content": "Overview of Lesson . This lesson firstly dives deeper in to fastai’s approach to loading data for deep learning: the data block API; and secondly goes into more advanced problems beyond classification that you can solve with deep learning and the fastai library. Namely: . Multi-label classification (Planet Amazon dataset) | Regression problems (Head Orientation dataset) | Image Segmentation (Camvid dataset) | Text Classification (IMDB dataset) | . The lesson ends with a brief look at the fundamentals of deep learning: non-linearity and the Universal Approximation theorem. . DataBlock API . The trickiest step previously in deep learning has often been getting the data into a form that you can get it into a model. So far we’ve been showing you how to do that using various “factory methods” which are methods where you say, “I want to create this kind of data from this kind of source with these kinds of options.” That works fine, sometimes, and we showed you a few ways of doing it over the last couple of weeks. But sometimes you want more flexibility, because there’s so many choices that you have to make about: . Where do the files live | What’s the structure they’re in | How do the labels appear | How do you spit out the validation set | How do you transform it | . In fastai there is this unique API called the data block API. The data block API makes each one of those decisions a separate decision that you make. There are separate methods with their own parameters for every choice that you make around how to create / set up the data. . To give you a sense of what that looks like, the first thing I’m going to do is go back and explain what are all of the PyTorch and fastai classes you need to know about that are going to appear in this process. Because you’re going to see them all the time in the fastai docs and PyTorch docs. . We will now explain the different PyTorch and fastai classes that appear in the data block API. . Dataset (PyTorch) . The first class you need to know is the Dataset class, which is part of PyTorch. It is very simple, here is the source code: . . It actually does nothing at all. It is an abstract class, defining that subclasses of Dataset must implement __getitem__ and __len__ methods. The first means you can use the python array indexing notation with a Dataset object: d[12]; and the second means that you can get the length of the Dataset object: len(d). . DataLoader (PyTorch) . A Dataset is not enough to train a model. For SGD we need to be able to produce mini-batches of data for training. To create mini-batches we use another PyTorch class called a DataLoader. Here is the documentation for that: . . This takes a Dataset as a parameter. | It will create batches of size batch_size by grabbing items at random from the dataset. | The dataloader then sends the batch over to the GPU to your model. | . DataBunch (fastai) . The DataLoader is still not enough to train a model. To train a model we need to split the data into training, validation, and testing. So for that fastai has its own class called DataBunch. . . The DataBunch combines together a training DataLoader, a validation DataLoader, and optionally a test DataLoader. . You can read from the documentation that DataBunch also handles on-the-fly data transformations with tfms and allows you to create a custom function for building the mini-batches with collate_fn. . Learn to use the data block API by example . I won’t reproduce the examples here because fastai’s documentation already has a fantastic page full of data block API examples here. I recommend you read the whole thing or download and run it because the documentation pages are all Jupyter notebooks! . Image Transforms (docs) . fastai provides a complete image transformation library written from scratch in PyTorch. Although the main purpose of the library is data augmentation for use when training computer vision models, you can also use it for more general image transformation purposes. | Data augmentation is perhaps the most important regularization technique when training a model for Computer Vision: instead of feeding the model with the same pictures every time, we do small random transformations (a bit of rotation, zoom, translation, etc…) that don’t change what’s inside the image (to the human eye) but do change its pixel values. Models trained with data augmentation will then generalize better. | get_transforms creates a list of a image transformations. | Which image transformations are appropriate to use depends on your problem and what would likely appear in the real data. Flipping images of cats/dogs vertical isn’t useful because they wouldn’t appear upside-down. While for satellite images it makes no sense to zoom, but flipping them vertically and horizontally would make sense. | fastai is unique in that it provides a fast implementation of perspective warping. This is the max_wap option in get_transforms. This is like the kind of warping that occurs if you take a picture of a cat from above versus from below. This kinds of transformation wouldn’t make sense for satellite images, but would for cats and dogs. | . Planet Amazon: Multi-label Classification . (Link to Notebook) . The Planet Amazon dataset is an example of a multi-label classification problem. The dataset consists of satellite images taken of the Amazon rainforest, each of which has a list of labels describing what’s in the image, for example: weather, trees, river, agriculture. . Here is a sample of the images: . Here is what some of the training labels look like: . df = pd.read_csv(path/&#39;train_v2.csv&#39;) df.head() .   image_name tags . 0 | train_0 | haze primary | . 1 | train_1 | agriculture clear primary water | . 2 | train_2 | clear primary | . 3 | train_3 | clear primary | . 4 | train_4 | agriculture clear habitation primary road | . There are many different labels that an image can have and so there are a huge number of combinations. This makes treating it as a single label classification impractical. If there were 20 different labels then there could be as many as $2^{20}$ possible combinations. Better to have the model output a 20d vector than have it try to learn $2^{20}$ individual labels! . Loading the Data . Code for loading data with comments: . tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.) src = (ImageList.from_csv(path, &#39;train_v2.csv&#39;, folder=&#39;train-jpg&#39;, suffix=&#39;.jpg&#39;) # image names are listed in a csv file (names found by default in first column) # path to the data is `path` # the folder containing the data is `folder` # the file ext is missing from the names in the csv so add `suffix` to them. .split_by_rand_pct(0.2) # split train/val randomly with 80/20 split .label_from_df(label_delim=&#39; &#39;) # label from dataframe (the output of `from_csv`). Default is second column. # split the tags by &#39; &#39; ) data = (src.transform(tfms, size=128) .databunch().normalize(imagenet_stats)) . Create the Model . To create a Learner for multi-label classification you don’t need to do anything different from before. fastai create_cnn takes the DataBunch object and see that the type of the target variable and takes care of creating the output layers etc for you behind the scenes. . For this particular problem the only thing we do different is to pass a few different metrics to the Learner. . acc_02 = partial(accuracy_thresh, thresh=0.2) f_score = partial(fbeta, thresh=0.2) learn = cnn_learner(data, models.resnet50, metrics=[acc_02, f_score]) . Note: metrics change nothing about how the model learns. They are only an output for you to see how well it is learning. They are not to be confused with the model’s loss function. . What are these metrics about here? Well the network will output some M-dimensional vector with numbers between 0 and 1. Each of the the elements in this vector indicate the presence of one of the labels, but we need to decide a threshold value, above which we will say that this or that label is ‘on’. We set this threshold to 0.2. acc_02 and f_score here are the accuracy and f-score after applying 0.2 thresholding to the model output. . Train the Model . The model is trained in the basically same way as in the previous lessons: . Freeze all the layers except the head. | Run lr_find(). | Train the head for a few cycles. | Unfreeze the rest of the network and run lr_find() again. | Train the whole model for some more cycles with a differential learning rate. | After this however, Jeremy shows a cool new trick however called progressive resizing. Here you train your network on images that are smaller, then continue training the network on larger images. So start $128^2$ for a few cycles, then $256^2$, $512^2$ etc. You can save time on training for higher resolution images by effectively using smaller resolution models as pretrained models for larger resolutions. . To do this you simply have to tweak the DataBunch from before and give it a new size then give that to the Learner: . data = (src.transform(tfms, size=256) .databunch().normalize(imagenet_stats)) learn.data = data data.train_ds[0][0].shape . I won’t reproduce anymore here, but the full example is covered in this Lesson 3 homework notebook. . Camvid: Image Segmentation . (Link to Notebook) . The next example we’re going to look at is this dataset called CamVid. It’s going to be doing something called segmentation. We’re going to start with a picture like the left and produce a colour-coded picture on the right: . Input Image Segmented Output . | | . All the bicycle pixels are the same colour, all the car pixels are the same color etc. . Segmentation is an image classification problem where you need to label every single pixel in the image. . | In order to build a segmentation model, you actually need to download or create a dataset where someone has actually labeled every pixel. As you can imagine, that’s a lot of work, so you’re probably not going to create your own segmentation datasets but you’re probably going to download or find them from somewhere else. . | We use Camvid dataset. fastai comes with many datasets available for download through the fastai library. They are listed here. . | . Loading the Data . Segmentation problems come with sets of images: the input image and a segmentation mask. | The segmentation mask is a 2D array of integers. | fastai has a special function for opening image masks called open_mask. (For ordinary images use open_image). | . Create a databunch: . codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) get_y_fn = lambda x: path_lbl/f&#39;{x.stem}_P{x.suffix}&#39; src = (SegmentationItemList.from_folder(path_img) #Where to find the data? -&gt; in path_img and its subfolders .split_by_fname_file(&#39;../valid.txt&#39;) #How to split in train/valid? -&gt; using predefined val set in valid.txt .label_from_func(get_y_fn, classes=codes) #How to label? -&gt; use the label function on the file name of the data ) data = (src.transform(get_transforms(), tfm_y=True, size=128) #Data augmentation? -&gt; use tfms with a size of 128, # also transform the label images (tfm_y) .databunch(bs=8) #Finally -&gt; use the defaults for conversion to databunch ).normalize(imagenet_stats) . fastai shows the images with the masks superimposed for you with show_batch: . . Create the Model . For segmentation an architecture called UNET turns out to be better than using a CNN. Here’s what it looks like:. | Here is a link to the University website where they talk about the U-Net. But basically this bit down on the left hand side is what a normal convolutional neural network looks like. It’s something which starts with a big image and gradually makes it smaller and smaller until eventually you just have one prediction. What a U-Net does is it then takes that and makes it bigger and bigger and bigger again, and then it takes every stage of the downward path and copies it across, and it creates this U shape. | It is a bit like a convolutional autoencoder except there are these data sharing links that cross horizontal across the network in the diagram. | In fastai you create a UNET with unet_learner(data, models.resnet34, metrics=metrics, wd=wd) and you pass in all the same stuff as with cnn_learner. | . Results . With UNET and the default fastai results Jeremy managed to achieve a SOTA result of 0.92 accuracy. . BIWI Head Pose: Regression . (Link to Notebook) . . In this problem we want to locate the center point of the face of a person in an image. . So far, everything we’ve done has been a classification model﹣something that created labels or classes. This, for the first time, is what we call a regression model. A lot of people think regression means linear regression, it doesn’t. Regression just means any kind of model where your output is some continuous number or set of numbers. So we need to create an image regression model (i.e. something that can predict these two numbers). How do you do that? Same way as always - data bunch api then CNN model. . Loading the Data . tfms = get_transforms(max_rotate=20, max_zoom=1.5, max_lighting=0.5, max_warp=0.4, p_affine=1., p_lighting=1.) data = (PointsItemList.from_folder(path) .split_by_valid_func(lambda o: o.parent.name==&#39;13&#39;) .label_from_func(get_ctr) .transform(tfms, tfm_y=True, size=(120,160)) .databunch().normalize(imagenet_stats) ) . This is exactly the same as we have already seen except the target variable is represented as a different data type - the ImagePoints class. An ImagePoints object represents a ‘flow’ (it’s just a list) of 2D points on an image. The points have the convention of (y, x) and are scaled to be between -1 and 1. . An example flow looks like: . . In the case of BIWI, there is only one point in the flow however, but you get the idea. For facial keypoints type problems ImagePoints is what you want to use. . When it comes to training the model you use the same cnn_learner as in the other examples, the only difference being the loss function. For problems where you are predicting a continuous value like here you typically use the Mean Squared Error (MSELossFlat() in fastai) as the loss function. In fastai you don’t need to specify this, cnn_learner will select it for you. . IMDB: Text Classification . (Link to the Notebook) . This is a short section on a Natural Language Processing (NLP) problem. Instead of classifying images we will look classifying documents. We will use the IMDB data set and classify if a movie review is negative or positive. . Use different fastai module for text: . from fastai.text import * . Loading the Data . The data is a csv and looks like: . . Use the datablock/databunch API to load the csv for text data types: . data_lm = TextDataBunch.from_csv(path, &#39;texts.csv&#39;) . From this you can create a learner and train on this. There are two steps not shown here that transform the text from text to something that you can give to a neural network to train on (i.e. numbers). These steps are Tokenization and Numericalization. . Tokenization . Split raw sentences into words, or ‘tokens’. It does this by: . Splitting the string into just the words | Takes care of punctuation | Separates contractions from words: “didn’t” -&gt; “did” + “n’t” | Replacing unknown words with a single token “xxunk”. | Cleaning out HTML from the text. | . data = TextClasDataBunch.from_csv(path, &#39;texts.csv&#39;, valid_pct=0.01) data.show_batch() . Example: . “Raising Victor Vargas: A ReviewYou know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It’s warm and gooey, but you’re not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn’t quite feel right. … . =&gt; . xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review n n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it ‘s warm and gooey , but you ‘re not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj … . Anything starting with ‘xx’ is some special token. . Numericalization . Once we have extracted the tokens from the text, we can convert them to integers by create a big list of all the tokens used: vocabulary. This list only includes words that are used at least twice and is truncated with a maximum size of 60,000 (by default). Words that don’t make the cut are replaced with ‘XXUNK’. . From the notebook: . . With the data block API . Here are the previous steps done this time with the data block API: . data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) .split_from_df(col=2) .label_from_df(cols=0) .databunch()) . Training a Classifier Preview . Lesson 4 covers the training of the text classifier in detail. Here are the steps covered as a preview. . You need to first create a language model trained on your text corpus. fastai has language_model_learner for this. This training is quite time/compute intensive. | Then you create a text classifier - text_classifier_model - and use the language model trained in 1 as the feature encoder. | What is deep learning fundamentally? . Up to this point we’ve seen loads of different problems that deep learning helps us tackle. Deep learning is buzzword for algorithms that use these things called neural networks, which sound like something complicated that may have something to do with how the human brain works. If you remove all the mystique from deep learning you see that it is basically a model with parameters that are updated using Stochastic Gradient Descent. These parameters are parameters to matrix multiplications (convolutions also a tweaked kind of matrix multiplication). . A matrix multiply is a linear function and any stacking of matrix multiplies is a also a linear function because of linearity. Telling the difference between cats and dogs is far more than a linear function can do. So after the matrix multiplications we have something called a non-linearity of activation function. This takes the result of the matrix multiplication and sticks it through some non-linear function. . In the old days the most common function used was the sigmoid, e.g. tanh: . . These days the workhorse is the rectified linear unit (ReLU): . . Sounds fancy, but in reality it’s this: . def relu(x): max(x, 0) . So how can a stack of matrix multiplications and relu’s result in a model that can classify IMDB reviews or galaxies? Because of a thing called the Universal Approximation Theorem. What it says is that if you have stacks of linear functions and nonlinearities, the thing you end up with can approximate any function arbitrarily closely. So you just need to make sure that you have a big enough matrix to multiply by, or enough of them. If you have this function which is just a sequence of matrix multiplies and nonlinearities where the nonlinearities can be basically any of these activation functions, if that can approximate anything, then all you need is some way to find the particular values of the weight matrices in your matrix multiplies that solve the problem you want to solve. We already know how to find the values of parameters. We can use gradient descent. So that’s actually it. . There is a nice website that has interactive javascript demos that demonstrate this: http://neuralnetworksanddeeplearning.com. . Jeremy Says… . If you use a dataset, it would be very nice of you to cite the creator and thank them for their dataset. | This week, see if you can come up with a problem that you would like to solve that is either multi-label classification or image regression or image segmentation or something like that and see if you can solve that problem. Context: Fast.ai Lesson 3 Homework 36 | Always use the same stats that the model was trained with (e.g. imagenet). (See relevant question in Q &amp; A section). Context: Lesson 3: Normalized data and ImageNet 7 | (Source: Robert Bracco) . Q &amp; A . When your model makes an incorrect prediction in a deployed app, is there a good way to “record” that error and use that learning to improve the model in a more targeted way? [42:01] If you had, for example, an image classifier online you could have a user tell you if the classifier got it wrong and what the right answer is. | You could then store that image that was incorrectly classified. | Every so often you could go and fine-tune your network on a new data bunch of just the misclassified images. | You do this by taking your existing network, unfreezing the layers, and then run some epochs on the misclassified images. You may want to run with a slightly higher learning rate or for more epochs because these images are more interesting/suprising to the model. | . | What resources do you recommend for getting started with video? For example, being able to pull frames and submit them to your model. [47:39] . The answer is it depends. If you’re using the web which I guess probably most of you will be then there’s web API’s that basically do that for you. So you can grab the frames with the web API and then they’re just images which you can pass along. If you’re doing a client side, I guess most people would tend to use OpenCV for that. But maybe during the week, people who are doing these video apps can tell us what have you used and found useful, and we can start to prepare something in the lesson wiki with a list of video resources since it sounds like some people are interested. . | Is there a way to use learn.lr_find() and have it return a suggested number directly rather than having to plot it as a graph and then pick a learning rate by visually inspecting that graph? (And there are a few other questions around more guidance on reading the learning rate finder graph) [1:00:26] . The short answer is no and the reason the answer is no is because this is still a bit more artisanal than I would like. As you can see, I’ve been saying how I read this learning rate graph depends a bit on what stage I’m at and what the shape of it is. I guess when you’re just training the head (so before you unfreeze), it pretty much always looks like this: . . And you could certainly create something that creates a smooth version of this, finds the sharpest negative slope and picked that. You would probably be fine nearly all the time. . But then for you know these kinds of ones, it requires a certain amount of experimentation: . . But the good news is you can experiment. Obviously if the lines going up, you don’t want it. Almost certainly at the very bottom point, you don’t want it right there because you needed to be going downwards. But if you kind of start with somewhere around 10x smaller than that, and then also you could try another 10x smaller than that. Try a few numbers and find out which ones work best. . And within a small number of weeks, you will find that you’re picking the best learning rate most of the time. So at this stage, it still requires a bit of playing around to get a sense of the different kinds of shapes that you see and how to respond to them. Maybe by the time this video comes out, someone will have a pretty reliable auto learning rate finder. We’re not there yet. It’s probably not a massively difficult job to do. It would be an interesting project﹣collect a whole bunch of different datasets, maybe grab all the datasets from our datasets page, try and come up with some simple heuristic, compare it to all the different lessons I’ve shown. It would be a really fun project to do. But at the moment, we don’t have that. I’m sure it’s possible but we haven’t got them. . | Could you use unsupervised learning here (pixel classification with the bike example) to avoid needing a human to label a heap of images[1:10:03] . Not exactly unsupervised learning, but you can certainly get a sense of where things are without needing these kind of labels. Time permitting, we’ll try and see some examples of how to do that. You’re certainly not going to get as such a quality and such a specific output as what you see here though. If you want to get this level of segmentation mask, you need a pretty good segmentation mask ground truth to work with. . | Is there a reason we shouldn’t deliberately make a lot of smaller datasets to step up from in tuning? let’s say 64x64, 128x128, 256x256, etc… [1:10:51] . Yes, you should totally do that. It works great. This idea, it’s something that I first came up with in the course a couple of years ago and I thought it seemed obvious and just presented it as a good idea, then I later discovered that nobody had really published this before. And then we started experimenting with it. And it was basically the main tricks that we use to win the DAWNBench ImageNet training competition. . Not only was this not standard, but nobody had heard of it before. There’s been now a few papers that use this trick for various specific purposes but it’s still largely unknown. It means that you can train much faster, it generalizes better. There’s still a lot of unknowns about exactly how small, how big, and how much at each level and so forth. We call it “progressive resizing”. I found that going much under 64 by 64 tends not to help very much. But yeah, it’s a great technique and I definitely try a few different sizes. . | What does accuracy mean for pixel wise segmentation? Is it#correctly classified pixels / #total number of pixels? [1:12:35] . Yep, that’s it. So if you imagined each pixel was a separate object you’re classifying, it’s exactly the same accuracy. So you actually can just pass in accuracy as your metric, but in this case, we actually don’t. We’ve created a new metric called acc_camvid and the reason for that is that when they labeled the images, sometimes they labeled a pixel as Void. I’m not quite sure why but some of the pixels are Void. And in the CamVid paper, they say when you’re reporting accuracy, you should remove the void pixels. So we’ve created accuracy CamVid. So all metrics take the actual output of the neural net (i.e. that’s the input to the metric) and the target (i.e. the labels we are trying to predict). . . We then basically create a mask (we look for the places where the target is not equal to Void) and then we just take the input, do the argmax as per usual, but then we just grab those that are not equal to the void code. We do the same for the target and we take the mean, so it’s just a standard accuracy. . It’s almost exactly the same as the accuracy source code we saw before with the addition of this mask. This quite often happens. The particular Kaggle competition metric you’re using or the particular way your organization scores things, there’s often little tweaks you have to do. And this is how easy it is. As you’ll see, to do this stuff, the main thing you need to know pretty well is how to do basic mathematical operations in PyTorch so that’s just something you kind of need to practice. . | I’ve noticed that most of the examples and most of my models result in a training loss greater than the validation loss. What are the best ways to correct that? I should add that this still happens after trying many variations on number of epochs and learning rate. [1:15:03] . Remember from last week, if your training loss is higher than your validation loss then you’re underfitting. It definitely means that you’re underfitting. You want your training loss to be lower than your validation loss. If you’re underfitting, you can: . Train for longer. | Train the last bit at a lower learning rate. | . But if you’re still under fitting, then you’re going to have to decrease regularization. We haven’t talked about that yet. In the second half of this part of the course, we’re going to be talking quite a lot about regularization and specifically how to avoid overfitting or underfitting by using regularization. If you want to skip ahead, we’re going to be learning about: . weight decay | dropout | data augmentation | . They will be the key things that are we talking about. . | For a dataset very different than ImageNet like the satellite images or genomic images shown in lesson 2, we should use our own stats. Jeremy once said: “If you’re using a pretrained model you need to use the same stats it was trained with.” Why it is that? Isn’t it that, normalized dataset with its own stats will have roughly the same distribution like ImageNet? The only thing I can think of, which may differ is skewness. Is it the possibility of skewness or something else the reason of your statement? And does that mean you don’t recommend using pre-trained model with very different dataset like the one-point mutation that you showed us in lesson 2? [1:46:53] . Nope. As you can see, I’ve used pre-trained models for all of those things. Every time I’ve used an ImageNet pre-trained model, I’ve used ImageNet stats. Why is that? Because that model was trained with those stats. For example, imagine you’re trying to classify different types of green frogs. If you were to use your own per-channel means from your dataset, you would end up converting them to a mean of zero, a standard deviation of one for each of your red, green, and blue channels. Which means they don’t look like green frogs anymore. They now look like grey frogs. But ImageNet expects frogs to be green. So you need to normalize with the same stats that the ImageNet training people normalized with. Otherwise the unique characteristics of your dataset won’t appear anymore﹣you’ve actually normalized them out in terms of the per-channel statistics. So you should always use the same stats that the model was trained with. . | There’s a question about tokenization. I’m curious about how tokenizing words works when they depend on each other such as San Francisco. [1:56:45] . How do you tokenize something like San Francisco. San Francisco contains two tokens San Francisco. That’s it. That’s how you tokenize San Francisco. The question may be coming from people who have done traditional NLP which often need to use these things called n-grams. N-rams are this idea of a lot of NLP in the old days was all built on top of linear models where you basically counted how many times particular strings of text appeared like the phrase San Francisco. That would be a bi-gram for an n-gram with an n of 2. The cool thing is that with deep learning, we don’t have to worry about that. Like with many things, a lot of the complex feature engineering disappears when you do deep learning. So with deep learning, each token is literally just a word (or in the case that the word really consists of two words like you&#39;re you split it into two words) and then what we’re going to do is we’re going to then let the deep learning model figure out how best to combine words together. Now when we see like let the deep learning model figure it out, of course all we really mean is find the weight matrices using gradient descent that gives the right answer. There’s not really much more to it than that. . Again, there’s some minor tweaks. In the second half of the course, we’re going to be learning about the particular tweak for image models which is using a convolution that’ll be a CNN, for language there’s a particular tweak we do called using recurrent models or an RNN, but they’re very minor tweaks on what we’ve just described. So basically it turns out with an RNN, that it can learn that San plus Francisco has a different meaning when those two things are together. . | Some satellite images have 4 channels. How can we deal with data that has 4 channels or 2 channels when using pre-trained models? [1:59:09] . I think that’s something that we’re going to try and incorporate into fast AI. So hopefully, by the time you watch this video, there’ll be easier ways to do this. But the basic idea is a pre-trained ImageNet model expects a red green and blue pixels. So if you’ve only got two channels, there’s a few things you can do but basically you’ll want to create a third channel. You can create the third channel as either being all zeros, or it could be the average of the other two channels. So you can just use you know normal PyTorch arithmetic to create that third channel. You could either do that ahead of time in a little loop and save your three channel versions, or you could create a custom dataset class that does that on demand. . For 4 channel, you probably don’t want to get rid of the 4th channel. So instead, what you’d have to do is to actually modify the model itself. So to know how to do that, we’ll only know how to do in a couple more lessons time. But basically the idea is that the initial weight matrix (weight matrix is really the wrong term, they’re not weight matrices; their weight tensors so they can have more than just two dimensions), so that initial weight tensor in the neural net, one of its axes is going to have three slices in it. So you would just have to change that to add an extra slice, which I would generally just initialize to zero or to some random numbers. So that’s the short version. But really to understand exactly what I meant by that, we’re going to need a couple more lessons to get there. . | . Links and References . Link to Lesson 3 lecture | Homework notebooks: Notebook 1: lesson3-planet.ipynb | Notebook 2: lesson3-camvid.ipynb | Notebook 3: lesson3-imdb.ipynb | Notebook 4: lesson3-head-pose.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson3 Detailed Notes. | Universal approximation theorem + more: http://neuralnetworksanddeeplearning.com | Source for Jeremy says: https://forums.fast.ai/t/things-jeremy-says-to-do/36682 | Cyclical Learning Rates for Training Neural Networks paper by Leslie Smith | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/23/fast-ai-lesson-3-notes-multi-label-segmentation.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/23/fast-ai-lesson-3-notes-multi-label-segmentation.html",
            "date": " • Jul 23, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "SGD From Scratch in PyTorch",
            "content": "Overview . In this post I explore Stochastic Gradient Descent (SGD) which is an optimization method commonly used in neural networks. This continues Lesson 2 of fast.ai on Stochastic Gradient Descent (SGD). I will copy from the fast.ai notebook on SGD and dig deeper into the what’s going on there. . Linear Regression . We will start with the simplest model - the Linear model. Mathematically this is represented as: . [ vec{y} = X vec{a} + vec{b}] . Where $X$ is a matrix where each of the rows is a data point, $ vec{a}$ is the vector of model weights, and $ vec{b}$ is a bias vector. In the 1D case, these would correspond to the familiar ‘slope’ and ‘intercept’ of a line. We can make this more compact by combining the bias inside of the model weights and adding an extra column to $X$ with all values set to one. These are represented in Pytorch as tensors. . In Pytorch, a tensor is a data structure that encompasses arrays of any dimension. A vector is a tensor of rank 1, while a matrix is a tensor of rank 2. For simplicity we will stick to the case of a 1D linear model. In PyTorch $X$ would then be: . n=100 x = torch.ones(n,2) x[:,0].uniform_(-1.,1) . The model has two parameters and there are n=100 datapoints. x therefore has shape (100, 2). The .uniform_(-1., 1) generates floating point numbers between -1 and 1. The trailing _ is PyTorch convention that the function operates inplace. . Let’s look at the first 5 values of x: . &gt; x[:5] tensor([[ 0.7893, 1.0000], [-0.7556, 1.0000], [-0.0055, 1.0000], [-0.2465, 1.0000], [ 0.0080, 1.0000]]) . Notice how the second column is all 1s - this is the bias. . We’ll now set the true values for the model weights, $a$, to slope=3 and intersection=10: . a = tensor(3.,10) a_true = a . With x and a set we can now generate some fake data with some small normally distributed random noise: . y = x@a + torch.randn(n) * 0.6 . . Loss Function . We want to find parameters (weights) a such that they minimize the error between the points and the line x@a. Note that here a is unknown. For a regression problem the most common error function or loss function is the mean squared error. In python this function is: . def mse(y_hat, y): return ((y_hat-y)**2).mean() . Where y is the true value and y_hat is the predicted value. . We start with guess at the value of the weights a: . a = tensor(-1, 1) . We can make prediction for y, y_hat, and compute the error against the known values: . &gt; y_hat = x@a &gt; mse(y_hat, y) tensor(92.9139) . So far we have specified the model (linear regression) and the evaluation criteria (or loss function). Now we need to handle optimization; that is, how do we find the best values for a? How do we find the best fitting linear regression. . Gradient Descent . We would like to find the values of a that minimize mse_loss. Gradient descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient. Here is gradient descent implemented in PyTorch: . a = nn.Parameter(a) lr = 1e-1 def update(): y_hat = x@a loss = mse(y, y_hat) loss.backward() with torch.no_grad(): # don&#39;t compute the gradient in here a.sub_(lr * a.grad) a.grad.zero_() for t in range(100): update() . We are going to create a loop. We’re going to loop through 100 times, and we’re going to call a function called update. That function is going to: . Calculate y_hat (i.e. our prediction) | Calculate loss (i.e. our mean squared error) | Calculate the gradient. In PyTorch, calculating the gradient is done by using a method called backward. Mean squared error was just a simple standard mathematical function. PyTorch keeps track of how it was calculated and lets us automatically calculate the derivative. So if you do a mathematical operation on a tensor in PyTorch, you can call backward to calculate the derivative and the derivative gets stuck inside an attribute called .grad. | Then take the weights a and subtract the gradient from them (sub_). There is an underscore there because that’s going to do it in-place. It’s going to actually update those coefficients a to subtract the gradients from them. Why do we subtract? Because the gradient tells us if the whole thing moves downwards, the loss goes up. If the whole thing moves upwards, the loss goes down. So we want to do the opposite of the thing that makes it go up. We want our loss to be small. That’s why we subtract. | lr is our learning rate. All it is is the thing that we multiply by the gradient. | . Animate it! . Here is an animation of the training gradient descent with learning rate LR=0.1 . . Notice how it seems to spring up to find the intercept first then adjusts to get the slope right. The starting guess at the intercept is 1, while the real value is 10. At the start this would cause the biggest loss so the we would expect the gradient on the intercept parameter to be higher than the gradient on the slope parameter. . It sucessfully recovers, more or less, the weights that we generated the data with: . &gt; a tensor([3.0332, 9.9738] . Stochastic Gradient Descent . The gradient descent algorithm calculates the loss across the entire dataset every iteration. For this problem this works great, but it won’t scale. If we were training on imagenet then we’d have to compute the loss on 1.5 million images just to do a single update of the parameters. This would be both incredibly slow and also impossible to fit into computer memory. Instead we grab random mini-batches of 64, or so, data points and compute the loss and gradient with those and then update the weights. As code this looks almost identical to before, but with some random indexes added to x and y: . def update_mini(rand_idx): y_hat = x[rand_idx]@a loss = mse(y[rand_idx], y_hat) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . Using mini-batches approximates the gradient, but also adds random noise to the optimiser causing the parameters to ‘jump around’ a little. This can make it require more iterations to converge. We will see this visually in the next section. On the other hand, some random noise is a good thing in training neural networks because it allows the optimiser to better explore the high dimensional parameter space and potentially find a solution with a lower loss. . Animate it! . Here is an animation of the training with batch size of 16: . . It converges on the same answer as gradient descent, but it is a little slower and has a bit of jitter that isn’t in the gradient descent animation. . Experiments with the Learning Rate and Batch Size . We can gain a better understanding of how SGD works by playing with the parameters, learning rate and batch size, and visualising the learning process. . Learning Rate . Here the learning rate in SGD is varied, keeping the batch size fixed at 16. . Parameters Animation . SGD LR=1e-2bs=16 | | . SGD LR=1e-1bs=16 | | . SGD LR=1.0 bs=16 | | . With the learning rate of 0.01 it too small and it takes an age, but it does eventually converge on the right answer. With a learning rate of 1.0 the whole thing goes off the rails and it can’t get anywhere near the right answer. . Batch Size . Here the batch size in SGD is varied, holding the learning rate fixed at LR=0.1: . Parameters Animation . SGD bs=1 | | . SGD bs=2 | | . SGD bs=4 | | . SGD bs=8 | | . SGD bs=16 | | . SGD bs=32 | | . SGD bs=64 | | . All of the instances do converge to the right answer in this case (though in general that wouldn’t be the case for all problems). For bs=1 it jumps around a lot even after it gets into the right place. This is because the weights are updated using only one data point every iteration. So it jitters around the right solution and will never stop jittering with more iterations. . However with increasing batch size the jitter gets less and less. At batch size of 64 the animation is almost identical to the gradient descent animation. This makes sense since n=100 so with bs=64 we have almost gone back to the full gradient descent algorithm (which would be bs=n ). . References . Link to Lesson 2 lecture | SGD From Scratch Notebook | Lesson2 Detailed Notes by @hiromi. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html",
            "date": " • Jul 13, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Fast.ai v3 Lesson 2 Notes: Data Cleaning and Production",
            "content": "Overview of Lesson . This lesson has two parts. The first part is about constructing a image classifier from your own data. It details data collection from Google images, creating a validation set, and cleaning the data using the model. . In the second part, we construct a simple linear model from scratch in PyTorch and train it using gradient descent and stochastic gradient descent. That part got quite lengthy so I made it its own blog post here. . Download Your Own Image Data . There is a trick to downloading data from google images. You can do the search manually for the images, then run some javascript magic to get the URLs for the images. You can then save these in a file and then download them from the command line. . Go to Google images and search for your desired images. . | Open the browser javascript console: (⌘+⎇+J on Mac, Crtl+Shift+J on Windows/Linux). . | Run the following the console: . urls = Array.from(document.querySelectorAll(&#39;.rg_di.rg_meta&#39;)).map(el=&gt;JSON.parse(el.textContent).ou); window.open(&#39;data:text/csv;charset=utf-8,&#39; + escape(urls.join(&#39; n&#39;))); . | This initiates a download of a CSV that contains all the urls to the images shown on Google images. . | Use fastai’s download_images function and pass it the path to the CSV file as the argument. . | Remove images that aren’t valid. Use fastai’s verify_images to delete these. . | Then Train With A CNN . Following the steps from Lesson 1: . Load data using the DataBunch API: . np.random.seed(42) # fix seed for to get same validation set data = ImageDataBunch.from_folder(path, train=&#39;.&#39;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats) . | Create the CNN learner and specify the architecture: . learn = create_cnn(data, models.resnet34, metrics=error_rate) . | First fit the head of the pretrained CNN with a few cycles: . learn.fit_one_cycle(4) ... . | Then unfreeze the body of the pretrained CNN: . learn.unfreeze() . | Run the learning rate finder: . learn.lr_find() . | Inspect the learning rate graph and find the strongest downward slope whose negative trend persists for while with the increasing learning rate. Try to pick a learning rate corresponding to the steepest part of this slope. . . | Train the whole network again for a few cycles using a range of learning rates for each layer group, with the learning rate you picked being the highest. This is called Discriminative Layer Training in fastai. . learn.fit_one_cycle(2, max_lr=slice(3e-5, 3)) . | In the Bear example Jeremy does this produces an error rate of 1.4% with a few hundred images and a few minutes of training time on a GPU. . Intepretation . For a classification task such as the Bear example in the lecture, you want to look at the confusion matrix to see where it is failing (well, except where you have loads of classes). FastAI has a handy class for interpreting classification results: . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . . Pretty good - only one mistake! . Cleaning Up Your Dataset . Maybe there is noise or mistakes in your dataset. If we download images from google then perhaps there are images that are of the wrong thing. We want to clean it up. Here is where human intelligence and a computer learner can be combined! It’s very unlikely that a mislabeled data is going to be predicted correctly and with high confidence. We can look at the instances that the computer learner is least confident about - i.e. the instances with the highest loss. There is a nice widget for Jupyter notebook for inspecting and deleting things called FileDeleter: . from fastai.widgets import * losses,idxs = interp.top_losses() top_loss_paths = data.valid_ds.x[idxs] . After cycling through FileDeleter and deleting the bad data you should eventually see fewer and fewer bad data points. At this point you are done and should retrain your model on the cleaned dataset. . Generally these CNN models are pretty good at handling small amounts of noise so this data cleaning will normally give you a small improvement. . Putting your Model into Production . You probably want to use CPU for inference, except for massive scale (and you almost certainly don’t need to train in real time). GPU is only effective if you can get things into neat batches with sizes like 64, which exploits the GPU parallelism. In PyTorch you can specify CPU via: . fastai.defaults.device = torch.device(&#39;cpu&#39;) . Let’s use the trained model for inference. We upload an image of a bear and store that in a variable img: . img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) . . And as per usual, we created a data bunch, but this time, we’re not going to create a data bunch from a folder full of images, we’re going to create a special kind of data bunch which is one that’s going to grab one single image at a time. So we’re not actually passing it any data. The only reason we pass it a path is so that it knows where to load our model from. That’s just the path that’s the folder that the model is going to be in. . You also need to pass it the same transformations , size, and normalizations that you used when training the CNN. You then create_cnn with this fake dataset and then load the weights that were saved in the training phase: . classes = [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddys&#39;] data2 = ImageDataBunch.single_from_classes(path, classes, tfms=get_transforms(), size=224).normalize(imagenet_stats) learn = create_cnn(data2, models.resnet34) learn.load(&#39;stage-2&#39;) . Then prediction is done using the predict method and passing in the real single image data: . pred_class,pred_idx,outputs = learn.predict(img) pred_class &gt; &#39;black&#39; . This is the engine of an web-app. The rest of the app can be coded up in a framework like Flask or Starlette. Here is a great example that uses Starlette: cougar-or-not. . There are services for hosting, such as: https://www.pythonanywhere.com/ . Things That Can Go Wrong . The problems will basically be either: . The learning rate is too high or too low | The number of epochs is too many or too few | . Learning rate too high: basically ruins everything and results in a super high validation loss . Learning rate too low: the error rate goes down really slowly. The other thing you see if your learning rate is too small is that your training loss will be higher than your validation loss. You never want a model where your training loss is higher than your validation loss. That always means you are under-fitting which means either your learning rate is too low or your number of epochs is too low. So if you have a model like that, train it some more or train it with a higher learning rate. . Number of epochs too few: training loss much higher than validation loss, which is a symptom of under-fitting. It needs to learn more. . Number of epochs too many: Too many epochs create something called “overfitting”. If you train for too long as we’re going to learn about it, it will learn to recognize your particular teddy bears but not teddy bears in general. . This is a good post about diagnosing your fit in machine learning: machine learning mastery. . The Truth About Overfitting . The only thing that tells you you are overfitting is that the error rate improves for a while and then starts getting worse again. . Myth: If the training loss is less than the validation loss then the model is overfitting. Absolutely not true. . Any model that is trained correctly will always have a lower training loss than validation loss . You want your model to have a low error. So as long as you’re training and your model error is improving, you’re not overfitting. . In Jeremy’s option, despite what you hear, it’s actually very hard to overtrain in deep learning. . Underfitting . How can the training loss be higher than the validation loss? This doesn’t really seem like it could happen except if you had some contrived validation set. It can however happen quite easily with training neural networks because of dropout. . Dropout is turned on while training and turned off in the validation. If the result is made much worse by dropout then it means that the network has not learned sufficiently well and it is therefore underfitting. Ways to fix this are: train with more epochs, use higher learning rate, use less dropout, or adjust weight decay parameters. . Batch Norm also operates differently at training and test time. . SGD From Scratch . This part kind of outgrew this blog post so I have spun this out into its own blog post here. . Jeremy Says… . If forum posts are overwhelming, click “summarize this topic” at the bottom of the first post. (Only works for &gt;50 replies). | Please follow the official server install/setup instructions, they work and are easy. | It’s okay to feel intimidated, there’s a lot, but just pick one piece and dig into it. Try to push a piece of code, or learn a concept like regular expressions, or create a classifier, or whatever. Context: Lesson 2: It’s okay to feel intimidated 30 | If you’re stuck, keep going. See image below! Context: Lesson 2: If you’re stuck, keep going 38 | If you’re not sure which learning rate is best from plot, try both and see. | When you put a model into production, you probably want to use CPU for inference, except at massive scale. Context: Lesson 2: Putting Model into Production 17 | Most organizations spend too much time gathering data. Get a small amount first, see how it goes. | If you think you’re not a math person, check out Rachel’s talk: There’s no such thing as “not a math person” 56. My own input: only 6 minutes, everyone should watch it! | . (Source: Robert Bracco) . Q &amp; A . When generating new image dataset, how do you know how many images are enough? What are ways to measure “enough”? . That’s a great question. Another possible problem you have is you don’t have enough data. How do you know if you don’t have enough data? Because you found a good learning rate (i.e. if you make it higher than it goes off into massive losses; if you make it lower, it goes really slowly) and then you train for such a long time that your error starts getting worse. So you know that you trained for long enough. And you’re still not happy with the accuracy﹣it’s not good enough for the teddy bear cuddling level of safety you want. So if that happens, there’s a number of things you can do and we’ll learn pretty much all of them during this course but one of the easiest one is get more data. If you get more data, then you can train for longer, get higher accuracy, lower error rate, without overfitting. . Unfortunately, there is no shortcut. I wish there was. I wish there’s some way to know ahead of time how much data you need. But I will say this﹣most of the time, you need less data than you think. So organizations very commonly spend too much time gathering data, getting more data than it turned out they actually needed. So get a small amount first and see how you go. . | What do you do if you have unbalanced classes such as 200 grizzly and 50 teddies? . Nothing. Try it. It works. A lot of people ask this question about how do I deal with unbalanced data. I’ve done lots of analysis with unbalanced data over the last couple of years and I just can’t make it not work. It always works. There’s actually a paper that said if you want to get it slightly better then the best thing to do is to take that uncommon class and just make a few copies of it. That’s called “oversampling” but I haven’t found a situation in practice where I needed to do that. I’ve found it always just works fine, for me. . | . Links and References . Link to Lesson 2 lecture | Homework notebooks: Notebook 1: lesson2-download.ipynb | Notebook 2: lesson2-sgd.ipynb | . | Parts of my notes have been copied from the excellent lecture transcriptions made by @hiromi. Link: Lesson2 Detailed Notes. | This is an in-depth tutorial on PyTorch: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e | How (and why) to create a good validation set by @rachel | There’s no such thing as “not a math person” by @rachel | Responder - a web app framework built on top of Starlette | Post about an alternative image downloader/cleaner by @cwerner | A tool for excluding irrelevant images from Google Image Search results by @melonkernel | Machine Learning is Fun - source of image/number GIF animation shown in lesson | A systematic study of the class imbalance problem in convolutional neural networks, mentioned by Jeremy as a way to solve imbalanced datasets. | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/12/fast-ai-lesson-2-notes-data-cleaning-and-production.html",
            "relUrl": "/deep-learning/machine-learning/fastai/fastai-v3-notes/2019/07/12/fast-ai-lesson-2-notes-data-cleaning-and-production.html",
            "date": " • Jul 12, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "Fast.ai v3 Lesson 1 Notes: Image Classification",
            "content": "Overview of Lesson . This is the introductory lesson to fastai part 1! . The key outcome of this lesson is that we’ll have trained an image classifier which can recognize pet breeds at state of the art accuracy. The key to this success is the use of transfer learning, which will be a key platform for much of this course. We’ll also see how to analyze the model to understand its failure modes. In this case, we’ll see that the places where the model is making mistakes is in the same areas that even breeding experts can make mistakes. . Task 1 - World Class Image Classifier . Fastai opts to teach deep learning backwards - rather than starting at the level of neurons they start with learning to use the state of the art algorithms and networks from the beginning. Learning to become a practitioner with the best practices first and then gradually learning the technical details later. . | Task 1: Training a world class image classification model. . | Image classification has been one of deep learning’s biggest successes so far. . | 10 years ago separating cat and dog images was a hard problem. With classical methods researchers were scoring ~80%. With today’s algorithms it’s actually too easy and scores on the cats vs dogs dataset are almost 100%. That’s why we used the harder dataset of cat and dog breeds. . | Cat breeds and dog breeds dataset from Oxford: Cats and Dogs Breeds Classification Oxford Dataset | Kaggle. . | This task found in the Jupyter notebook: lesson1-pets.pynb . | . Load the Data . FastAI has its own way of handling different datasets: the DataBunch API. . | This integrates the loading of different types of data, the labeling, splitting into train/val/test, and the data transformations for standardisation, normalisation, and training augmentation. . | To load the cats and dogs dataset: . pat = r&#39;/([^/]+)_ d+.jpg$&#39; data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=64, ).normalize(imagenet_stats) . | What is this doing? Let’s look at the docs: . class ImageDataBunch(fastai.basic_data.DataBunch) | DataBunch suitable for computer vision. ... | from_name_re(path:Union[pathlib.Path, str], fnames:Collection[pathlib.Path], pat:str, valid_pct:float=0.2, **kwargs) from builtins.type | Create from list of `fnames` in `path` with re expression `pat`. . | get_transforms is a function that returns a list of default image transformations for data augmentation. | size=224 resizes the images to 224x224. This is the size that the network we are using (resnet34) has been trained on. | bs is the batchsize. | normalize(imagenet_stats) normalizes the images so that the pixel values are between 0 and 1 (necessary for using the neural network). The network has been pretrained on imagenet data so we need to normalize our new data with respect to the imagenet data. This way the images are placed near the distribution that the network was trained on and so gives the network data that it is ‘used to’ seeing. | . Training a Model using a Pretrained ResNet . ‘ResNet’ is the name of a particular kind of Convolutional Neural Network (CNN). Details of it will be covered later. . | The ResNet we will use has been pretrained. This means that it was trained to solve another image classification problem (namely ImageNet) and we are reusing the learned weights of that network as a starting point for a new imaging problem. . | Why ResNet and not some other architecture? From looking at benchmarks it has been found that ResNet generally ‘just works well’ for image tasks. (See also question in Q &amp; A section below). . | Here’s how to create a CNN with the fastai library: . learn = create_cnn(data, models.resnet34, metrics=error_rate) . | data is the DataBunch object of the cats/dogs data we created earlier. . | Here we are using a variant of ResNet called resnet34. The 34 simply means it has 34 layers. There are others avaiable with 18, 50, and more layers. . | One-cycle policy: . learn.fit_one_cycle(4) . | People train neural networks using Stochastic Gradient Descent (SGD). Here the training set is divided into random batches (say of size 64) and the network weights are updated after each batch. After the network has seen all the batches, this is called an epoch. The rate at which the weights are changed is called the learning rate. Typically people set this to a single value that remains unchanged during an epoch. Not here though. . | The One-cycle policy is a way of training the neural network using SGD faster by varying the learning rate and solver momentum over a group of epochs. . | Sylvain explains (source): . He [Leslie] recommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude. . | Here are plots of how the learning rate and momentum vary over the iterations (batches): . . | The peak of the learning rate has a value of 1x the inputted learning rate. The bottom value is 0.1x the inputted learning rate. The bottom of the momentum value is 0.85x the inputted momentum value. . | The momentum varies contra to the learning rate. What’s the intuition behind this? When the learning rate is high we want momentum to be lower. This enables the SGD to quickly change directions and find a flatter region in parameter space. . | . | Learning Rate Finder . The method is basically successively increasing $ eta$ every batch using either a linear or exponential schedule and looking the loss. While $ eta$ has a good value, the loss will be decreasing. When $ eta$ gets too large the loss will start to increase. You can plot the loss versus $ eta$ and see by eye a learning rate that is largest where the loss is decreasing fastest. . | learn.lr_find() learn.recorder.plot() . | . | More on this will be covered in the next lesson. | . | . Getting Started With the Notebooks . All the course notebooks for part 1 are found here: notebooks [github]. | The course guide can be found here: course guide. | For running and experimenting with the fastai notebooks I personally like to use: kaggle kernels | or google colab. | . | . Jeremy Says… . Don’t try to stop and understand everything. | Don’t waste your time, learn Jupyter keyboard shortcuts. Learn 4 to 5 each day. | Please run the code, really run the code. Don’t go deep on theory. Play with the code, see what goes in and what comes out. | Pick one project. Do it really well. Make it fantastic. | Run this notebook (lesson1-pets.ipynb), but then get your own dataset and run it! (extra emphasis: do this!) If you have a lot of categories, don’t run confusion matrix, run… interp.most_confused(min_val=n) | (Source: Robert Bracco) . Q &amp; A . As GPU mem will be in power of 2, doesn’t size 256 sound more practical considering GPU utilization compared to 224? . The brief answer is that the models are designed so that the final layer is of size 7 by 7, so we actually want something where if you go 7 times 2 a bunch of times (224 = 7x2x2x2x2x2), then you end up with something that’s a good size. Objects often appear in the middle of an image in the ImageNet dataset. After 5 maxpools, a 224x224 will be 7x7 meaning that it will have a centerpoint. A 256x256 image will be 8x8 and not have a distinct centerpoint. . | Why resnet and not inception architecture? . Resnet is Good Enough! See the DAWN benchmarks - the top 4 are all Resnet.You can consider different models for different use cases. For example, if you want to do edge computing, mobile apps, Jeremy still suggests running the model on the local server and port results to the mobile device. But if you want to run something on the low powered device, there are special architectures for that. . Inception is pretty memory intensive. fastai wants to show you ways to run your model without much fine-tuning and still achieve good results. The kind of stuff that always tends to work. Resnet works well on a lot of image classification applications. . | Will the library use multi GPUs in parallel by default? . The library will use multiple CPUs by default but just one GPU by default. We probably won’t be looking at multi GPU until part 2. It’s easy to do and you’ll find it on the forum, but most people won’t be needing to use that now. . | . Links and References . Link to Lesson 1 lecture | Homework notebooks: Notebook 1: lesson1-pets.pynb | . | Detailed lesson notes - thanks to @hiromi | Stanford DAWN Deep Learning Benchmark (DAWNBench) · | [1311.2901] Visualizing and Understanding Convolutional Networks | Another data science student’s blog – The 1cycle policy | Learning Rate Finder Paper | .",
            "url": "https://jimypbr.github.io/blog/deep-learning/machine-learning/computer-vision/fastai/fastai-v3-notes/2019/07/06/fast-ai-lesson-1-notes-image-classification.html",
            "relUrl": "/deep-learning/machine-learning/computer-vision/fastai/fastai-v3-notes/2019/07/06/fast-ai-lesson-1-notes-image-classification.html",
            "date": " • Jul 6, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "My First Pull Request! :)",
            "content": "I’m very proud to have made my first open source contribution! :-) I added a feature to the FastAI deep learning library to make its data types ‘countable’ and thus work with the collections.Counter class. . Problem . While I was working through the image classification homework from lesson 2. I wanted to check the how many images of each class there were in my data block. The best way to count the number of values in a collection in python is to use the collections.Counter class which creates a dictionary mapping value to count. . However when I tried this with the data block I got this: . &gt; Counter(data.train_ds.y) Counter({ Category chimp 1 Category gorilla 1 Category gorilla 1 Category chimp 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 Category gorilla 1 ... . Solution . This problem is caused by fact that there was no __eq__ implemented for the Category class. When different Category objects were compared python’s default equality would only check whether they were literally the same object rather than checking their values. To get an object to work with a dictionary class in python you also have to implement a __hash__ method. . I confirmed this with the hot patch: . &gt; Category.__eq__ = lambda self, that: self.data == that.data &gt; Category.__hash__ = lambda self: hash(self.obj) &gt; Counter(data.train_ds.y) Counter({Category orangutan: 56, Category gorilla: 177, Category chimp: 173}) . With Sylvain Gugger’s guidance, I then implemented __eq__ method properly in fastai for the ground class ItemBase so that all of the different data classes in fastai could have equality. Hash didn’t make sense for all the subclasses (like floats or arrays of numbers), so we compromised on implementing hash methods only on the subclasses where it made sense. . Here is the link to my pull request: https://github.com/fastai/fastai/pull/1717. . Aside: Making Python Objects Counter-Ready . In order to make your python objects play nice with dictionary’s they need to override two python built-ins: . __eq__ | __hash__ | Suppose that the python object contains some value val that defines the object’s uniquness. . Let’s create a python class for categories called Cat: . class Cat: def __init__(self, val): self.val = val def __str__(self): return f&#39;Cat({self.val})&#39; def __repr__(self): return f&#39;Cat({self.val})&#39; . This class won’t work properly with Counters: . &gt; xs = [Cat(2), Cat(2), Cat(1), Cat(3)] &gt; Counter(xs) Counter({Cat(2): 1, Cat(2): 1, Cat(1): 1, Cat(3): 1}) . Equality doesn’t work: . &gt; Cat(2) == Cat(2) False . Two objects with the same value don’t have the same hash: . &gt; hash(Cat(2)) -9223372036573193412 &gt; hash(Cat(2)) 281542562 . You have to implement the hash and equality built-ins: . class Cat: def __init__(self, val): self.val = val def __str__(self): return f&#39;Cat({self.val})&#39; def __repr__(self): return f&#39;Cat({self.val})&#39; def __eq__(self, other): return self.val == other.val def __hash__(self): return hash(self.val) . Now it works: . &gt; xs = [Cat(2), Cat(2), Cat(1), Cat(3)] Counter(xs) Counter({Cat(2): 2, Cat(1): 1, Cat(3): 1}) . How does this work? The Cat objects are being used as keys in the Counter. When a new Cat object comes into the Counter we need to compare it with all the other keys already in the Counter. If a Cat object of the same value is there already then we need to increment the value associated with that Cat object. . For efficiency, however, dictionaries in python don’t store the keys in a big list rather in buckets. When a new Cat object comes into the Counter it is assigned to a bucket using its hash value. Here three things can happen. . If the bucket is empty then store the value there. | If the bucket isn’t empty compare the incoming object with the objects there using eq. If they are the same, increment the counter. | If they are different you have a hash collision. Store the incoming object in the bucket and set the counter value to 1. | In Summary . For correctness with dictionaries: . obj1 == obj2 if obj1.val == obj2.val | hash(obj1) == hash(obj2) if obj1.val == obj2.val | If the object is to be used as a key in a Counter we need to be able to correctly compare it to other keys in the Counter. If two objects are equal then we know that they are the same key and we can increment the counter. Two objects with the same value need to be hashed to the same bucket. . For efficiency with dictionaries: . hash(obj1) should ideally != hash(obj2) if obj1.val != obj2.val | It is possible, though undesirable, that two objects with different values get hashed to the same bucket. This is called a hash collision. This isn’t a correctness problem, rather an efficiency problem. Every hash collision is like an if statement in the dictionary to specially handle those cases. Ideally, every unique value should have its own unique hash so that there are no hash collisions. If we imagine the worst case where our hashing algorithm is something like def hash(x): return 0, then ever item ends up in the same bucket. To look up an item in this dictionary where everything is a hash collision we’d have to brute-force it and on average look at every item individually before we find what we are looking for. This would reduce the dictionary look-up performance to $ mathcal{O}(N)$, the same as an unordered list, instead of the $ mathcal{O}(1)$ performance that it should have. . Links . Link to pull request: https://github.com/fastai/fastai/pull/1717 | Link to forum post: https://forums.fast.ai/t/get-value-counts-from-a-imagedatabunch/38784/21 | .",
            "url": "https://jimypbr.github.io/blog/python/fastai/2019/04/01/my-first-pull-request.html",
            "relUrl": "/python/fastai/2019/04/01/my-first-pull-request.html",
            "date": " • Apr 1, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "SIFTing Images",
            "content": ". The Fleuron Project . I have been involved in the Fleuron project this year. The aim of this project is to use computer vision to extract printers’ ornaments from a large corpus of ~150,000 scanned documents (32 million pages) from the 18th century. Printed books in the 18th century were highly decorated with miniature pieces of printed artwork - ‘ornaments’. Their pages were adorned with ornaments that ranged from small floral embellishments to large and intricate head- and tailpieces, depicting all manner of people, places, and things. Printers’ ornaments are of interest to historians from many disciplines, not least for their importance as examples of early graphic design and craftsmanship. They can help solve the mysteries of the book trade, and they can be used to detect piracy and fraud. . In this project an OpenCV based code was developed to automatically detect ornaments in a scanned image of a page and extract them into their own file. This code worked very well and extracted over 3 million images from the data, but it was quite over-sensitive in its detection so there were many false-positives. The code was heuristic based and didn’t use any machine intelligence to further evaluate the potential images for validity. We therefore chose to tune the code to have good recall at the expense of precision – i.e. we would rather it didn’t miss valid images, even if it means that some invalid images get through too. Often these invalid images were of blocks of text so we initially experimented with using OCR to catch these cases. However this had the unwanted effect of making recall worse. We decided a better solution would be to train a machine learning classifier to discriminate between the valid and invalid images. . My contribution to the project was to use the extraction code to generate data, which I then hand-labelled to create a training set to train a machine learning based filter to remove the bad images. The final filtered dataset is presented on the website: http://fleuron.lib.cam.ac.uk (EDIT 2021: Fleuron now defunct. Has since become Compositor), which I also designed and built. In this blog post I will describe the methodology and results of the image filtering part of the project. . Extraction . The first challenge is to extract the ornaments from the raw page scans. This is an example of a page containing two ornaments, at the top of the page and at the start of the text body: . . We required an algorithm that could ignore the text and draw bounding boxes around the two ornaments on the page. To solve this problem we enlisted Dirk Gorissen to develop a method using Python and OpenCV. I will not dive deeply into how Dirk’s algorithm works here. Basically it uses combines heuristics of where ornaments are typically located and how they look with various image filtering techniques to weed out text and other artifacts on the page to leave just the artwork intact. . Here is a demonstration of how each of the different stages of the algorithm work using on the single page shown above as an example: . . Ornaments are visually very dense compared to the text. In the first stage the image is cleaned removing dust and stains in the white space of the page. Then through several iterations of blurring and contouring are applied until just the ornaments are left as single contours as seen in stage 5. A bounding box is then drawn around these contours and content of these boxes is then extracted from the original image. . This method is simple and effective, but it is also apt to falsely classifying blocks of text. In the following example you can see clearly how this can happen: . . After running extraction on all of the pages, I found that in a random sample of the images, most of them were just images of blocks of text! However, given that most of the pages in the dataset contain only text and no ornaments, perhaps this is to be expected even if the algorithm is fairly good at removing text. . After extracting the ornaments from a large sample of the books, I hand labeled a random sample of 15000 images as valid and invalid. Here is a collage of valid images: . . Here is a collage of invalid images that we want to filter out: . . Image Filtering Pipeline . The choice of image representation is essential to getting a well performing machine learning based classifier. The Images are black and white and so we can’t use any colour features and contain very rich textures. The pipeline for the image filtering system: . Create a labelled data set for training | Represent each training image by a vector using Bag of Visual Words | Train a classifier on the vector to discriminate between valid and invalid images | Apply the classifier to unseen images in the data set. | Bag of Visual Words (BoVW) . The Bag of Visual Words (BoVW) method is a common feature representation of images in computer vision. The method is directly inspired by the Bag of Words (BoW) method used in text classificiation. In the BoW method, the basic idea is that a text document is split up into its component words. Each of the words in the document is then matched to a word in the dictionary, and the number of unique words in the document is counted. The text document is then represented as a sparse histogram of word counts that is as long as the dictionary. . This histogram can be interpreted as a vector in some high dimensional space, and two different documents will be represented by two different vectors. So for a dictionary with $D$ words the vector for document $i$ is: . [v_i = [n(w_1,i), n(w_2,i), …, n(w_D, i)]] . Where $n(w)$ counts the number of occurrences of word $w$. The distance between these two vectors (e.g. L2, cosine, etc) can therefore be used as a proxy for the similarity of the two documents. If everything is working well, then a low distance will indicate high similarity and a large distance will represent a high dissimilarity. With this representation we are able to throw machine learning algorithms at the data or do document retrieval. . BoVW is exactly the same method except that instead of using actual words it uses ‘visual words’ extracted from the images. Visual words basically take the form of ‘iconic’ patches or fragments of an image. . 1. Extract notable features from the images . 2. Learn a visual dictionary . Use a clustering algorithm like k-means with a apriori number of clusters (&gt;1000) to learn a set of $k$ compound visual words. . . 3. Quantize features using the visual vocabulary . Now we could then take an image, find its visual words and match each of those words to their nearest equivalent in the dictionary. . 4. Represent images by histogram of visual word counts . By counting how many times a word in the dictionary is matched, the image can be re-represented as a histogram of word counts: . . Similar looking images will have contains many of the same words and counts. . SIFT - The Visual Word . Now that we have outlined the concept of the BoVW method, what do we actually use as the ‘visual word’? To create the visual words I used SIFT - ‘Scale Invariant Feature Transform’. SIFT is a method for detecting multiple interesting keypoints in a grey-scale image and describing each of those points using a 128 dimensional vector. The SIFT descriptor is invariant to scale, rotation, and illumination, which is why it is such a popular method in classification and CBIR. An excellent technical description of SIFT can be found here. . OpenCV has an implementation of a SIFT detector included. The following code finds all the keypoints in an image and draws them back onto the image. . img = cv2.imread(&#39;image_2.png&#39;) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) kp, desc = sift.detectAndCompute(img_gray, None) img2 = cv2.drawKeypoints(img_gray, kp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) cv2.imwrite(&#39;image_sift_2.png&#39;, img2) . Here is the output of this for two images from the dataset, one valid and the other invalid: . There is a simple improvement that can be made to SIFT called RootSIFT. RootSIFT is a small modification to the SIFT descriptor that corrects the L2 distance between two SIFT descriptors. This generally always improves performance for classification and image retrieval. Here is an implementation in python: . def rootsift_descriptor(f): &quot;&quot;&quot; Extract root sift descriptors from image stored in file f :param: f : str or unicode filename :return: desc : numpy array [n_sift_desc, 128] &quot;&quot;&quot; img = cv2.imread(f) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) sift = cv2.SIFT() kp, desc = sift.detectAndCompute(img_gray, None) if desc is None: print(&#39;Warning: No SIFT features found in {}&#39;.format(f)) return None desc /= (desc.sum(axis=1, keepdims=True) + 1e-7) desc = np.sqrt(desc) return desc . Building a Visual Word Dictionary . To create a visual word dictionary we need to first collect and store all the RootSIFT descriptors from a large sample of images from our dataset. Here I used a sample size of 50,000 images. For 50,000 images, $N approx 1~ billion$. The following script iterates through every image in the target directory, finds the RootSIFT descriptors of the image, and then stores them in a large $N times128$ array in a HDF5 file. . import os import glob import cv2 import tables import numpy as np from bovw import rootsift_descriptor from random import sample, shuffle def create_descriptor_bag(filelist): &quot;&quot;&quot; creates an array of descriptor vectors generated from every file in filelist &quot;&quot;&quot; X = np.empty((0, 128), dtype=np.float32) N = len(filelist) for n, f in enumerate(filelist): print(&#39;Processing file: {} of {}...&#39;.format(n, N)) desc = rootsift_descriptor(f) if desc is not None: X = np.vstack((X, desc)) return X def main(): h5f = tables.openFile(&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;, &#39;w&#39;) atom = tables.Atom.from_dtype(np.dtype(&#39;Float32&#39;)) ds = h5f.createEArray(h5f.root, &#39;descriptors&#39;, atom, shape=(0, 128), expectedrows=1000000) PATH = &#39;/home/jb914/ECCO_dict/random50k/&#39; all_files = glob.glob(os.path.join(PATH, &#39;*.png&#39;)) rand_sample = [all_files[i] for i in sample(range(1, len(all_files)), 5000)] chunk_size = 100 for i in xrange(0,len(rand_sample), chunk_size): print(&#39;Creating rootsift descriptor bag...&#39;) X = create_descriptor_bag(rand_sample[i:i+chunk_size]) print(X[0].shape) print(len(X)) print(&#39;Writing file: rootsift_vectors_5000.hdf&#39;) for i in xrange(len(X)): ds.append(X[i][None]) ds.flush() #ds[:] = X h5f.close() if __name__ == &#39;__main__&#39;: main() . HDF5 files are great for storing large multidimensional arrays of data to disk because they store the meta-data of the array dimensions and allow for streaming the data from disk to memory. This is especially useful when the full data is much larger than memory like here. . Creating the dictionary is the hardest and most time consuming part with BoVW. There are many vectors to cluster, the number of words is very large (between 1000 and 1,000,000), and the vectors are high dimensional. This stretches the capabilities of many clustering algorithms in all possible ways. In my experiments I found that the standard K-Means clustering algorithm quickly became intractable for larger numbers of vectors and clusters. Moreover the algorithm is offline - it needs to see all the data at once. Algorithms better suited for this task are approximate k-means (AKM) and mini-batch k-means. . I found success with two open source implementations of these in fastcluster (AKM), and in MiniBatchKMeans from scikit-learn. Fastcluster has the advantage that it uses distributed parallelism via MPI to split the large data up across multiple machines. However this useful code lacks documentation and no longer maintained. MiniBatchKMeans on the other hand isn’t parallel, however it does allow for streaming of the data through memory so it works great with HDF5. . In my experiments I found that setting the dictionary size to 20,000 words was sufficient. . The following script can stream a HDF5 file in a user defined number of chunks performing clustering with those chunks. The total clustering time for this was approximately 24 hours running in serial on a Intel Xeon Ivybridge CPU. . from __future__ import print_function import numpy as np import tables import pickle from sklearn.cluster import MiniBatchKMeans from time import time def main(n_clusters, chunk=0, n_chunks=32, checkpoint_file=None): datah5f = tables.open_file(&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;, &#39;r&#39;) shape = datah5f.root.descriptors.shape datah5f.close() print(&#39;Read in SIFT data with size in chunks: &#39;, shape) print(&#39;Running MiniBatchKMeans with cluster sizes: &#39;, n_clusters) c = n_clusters print(&#39;n_clusters:&#39;, c) if checkpoint_file: mbkm = pickle.load(open(checkpoint_file, &#39;r&#39;)) else: mbkm = MiniBatchKMeans(n_clusters=c, batch_size=10000, init_size=30000, init=&#39;random&#39;, compute_labels=False) step = shape[0] / n_chunks start_i = chunk*step for i in xrange(start_i, shape[0], step): datah5f = tables.open_file(&#39;/scratch/ECCO/rootsift_vectors_{}.hdf&#39;.format(datasize), &#39;r&#39;) X = datah5f.root.descriptors[i:i+step] datah5f.close() t0 = time() mbkm.partial_fit(X) print(&#39; t ({} of {}) Time taken: {}&#39;.format(chunk, n_chunks, time()-t0)) chunk += 1 pickle.dump(mbkm, open(&#39;chkpt_{}.p&#39;.format(n_clusters), &#39;w&#39;)) X = mbkm.cluster_centers_ print(X.shape) f = tables.open_file(&#39;fleuron_codebook_{}.hdf&#39;.format(n_clusters), &#39;w&#39;) atom = tables.Atom.from_dtype(X.dtype) ds = f.create_carray(f.root, &#39;clusters&#39;, atom, X.shape) ds[:] = X f.close() if __name__ == &#39;__main__&#39;: main(n_clusters=20000, chunk=0, n_chunks=4, checkpoint_file=None) . Matching Key Points to the Dictionary . With the dictionary created the next step is to represent all the images in the labeled training set as a histogram of matching keypoints. This is a nearest neighbour matching problem so with a brute force algorithm this is a $O(N)$ so this is slow for very high numbers of words. Faster nearest neighbour matching can be achieved with the FLANN library. OpenCV contains a wrapper for FLANN. I wrote a class that uses the FLANN matcher in OpenCV to match an array of descriptors to a codebook: . FLANN_INDEX_COMPOSITE = 3 FLANN_DIST_L2 = 1 class Codebook(object): def __init__(self, hdffile): clusterf = tables.open_file(hdffile) self._clusters = np.array(clusterf.get_node(&#39;/clusters&#39;)) clusterf.close() self._clusterids = np.array(xrange(0, self._clusters.shape[0]), dtype=np.int) self.n_clusters = self._clusters.shape[0] self._flann = cv2.flann_Index(self._clusters, dict(algorithm=FLANN_INDEX_COMPOSITE, distance=FLANN_DIST_L2, iterations=10, branching=16, trees=50)) def predict(self, Xdesc): &quot;&quot;&quot; Takes Xdesc a (n,m) numpy array of n img descriptors length m and returns (n,1) where every n has been assigned to a cluster id. &quot;&quot;&quot; (_, m) = Xdesc.shape (_, cm) = self._clusters.shape assert m == cm result, dists = self._flann.knnSearch(Xdesc, 1, params={}) return result . The following code takes a list of image files and a dictionary and returns the count vectors for each of those image files using the sparse matrix type from scipy. It is also multi-threaded using the joblib library: . def count_vector(f, codebook): &quot;&quot;&quot; Takes a list of SIFT vectors from an image and matches each SIFT vector to its nearest equivalent in the codebook :param: f : Image file path :return: countvec : sparse vector of counts for each visual-word in the codebook &quot;&quot;&quot; desc = rootsift_descriptor(f) if desc is None: # if no sift features found return 0 count vector return lil_matrix((1, codebook.n_clusters), dtype=np.int) matches = codebook.predict(desc) unique, counts = np.unique(matches, return_counts=True) countvec = lil_matrix((1, codebook.n_clusters), dtype=np.int) countvec[0, unique] = counts return countvec class CountVectorizer(object): def __init__(self, vocabulary_file, n_jobs=1): self._codebook = Codebook(hdffile=vocabulary_file) if n_jobs == -1: self.n_jobs = cpu_count() else: self.n_jobs = n_jobs def transform(self, images): &quot;&quot;&quot; Transform image files to a visual-word count matrix. :param: images : iterable An iterable of str or unicode filenames :return: X : sparse matrix, [n_images, n_visual_words] visual-word count matrix &quot;&quot;&quot; sparse_rows = Parallel(backend=&#39;threading&#39;, n_jobs=self.n_jobs)( (delayed(count_vector)(f, self._codebook) for f in images) ) X = lil_matrix((len(images), self._codebook.n_clusters), dtype=np.int) for i, sparse_row in enumerate(sparse_rows): X[i] = sparse_row return X.tocsr() . Given a list of files the following code will return the count vectors for those images: . vectorizer = CountVectorizer(&#39;codebook_20k.hdf&#39;, n_jobs=-1) Xcounts = vectorizer.transform(images) . tf-idf . Not all words are created equal, some are more frequent than others. This is the same in human language and in the create visual vocabulary. Words like ‘the’, ‘what’, ‘where’, etc will swamp the count vectors of english words in almost all english documents. Clearly they are less interesting than a rare word like ‘disestablishment’ and that we’d like two different documents both containing a word like ‘disestablishment’ to have a high similarity. So we’d like to reweight words which appear in few documents so that they have a higher importance, and words that appear in most documents to have lower importance. In another case, if a document only contained the word ‘disestablishment’ 1000 times should it be 1000 times more relevant than a document containing it once? So within an individual document we may want to reweight words that are repeated over and over so that they cannot artificially dominate. . These two reweightings can be achieved using tf-idf (term frequency inverse document frequency) weighting. This weighting is designed to reflect how important a particular word is in a document corpus. It is perfectly applicable in our visual word case also. Scikit-learn has an implementation of a tf-idf transformer for text classification that we can repurpose here. To following produces the final representation for the training data that we can use in the machine learning algorithm, $X$. . from sklearn.feature_extraction.text import TfidfTransformer transformer = TfidfTransformer() X = transformer.fit_transform(Xcount) . Visualisation of BoVW . That that we’ve transformed the images into tf-idf weighted, 20k dimensional, sparse vectors to visual word counts, we can visualise them and see if there is any apparent structure in this high dimensional data. Great algorithms for visualising high dimensional data are PCA and T-SNE, both of which have implementations in scikit-learn. I found here that PCA worked best. For high dimensional sparse data, the TruncatedSVD algorithm works best: . from sklearn.decomposition import TruncatedSVD svd = TruncatedSVD(n_components=2) Z = svd.fit_transform(X) . We can plot this with the images inlined and with colours representing the valid (red) and invalid (blue) labels: . . You can clearly see that there is clear structure in the higher dimensions and that the valid and invalid images separate quite well from each other. This is quite promising for the performance of a machine learning algorithm! . Classifying the Bad Images with Machine Learning . I tried a number of algorithms including Random forest, logistic regression and linear SVM. I found that SVM with a linear kernel by far performed the best compared to the other algorithms. . from sklearn.svm import LinearSVC clf = LinearSVC() clf.fit(X_train, y_train) . LinearSVC with the default settings performed very well with $97 %$ accuracy. High accuracy is generally a good sign, especially here where the numbers of valid and invalid images are of a similar size. Two other important statistics for classification are precision and recall. . Recall is a measure of what the probability that the classifier will identify and image as invalid given that it is invalid: $P( hat{y}=1 | y=1)$. You can think of recall as the ratio of the number of images correctly classed as invalid over the number of all invalid images. . Precision on the other hand is a measure of the probability that an image is invalid given that the classifier says it is invalid: $P(y=1| hat{y}=1)$. You can think of precision as the ratio of the number of images correctly classed as invalid over the number of all images classified. . The difference between them is subtle (here is a great explanation of the difference), but you may want to favour a trade-off of one for the other depending on your business case. In our case it is worse to misclassify a valid images as invalid because we are losing good images. We would much rather have some invalid images get through than lose good images, which is the same as favouring extra precision over recall. . We can tune the precision by adjusting the class weights of the Linear SVM, such that the penalty for classifying a valid image as invalid is much worse than classifying an invalid image as valid. I used cross-validation to find the best values for these. These give the valid images a weight of 20 and invalid images a weight of 0.1: . from sklearn.svm import LinearSVC clf = LinearSVC(class_weight={0: 20, 1: 0.1}) clf.fit(X_train, y_train) . This yielded the final performance of $95 %$ accuracy, $99.5 %$ precision, and $93.8 %$ recall: . Confusion matrix: [[1134 11] [ 161 2439]] Accuracy score: 0.954072096128 False Positive Rate: 0.00960698689956 False Negative Rate: 0.0619230769231 Precision: 0.995510204082 Recall: 0.938076923077 F1 Score: 0.965940594059 . The performance of approach this is very good. The trained classifier was then applied across the whole image dataset. In the end there were approximately 3 million invalid images and 2 million valid images detected. . Further Ideas . Image search . The BoVW approach is also very useful for image retrieval. This means that given some image we can find duplications and similar looking images in the rest of the image set simply by finding the BoVW vectors that are closest to that image’s own BoVW vector. This is just a nearest neighbour search. It is complicated by the number of images because scaling nearest neighbour search with large numbers of vectors that don’t necessarily fit into memory relies on more complicated algorithms. . Neural Networks . Convolutional Neural Networks (CNNs) have shown great application in image classification in recent years. While they perform well at classification, they also have the advantage that they can discover vector representations of the images given the just the raw pixels. So it doesn’t require all this work with inventing a representation for images such as BoVW. The downside is that they require a lot of data (10s of thousands of examples) to be effective. Rather than hand labelling more examples, it would be quicker to look at the output of images classified by the SVM, and eyeball any false negatives or false positives in there. Artificial data could also be created using image transformations like rotation and inversion. .",
            "url": "https://jimypbr.github.io/blog/python/machine-learning/computer-vision/2016/12/30/fleuronbovw.html",
            "relUrl": "/python/machine-learning/computer-vision/2016/12/30/fleuronbovw.html",
            "date": " • Dec 30, 2016"
        }
        
    
  
    
        ,"post15": {
            "title": "How to do Polymorphism in Clojure (3/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . Arithmetic with Mixed Types . We have so far built a number system with five different types and an add function that can take any two numbers of the same type and add them together with the same interface. . However what if I wanted to add a Complex-r and a Real together? We would need to convert the Real to a Complex-r and then add them together. We could do this by adding more multimethods: . (defmethod add [::complex-r ::real] (...)) (defmethod add [::real ::complex-r] (...)) . You can see that doing this for all combinations of types would lead to a combinatorial explosion of new multimethods! Clojure’s hierarchies can enable us to solve this problem without the need to write a factorial number of functions. . We can side-step the problem of writing a combinatorical number of new functions by noting that to add two different types of number together we have to promote one of the numbers to be the same type as the other. With this in mind we can create a catch-all method for add that catches all the cases of mixed types, coerces the types to be the same and then calls add again with the converted types. It will then call one of the previously defined add methods where the types are the same. . We can implement the catch-all case by creating a new abstraction ::number at the top of the hierarchy where every number we have created so far is a descendant of ::number: . (derive ::complex ::number) (derive ::integer ::number) (derive ::float ::number) (derive ::rational ::number) . We can now create the catch-all add method that will that will matc [::number ::number]: . (defmethod add [::number ::number] [n1 n2] (apply add (coerce-types n1 n2))) . This works because any combination of different types will fail all the rules of the other multimethods except this one, because all of our types are descendants of ::number. This method then calls the coercion function (which I will define later) to convert the arguments into the same type and then calls the add multi-function with these converted arguments. This will then find the correct multimethod for the now uniform types return the result. . Type Coercion with the Numeric Tower . We can convert any Integer to a Rational without a loss of information. You cannot convert any Rational to an Integer without a loss of information though. Similarly any Rational can be converted to a Float, and any Float can be converted to a Complex number. . This chain of conversions is the numeric tower: . Integer -&gt; Rational -&gt; Float -&gt; Complex | . We want to be able to call a function raise on one of our types and get back the same numeric value, but represent by the next type in the tower. The function depends on the type of the argument so we can create another protocol: . (defprotocol PRaise (raise [x] &quot;raises a number type to the next type in the numeric tower with equivalent value&quot;)) (extend-protocol PRaise Int (raise [x] (rational (:n x) 1)) Rational (raise [x] (float (/ (numer x) (double (denom x))))) Float (raise [x] (complex-r (:n x) 0))) . Given a pair of types, e.g. [::integer ::float], we need a way to encode the fact that ::float is higher in the tower than ::integer, and so ::integer needs to be raised until it is a ::float. This could be done with a map associating the types to a rank number, but this is pretty inflexible if we add more types. . The numeric tower is a hierarchy so we are actually better off using Clojure’s ad-hoc hierarchies again. In Clojure there is a global hierarchy structure which we have used so far for the arithmetic, but you are free to create your own hierarchies with make-hierarchy. This exists independently of the global hierarchy we used earlier. This is handy because the numeric tower hierarchy is different from the arithmetic hierarchy. . (def numeric-tower (-&gt; (make-hierarchy) (derive ::complex ::complex-p) (derive ::complex ::complex-r) (derive ::float ::complex) (derive ::rational ::float) (derive ::integer ::rational))) . Using this we can create comparator functions for two keyword types. For example: . =&gt; (higher? ::float ::integer) true =&gt; (lower? ::rational ::complex-r) true . These are easy to implement using the functions for querying hierarchies, ancestors and descendents: . (defn higher? &quot;Is type 1 higher in the numeric tower than type 2?&quot; [t1 t2] (if (contains? (ancestors numeric-tower t2) t1) true false)) (defn lower? &quot;Is type 1 lower in the numeric tower than type 2?&quot; [t1 t2] (if (contains? (descendants numeric-tower t2) t1) true false)) . With these functions we can implement coerce-types: . (defn coerce-types &quot;Given two different number types raise the lesser type up to be the same as the greater type&quot; [x y] (let [t1 (kind x) t2 (kind y)] (cond (lower? t1 t2) (recur (raise x) y) (higher? t1 t2) (recur x (raise y)) :else [x y]))) . Trying this out in a REPL: . =&gt; (raise (integer 3)) #numbers.core.Rational{:n 3, :d 1} =&gt; (raise (float 4)) #numbers.core.Complex-r{:real 4.0, :imag 0} =&gt; (coerce-types (integer 4) (rational 5 6)) [#numbers.core.Rational{:n 4, :d 1} #numbers.core.Rational{:n 5, :d 6}] =&gt; (coerce-types (rational 5 6) (complex-r 7 8)) [#numbers.core.Complex-r{:real 0.8333333333333333, :imag 0} #numbers.core.Complex-r{:real 7, :imag 8}] . Final Product and Further Ideas . We have implemented a number system that can represent integers, floating point numbers, rational numbers, rectangular complex numbers, and polar complex numbers. It can perform basic binary arithmetic operations add, subtract, multiply, and divide on any combination of number types. . Let’s demonstrate the final product in the REPL: . (add (integer 4) (integer 6)) #numbers.core.Int{:n 10} (add (integer 4) (float 6)) #numbers.core.Float{:n 10.0} (add (rational 5 6) (float 6)) #numbers.core.Float{:n 6.833333333333333} (mul (float 6) (complex-p 10 0.5)) #numbers.core.Complex-p{:magnitude 60.0, :angle 0.5} (div (integer 5) (rational 7 5)) #numbers.core.Rational{:n 25, :d 7} (div (integer 5) (complex-r 7 5)) #numbers.core.Complex-p{:magnitude 0.5812381937190965, :angle -0.6202494859828215} . Seems to work fairly well! . I could continue building on this, but that would be beyond the scope of this blog post. There are a few things worth thinking about nonetheless. One aesthetic improvement would be implementing a pretty REPL representation of the custom number types. E.g. having 3/4 instead of Rational(:n 3 :d 4). Clojure has a single function that takes care of printing things: clojure.lang.print-method. This is simply another multimethod like we’ve already been using. Adding nicer printing is straight-forward: . ;; nicer printing for rationals (defmethod print-method Rational [x ^java.io.Writer writer] (.write writer (str (numer x) / (denom x)))) ;; and similarly for the other types... ;; ... . In SICP there is also an exercise to extend the number system to be implemented purely with its own types. So while Integer and Float types would be wrappers for Clojure’s own types, the Rational, Complex-r, and Complex-p types could themselves be composed of any of the other types. So you could have a Rational number whose demoninator and numerator were complex numbers. Or conversely a complex number whose real and imaginary parts were rational numbers. This would be achieved with some modification to the existing code. You would need to replace all uses of Clojure’s primitive arithmetic functions (+, *, etc.) with our multimethods (add, mult, etc), and also create our own implementations of sqrt, sin, cos, and atan that handled our number types. This could be done by adding new protocols. . Final Thoughts . This is a nice non-trivial program from SICP that demonstrates the ideas and challenges in polymorphism. Implementing this in Clojure required us to use just about every feature for polymorphism in Clojure’s core library. But I am satified with how Clojure could handle everything without any requiring any ‘hacks’ or major redesigns. . I think that this highlights one of the good aspects of Clojure’s design and philosophy. Namely, the decoupling (or ‘decomplecting’) of ideas in the language. In this demo there are the following concepts: . Data (in the form of records). | Functions. | Single and multiple dispatch (protocols and multimethods) to functions. | Hierarchies of types. | Functions and data are decoupled because records are just data and you can’t attach methods onto records in Clojure the way you can with class methods in OOP. You don’t need getter/setter methods because records are maps so you just the functions for maps. . Data and dispatch are decoupled. You do not need to know at design time which protocols or multimethods are going to use your record type. In Java where polymorphic single-dispatch is achieved via interfaces or abstract classes you need explicitly implement or extend your class when you write it. In Clojure you can add protocols or multimethods to any existing type when are where you want. . Dispatch in Clojure is simply a fancy wrapper for functions. For a user of a protocol, multimethod, or function it makes no difference how it is implemented - it looks exactly the same. This is a big win for extending or refactoring code without breaking things. For example, say we only had a complex type complex-r and implemented the methods real, imag, magnitude, and angle as functions via defn. But then later we required the complex-p type and it also needed the same methods. Refactoring the existing functions (real, imag, etc) into a protocol will make no difference to code already using these functions with the complex-r type - it looks and acts just like a function. Multimethods are the same. . Type hierarchies can be decoupled from types. In this example we built a hierarchy for the number types using namespaced keywords like ::complex-r. This exists independently from the records we defined. It is coupled to the record types via a one-to-one mapping of the records to a keyword, implemented by the protocol PNumberKind. This decoupling allowed us to create further abstractions such as ::number and ::complex. In Java you’d have to create abstract base classes retroactively and subclass the existing types, which would be a redesign. This decoupling was also useful later for the numeric tower where we actually required a completely different hierarchy of the types. This was made possible in Clojure because you can create multiple hierarchies ad-hoc - hierarchies are just data. You could even implement multimethods with different type hierarchies this way. In OOP this kind of polymorphic dispatch is strongly coupled to your class hierarchies, which you can’t just change. . Source Code . The complete source code of this tutorial can be found here: jimypbr/clojure-numbers. . References and Further Reading . SICP section 2.4 | Watch ‘Simple Made Easy’ by Rich Hickey. It’s a great talk that explains how Clojure aims to decouple key programming concepts from each other: Simple Made Easy, Rich Hickey | For more about multiple dispatch, Eli Bendersky’s series of blog posts are great: http://eli.thegreenplace.net/2016/a-polyglots-guide-to-multiple-dispatch | This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/23/polyclojure3.html",
            "relUrl": "/clojure/functional-programming/2016/11/23/polyclojure3.html",
            "date": " • Nov 23, 2016"
        }
        
    
  
    
        ,"post16": {
            "title": "How to do Polymorphism in Clojure (2/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . More Number Types . Let’s extend our number package with more number types: An Integer type, a Float type for real numbers, and a Rational type for fractions. Like before we create the records and constructors: . (defrecord Int [n]) (defrecord Rational [n d]) (defrecord Float [n]) ;; constructors (defn integer &quot;Make a new integer&quot; [n] {:pre [(number? n)]} (-&gt;Int (int n))) (defn float &quot;Make a new float&quot; [n] {:pre [(number? n)]} (-&gt;Float (double n))) (defn rational &quot;Make a new rational&quot; [n d] {:pre [(number? n) (number? d)]} (let [g (gcd n d)] (-&gt;Rational (/ n g) (/ d g)))) . Trying these out in the REPL: . =&gt; (float 3) #numbers.core.Float{:n 3.0} =&gt; (integer 6) #numbers.core.Int{:n 4} =&gt; (rational 6 3) #numbers.core.Rational{:n 2, :d 1} . Generic Arithmetic . We want to create an add function that can be called with either two integers, two rationals, two reals, or two complex types and do the right thing in every case. Protocols in Clojure allow for single dispatch only. Here we need to dispatch on the types of multiple arguments. . We could hack this with a mega-function that is just a big conditional statement: . (def mega-add &quot;one add to rule them all (don&#39;t do this)&quot; [n1 n2] (cond (and (= (type n1) Int) (= (type n2) Int)) (add-int n1 n2) (and (= (type n1) Float) (= (type n2) Float) (add-real n1 n2) ;; etc :else (throw &quot;unknown types&quot;))) . The problem with this solution is that it is closed for business. If a user of the our number library desired to extend the number system with a new type of number (e.g. a BigInt), they’d have to break in and edit this function directly. . Multi-Methods . Clojure’s core library provides multiple dispatch via multi-methods. While protocols in Clojure perform single-dispatch on just the type of the first argument, multi-methods are much more general and allow the programmer to define their own rules for dispatch using any number of arguments. You are not limited to dispatch with just the types of the arguments, but also their values. . Let’s throw out mega-add and do it properly with multi-methods. The multi-method is defined using the defmulti macro. It takes a docstring and a dispatch function as its arguments. For adding, the dispatch function will be mapped to the two numbers as arguments and so return a vector of the types of the arguments: . (defmulti add &quot;Generic add&quot; class) . So if we provided two Ints then the dispatch would return [Int Int]. With the dispatch machinery is in place, we now need to add the implementations for each of the types. This is done with defmethod, which defines a method for each valid output of the dispatch function: . (defmethod add [Int Int] &quot;Add two integers together&quot; [n1 n2] (integer (+ (:n n1) (:n n2)))) (defmethod add [Float Float] &quot;Add two floats together&quot; [n1 n2] (float (+ (:n n1) (:n n2)))) ;; etc . Trying this out in the repl: . =&gt; (add (integer 4) (integer 6)) #numbers.core.Int{:n 10} =&gt; (add (float 5) (float 10)) #numbers.core.Float{:n 15.0} . Neat! Multi-methods are easy to extend too. If I wanted to create a new number type (e.g. a BigInt), then all I need to do is add a new method with defmethod for the case of [BigInt BigInt]. . Similarly we can reimplement the add function defined previously for the two complex number types, using the new multi-method machinery: . (defmethod add [Complex-r Complex-r] &quot;Add two complex-r numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) (defmethod add [Complex-p Complex-p] &quot;Add two complex-p numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) . This works fine, but previously add for the two different complex number types was a single function, whereas now we have repetition. Moreover we can’t add a Complex-r to a Complex-p like we could before. . Multimethods have provided a lot of extensibility to new number types, but at the same time we have lost the polymorphic nature we had in the arithmetic functions of the two complex types. I will address this problem in the next section. . Keywords and Dispatch Hierarchies . We have an impression that Complex-r and Complex-p are subtypes of some imaginary abstract type Complex. However Clojure does not offer any notion of an ‘abstract type’ as we are used to in OOP. Instead Clojure provides an ad-hoc dynamic hierarchy system. The hierarchy system supports derivation relationships between names (either symbols or keywords), and relationships between classes and names. . The derive function creates these relationships, and the isa? function tests for their existence. We will use namespaced keywords (double colon) to represent the number types: . =&gt; (derive ::complex-r ::complex) =&gt; (derive ::complex-p ::complex) =&gt; (isa? ::complex-r ::complex) true =&gt; (isa? ::complex-p ::complex) true =&gt; (isa? ::complex-r ::complex-p) false =&gt; (ancestors ::complex-r) #{::complex} =&gt; (ancestors ::complex-p) #{::complex} . What we want to do is rewrite the arithmetic multi-methods to dispatch using these namespaced keywords in place of the number types. The complex add method could then be reduced to matching arguments that satisfy: [::complex ::complex]. To do this, we will require a one-to-one mapping of each type to its associated keyword: . Complex-r =&gt; ::complex-r | Complex-p =&gt; ::complex-p | Float =&gt; ::float | Rational =&gt; ::rational | Int =&gt; ::integer | . We could do this with a global lookup table or add the keywords to the record definitions, but these are cludgy solutions. The first requires maintaining some global data, and the second repeats information and forces us to rewrite the record definition, which would break existing code. A cleaner solution is just to create another protocol and extend our number types with it: . (defprotocol PNumberKind (kind [n] &quot;The keyword name for the kind of the number n&quot;)) (extend-protocol PNumberKind Complex-r (kind [z] ::complex-r) Complex-p (kind [z] ::complex-p) Float (kind [z] ::float) Rational (kind [z] ::rational) Int (kind [z] ::integer)) . In the REPL: . =&gt; (kind (integer 3)) :numbers.core/integer =&gt; (kind (complex-r 4 5)) :numbers.core/complex-r . We can now update the dispatch function used by the multimethod to dispatch using kind: . (defmulti add &quot;Generic add&quot; kind) . The methods can now be rewritten as: . (defmethod add [::integer ::integer] &quot;Add two integers together&quot; [n1 n2] (integer (+ (:n n1) (:n n2)))) (defmethod add [::float ::float] &quot;Add two reals together&quot; [n1 n2] (real (+ (:n n1) (:n n2)))) (defmethod add [::complex ::complex] &quot;Add two complex-p numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) . Since we added the rule (derive ::complex-r ::complex) to the hierarchy, the multimethod called with ::complex-r or ::complex-p implicitly satisfy the rule [::complex ::complex]. The hierarchy has therefore allowed us the implement a polymorphic add for adding different representations of complex numbers and their combinations. If we added more complex number representations, the generic add method for complex numbers would accomodate them automatically without modification. . Let’s try this in the REPL: . =&gt; (add (complex-r 2 3) (complex-r 6 7)) #numbers.core.Complex-r{:real 8, :imag 10} =&gt; (add (complex-p 4 5) (complex-p 1 0)) #numbers.core.Complex-r{:real 2.1346487418529048, :imag -3.835697098652554} =&gt; (add (complex-p 3 4) (complex-r 5 6)) #numbers.core.Complex-r{:real 3.039069137409164, :imag 3.7295925140762156} . This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/22/polyclojure2.html",
            "relUrl": "/clojure/functional-programming/2016/11/22/polyclojure2.html",
            "date": " • Nov 22, 2016"
        }
        
    
  
    
        ,"post17": {
            "title": "How to do Polymorphism in Clojure (1/3)",
            "content": "This post is composed of 3 parts: Part 1, Part 2, Part 3 . Introduction . Through the years of use of conventional programming languages such as Python and C++, my thinking about programming was trapped within the Object Oriented model of these languages. When it came to solving a new problem my thinking would be dictated by how those languages wanted me to think and I was blind to any better way. It wasn’t until I came across the 1980s cult-classic “Structure and Interpretation of Computer Programs” (SICP) and the programming language Clojure that I started to see past thought models I had learned. One of the parts that was a real ‘Ah-ha!’ moment for me was the final section of Chapter 2 on “Data-directed programming”. This chapter was the clearest explanation of the problem of Polymorphism that I’d ever read… . The concern of this section is in implementing a system for calculating with different types of numbers (e.g. complex, integers, rationals, etc). Variants of this example are commonly used beginners books to Java and C++ in order to demonstrated how Object Oriented programming (OOP) can model data and abstractions. However SICP is not a language manual and doesn’t concern itself with showing how to make some feature of a language do what you want. It instead focuses on the problem of trying to model numbers itself, and then creates the necessary structures to solve this problem. SICP uses this problem to motivate the actual purpose of OOP and it then creates a basic implementation of Objects in scheme. The book then goes beyond OOP and implements multiple dispatch from scratch entirely in Scheme. . I found this to be such an enlightening exercise that I started thinking about how well other languages could solve this problem. In this blog post I want to implement the Numbers program in Clojure, a language that provides multiple dispatch. I think this is a non-trivial example that demonstrates every feature for polymorphism in Clojure’s core libraries. . Complex Numbers . A complex number, $z$ is a number expressed in the form $z=u+iv$, where $u$ and $v$ are real numbers and $i^2=-1$. $u$ is called the real part and $v$ is called the imaginary part. . We can represent a complex number as a pair $(u, v)$, called the rectangular form. . | An alternative representation is the polar form where the complex number is represented by the pair $(r, phi)$, where $r$ is the magntitude and $ phi$ is the angle. . | . Rectangular and polar forms are related via the following formulae: . [ begin{eqnarray} u &amp;=&amp; r cos phi v &amp;=&amp; r sin phi r &amp;=&amp; sqrt{u^2 + v^2} phi &amp;=&amp; tan^{-1}(v/u) end{eqnarray}] . In our Complex number package we want to support the following arithmetic operations on pairs of complex numbers: add, sub, mult, and div. . When adding or subtracting complex numbers it is natural to work with their rectangular coordinates: . [ begin{eqnarray} Re(z_1 + z_2) &amp;=&amp; Re(z_1) + Re(z_2) Im(z_1 + z_2) &amp;=&amp; Im(z_1) + Im(z_2) end{eqnarray}] . While when multiplying and dividing complex numbers it is more natural to work with the polar coordinates: . [ begin{eqnarray} Magnitude(z_1 cdot z_2) &amp;=&amp; Magnitude(z_1) cdot Magnitude(z_2) Angle(z_1 cdot z_2) &amp;=&amp; Angle(z_1) + Angle(z_2) end{eqnarray}] . The product is the vector obtained by stretching the length of $z_1$ by the length of $z_2$, and rotating the angle of $z_1$ by the angle of $z_2$. . So there are two different representations which are suitable for different operations. However we want to be able to do all the arithmetic operations on complex numbers regardless of which representation is used. . Rectangular Representation . How can we model this number pair using the tools in Clojure? Clojure allows us to create an object called a Record. A Record is basically a map with a name, a minimum set of keys that it is guaranteed to have, and a constructor. Here’s how we could create a rectangular complex number with records: . (defrecord Complex-r [real imag]) . You can create some Complex-r’s: . =&gt; (-&gt;Complex-r 2 3) #user.Complex-r{:real 2, :imag 3} =&gt; (-&gt;Complex-r -1 16) #user.Complex-r{:real -1, :imag 16} . With records it is good practice to create your own constructor to give you freedom to add post- and pre-conditions when a new record is created. This is just a wrapper function: . (defn complex-r &quot;create a new Complex-r&quot; [re im] {:pre [(number? re) (number? im)]} (-&gt;Complex-r re im)) . I provided some preconditions that assert that the parameters are a type of Clojure’s native number. We can access the real and imaginary parts of a Complex-r: . =&gt; (:real (complex-r 4 5)) 4 =&gt; (:imag (complex-r 4 5)) 5 . However, getting the real and imaginary parts of a complex number using the keywords seems to expose the implementation of Complex-r too much. Better practice would be to wrap those in some functions: . (defn real-r &quot;Get the real part of a complex-r number.&quot; [z] (:real z)) (defn imag-r &quot;Get the imaginary part of a complex-r number.&quot; [z] (:imag z)) . So now we have: . =&gt; (real-r (complex-r 4 5)) 4 =&gt; (imag-r (complex-r 4 5)) 5 . We can also view the magnitude and angle of a rectangular complex number using the formulae above: . (defn magnitude-r &quot;Magnitude of a complex-r number&quot; [z] (Math/sqrt (+ (square (real-r z)) (square (imag-r z))))) (defn angle-r &quot;Angle of a complex-r number&quot; [z] (Math/atan (/ (imag-r z) (real-r z)))) . In the REPL: . =&gt; (magnitude-r (complex-r 3 4)) 5.0 =&gt; (angle-r (complex-r 3 4)) 0.9272952180016121 . Polar Representation . Similarly, we can implement the Polar form of complex numbers as another record: . (defrecord Complex-p [magnitude angle]) (defn complex-p &quot;create a new Complex-p&quot; [magnitude angle] {:pre [(number? magnitude) (number? angle)]} (-&gt;Complex-p magnitude angle)) . Again for the Polar representation we need to write some functions that will give us real and imaginary parts, and the magnitude and angle of a polar number. . (defn real-p &quot;real part of a complex-p number&quot; [z] (* (:magnitude z) (Math/sin (:angle z)))) (defn imag-p &quot;imaginary part of a complex-p number&quot; [z] (* (:magnitude z) (Math/cos (:angle z)))) (defn magnitude-p &quot;magnitude of a complex-p number&quot; [z] (:magnitude z)) (defn angle-p &quot;angle of a complex-p number&quot; [z] (:angle z)) . In the REPL: . =&gt; (angle-p (complex-p 5 0.5)) 0.5 =&gt; (magnitude-p (complex-p 5 0.5)) 5 =&gt; (real-p (complex-p 5 0.5)) 4.387912809451864 =&gt; (imag-p (complex-p 5 0.5)) 2.397127693021015 . Single Dispatch with Protocols . At this point we have two different types of complex number representations and two sets of functions that are specialised to handle each type. This is obviously bad because a user of this numbers module has to pay attention at all times to whether they are using Complex-r or Complex-p types. They need to specialise whatever code they write with them. . Rather than having real-r and real-p functions we instead want to have a single function real that takes any type of complex number and performs dispatch at runtime based on the type of the argument it has received. I.e. dispatch based on the type of the first argument passed to the function. Dispatch based on a single argument is called single dispatch. . Clojure allows single dispatch through Protocols. A protocol is a named set of functions and their signatures, with no implementions. The functions dispatch on the type of their first argument, and thus must have at least one argument. Implementations of the protocol functions have to be written for each type implementing the protocol. They are very similar to Java interfaces, but with one important improvement: which protocols are implemented for a type is not a design time decesion by the code author, rather protocols can extend any type whenever and wherever you want. . We create a protocol for complex numbers using defprotocol: . (defprotocol PComplex (real [z] &quot;Real part of a complex number&quot;) (imag [z] &quot;Imaginary part of a complex number&quot;) (magnitude [z] &quot;Magnitude of a complex number&quot;) (angle [z] &quot;Angle of a complex number&quot;)) . Implement the PComplex protocol for each of our types: . (extend-protocol PComplex ;; implementation of methods for Complex-r type Complex-r (real [z] (:real z)) (imag [z] (:imag z)) (magnitude [z] (Math/sqrt (+ (square (real z)) (square (imag z)))) (angle [z] (Math/atan (/ (imag z) (real z)))) ;; implemention of methods for Complex-p type Complex-p (real [z] (* (:magnitude z) (Math/sin (:angle z)))) (imag [z] (* (:magnitude z) (Math/cos (:angle z)))) (magnitude [z] (:magnitude z)) (angle [z] (:angle z))) . Trying this out in a REPL: . =&gt; (def z1 (complex-r 5 6)) =&gt; (def z2 (complex-p 3 1)) =&gt; (real z1) 5 =&gt; (real z2) 1.6209069176044193 =&gt; (magnitude z1) 7.810249675906654 =&gt; (magnitude z2) 5 . With protocol we now have a generic set of functions for dealing with any type of complex number. If we created a new type of complex number then we’d simple make it implement the PComplex protocol. . Arithmetic With Complex Numbers . The PComplex protocol allows us to write code that works with complex numbers and does not need to worry whether whether they are rectangular or polar. We can now write single implementations the arithmetic functions add, sub, mult, and div using the formulas above. . (defn add &quot;Add two complex numbers together&quot; [z1 z2] (complex-r (+ (real z1) (real z2)) (+ (imag z1) (imag z2))) (defn sub &quot;Subtract two complex numbers from each other&quot; [z1 z2] (complex-r (- (real z1) (real z2)) (- (imag z1) (imag z2)))) (defn mult &quot;Multiply two complex numbers together&quot; [z1 z2] (complex-p (* (magnitude z1) (magnitude z2)) (+ (angle z1) (angle z2)))) (defn div &quot;Divide two complex numbers by each other&quot; [z1 z2] (complex-p (/ (magnitude z1) (magnitude z2)) (- (angle z1) (angle z2)))) . Since the functions from PComplex work transparently for both representations of complex numbers we only need to write one function for add and it works not only for both types, but also all combinations of them for free! . Let’s try it all out in the REPL: . =&gt; (add (complex-r 5 5) (complex-r 6 6)) #numbers.core.Complex-r{:real 11, :imag 11} =&gt; (mul (complex-r 5 5) (complex-r 5 5)) #numbers.core.Complex-p{:magnitude 50.00000000000001, :angle 1.5707963267948966} =&gt; (add (complex-p 5 0.1) (complex-p 10 0)) #numbers.core.Complex-r{:real 14.975020826390129, :imag 0.4991670832341408} =&gt; (mul (complex-p 5 0.1) (complex-p 10 0)) #numbers.core.Complex-p{:magnitude 50, :angle 0.1} =&gt; (add (complex-r 5 5) (complex-p 10 0)) #numbers.core.Complex-r{:real 15.0, :imag 5.0} =&gt; (mul (complex-r 5 5) (complex-p 10 0)) #numbers.core.Complex-p{:magnitude 70.71067811865476, :angle 0.7853981633974483} . Without the polymorphism obtained from the protocol we would have to write 16 separate functions instead of just these 4. Moreover, if we wanted to create more complex number representations there would be a combinatorial explosion in the number of arithmetic functions we’d need to write. . This post is composed of 3 parts: Part 1, Part 2, Part 3 .",
            "url": "https://jimypbr.github.io/blog/clojure/functional-programming/2016/11/21/polyclojure1.html",
            "relUrl": "/clojure/functional-programming/2016/11/21/polyclojure1.html",
            "date": " • Nov 21, 2016"
        }
        
    
  
    
        ,"post18": {
            "title": "Experiments with Julia",
            "content": "SIMD Support . Since version 0.3 Julia has some vectorisation capabilities that can exploit SIMD instructions when executing loops. It seems to require some nudging though. There are macros that are a bit like the pragmas in OpenMP. . Example 1. SAXPY . function saxpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) for i = 1:n y[i] += a*x[i] end end . Sadly, in version 0.3.10 this obvious candidate does not auto-vectorise. You can inspect how this code has compiled nicely in Julia by using the macros @code_llvm to see the LLVM IR or @code_native to see the ASM. The ASM produced is: . Source line: 48 mov R8, QWORD PTR [RSI + 16] xor R11D, R11D xor ECX, ECX cmp RCX, R8 jae 65 cmp RCX, QWORD PTR [RDI + 16] jae 55 lea R10, QWORD PTR [4*R11] mov RAX, QWORD PTR [RSI + 8] sub RAX, R10 mov RDX, QWORD PTR [RDI + 8] sub RDX, R10 movss XMM1, DWORD PTR [RDX] mulss XMM1, XMM0 addss XMM1, DWORD PTR [RAX] movss DWORD PTR [RAX], XMM1 dec R11 inc RCX cmp R9, RCX jne -72 pop RBP ret movabs RAX, 140636405232096 mov RDI, QWORD PTR [RAX] movabs RAX, 140636390845696 mov ESI, 48 call RAX . Note the scalar instructions movss, mulss, and addss. . Example 2: SAXPY + SIMD . To make it generate vectorised instructions you have to use the explicit vectorisation macros @simd and @inbounds macros. @simd gives the compiler license to vectorise without checking the legality of the transformation. @inbounds is an optimisation that turns off subscript checking, because subscript checking might throw an exception and so isn’t vectorisable. . function axpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) @simd for i = 1:n @inbounds y[i] += a*x[i] end end . This now compiles with SIMD instructions: . ... Source line: 48 mov R8, QWORD PTR [RDI + 8] mov R9, QWORD PTR [RSI + 8] xor EDI, EDI mov RSI, RAX and RSI, -8 je 79 pshufd XMM1, XMM0, 0 # xmm1 = xmm0[0,0,0,0] xor EDI, EDI lea RCX, QWORD PTR [4*RDI] mov RDX, R8 sub RDX, RCX movups XMM3, XMMWORD PTR [RDX] movups XMM2, XMMWORD PTR [RDX + 16] mulps XMM3, XMM1 mov RDX, R9 sub RDX, RCX movups XMM5, XMMWORD PTR [RDX] movups XMM4, XMMWORD PTR [RDX + 16] addps XMM5, XMM3 movups XMMWORD PTR [RDX], XMM5 mulps XMM2, XMM1 addps XMM2, XMM4 movups XMMWORD PTR [RDX + 16], XMM2 add RDI, -8 mov RCX, RSI add RCX, RDI jne -69 mov RDI, RSI sub RAX, RDI je 41 ... . Note the instructions movups, addps, mulps… (English: move, unaligned, packed, single-precision). Packed =&gt; Vector. . We can also see from the LLVM IR: . define void @julia_axpy_21664(float, %jl_value_t*, %jl_value_t*) { ... br label %vector.body vector.body: ; preds = %vector.body, %vector.ph %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ] %25 = getelementptr float* %20, i64 %index, !dbg !4955 %26 = bitcast float* %25 to &lt;4 x float&gt;* %wide.load = load &lt;4 x float&gt;* %26, align 4 %.sum18 = or i64 %index, 4 %27 = getelementptr float* %20, i64 %.sum18 %28 = bitcast float* %27 to &lt;4 x float&gt;* %wide.load9 = load &lt;4 x float&gt;* %28, align 4 %29 = getelementptr float* %24, i64 %index, !dbg !4955 %30 = bitcast float* %29 to &lt;4 x float&gt;* %wide.load10 = load &lt;4 x float&gt;* %30, align 4 %31 = getelementptr float* %24, i64 %.sum18 %32 = bitcast float* %31 to &lt;4 x float&gt;* %wide.load11 = load &lt;4 x float&gt;* %32, align 4 %33 = fmul &lt;4 x float&gt; %wide.load10, %broadcast.splat13 %34 = fmul &lt;4 x float&gt; %wide.load11, %broadcast.splat13 %35 = fadd &lt;4 x float&gt; %wide.load, %33 %36 = fadd &lt;4 x float&gt; %wide.load9, %34 store &lt;4 x float&gt; %35, &lt;4 x float&gt;* %26, align 4 store &lt;4 x float&gt; %36, &lt;4 x float&gt;* %28, align 4 %index.next = add i64 %index, 8 %37 = icmp eq i64 %index.next, %n.vec br i1 %37, label %middle.block, label %vector.body middle.block: ; preds = %vector.body, %if %resume.val = phi i64 [ 0, %if ], [ %n.vec, %vector.body ] %cmp.n = icmp eq i64 %15, %resume.val br i1 %cmp.n, label %L7, label %L ... . Timing of a function is easy to do: . x = rand(Float32, 10000000) y = rand(Float32, 10000000) a = float32(0.1) @time axpy(a,x,y) . However, you probably want to run this more than once since the first time you call a function, Julia JITs it so the timing won’t be representative. Here are the timings of axpy with and without @simd with length(x) == 10000000: . @simd? Time (s) . yes | 0.006527373 | . no | 0.013172804 | . Setup: Hardware: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz (AVX) Julia version: 0.3.11 . Example 3: SAXPY + Implicit vectorisation . In my experiments, I found that it is sometimes possible to get implicit vectorisation by using just @inbounds to disable bounds checking: . function axpy(a::Float32, x::Array{Float32,1}, y::Array{Float32,1}) n = length(x) for i = 1:n @inbounds y[i] += a*x[i] end end . This generates the same ASM as Example 2. Other simple cases have required @simd as well, so explicit vectorisation seems to be the only option at the moment. . SIMD Limitations in Julia . No SIMD functions. Function calls have to be inlined. Julia can manage to inline short functions itself. | Code must be type-stable. This means that there isn’t implicit type conversion in the loop. This will prevent vectorisation and probably make it run slow serially too. | SIMD macro doesn’t have the bells and whistles of the OpenMP SIMD pragma. | Doesn’t appear to be any way to specify alignment of memory. | No outer loop vectorisation. | I can’t find any diagnostic information about how things were optimised. | Does handle any branching besides some ifelse function. | . More Info on Vectorisation . See Intel page Vectorization in Julia. . Case Study: Fitzhugh-Nagamo PDE . Experiment with a non-trivial example of a PDE solver using the reaction-diffusion system described by: . [ begin{eqnarray} frac{ partial u}{ partial t} &amp;=&amp; a nabla^2 u + u - u^3 - v - k tau frac{ partial v}{ partial t} &amp;=&amp; b nabla^2 v + u - v end{eqnarray}] . With Neumann boundary conditions, boundary of [1,1]^2, N=100^2, and T=[0.,10.]. This example is based on this online python example. I implement this in C++ and Julia and compare the performance and the vectorisation. $y=x^2$. . Here is the code in Julia: . Click to expand code… function fitzhugh_nagumo(size, T) # Define the constants const aa = 2.8e-4 const bb = 5.0e-3 const τ = 0.1 const κ = -0.005 const dx = 2.0 / size const dt = 0.9 * dx^2 / 2.0 const invdx2 = 1.0 / dx^2 const dt_τ = dt / τ # Random initial fields u_old = rand(Float64, (size,size)) v_old = rand(Float64, (size,size)) u_new = Array(Float64, (size,size)) v_new = Array(Float64, (size,size)) for t = 0.0 : dt : T for j = 2:size-1 for i = 2:size-1 Δu = invdx2 * (u_old[i+1,j] + u_old[i-1,j] + u_old[i,j+1] + u_old[i,j-1] - 4*u_old[i,j]) Δv = invdx2 * (v_old[i+1,j] + v_old[i-1,j] + v_old[i,j+1] + v_old[i,j-1] - 4*v_old[i,j]) u_new[i,j] = u_old[i,j] + dt * (aa*Δu + u_old[i,j] - u_old[i,j]*u_old[i,j]*u_old[i,j] - v_old[i,j] + κ) v_new[i,j] = v_old[i,j] + dt_τ * (bb*Δv + u_old[i,j] - v_old[i,j]) end end for i = 1:size u_new[i,1] = u_new[i,2] u_new[i,size] = u_new[i,size-1] v_new[i,1] = v_new[i,2] v_new[i,size] = v_new[i,size-1] end for j = 1:size u_new[1,j] = u_new[2,j] u_new[size,j] = u_new[size-1,j] v_new[1,j] = v_new[2,j] v_new[size,j] = v_new[size-1,j] end # swap new and old u_new, u_old = u_old, u_new v_new, v_old = v_old, v_new end return (u_old, v_old) end . Here is the code in C++. . Click to expand code… #include &lt;iostream&gt; #include &lt;random&gt; #include &lt;algorithm&gt; // swap #define N 100 std::random_device rd; std::mt19937 mt(rd()); double fitzhugh_nagumo(double T) { // Define the constants const double aa = 2.8e-4; const double bb = 5.0e-3; const double tau = 0.1; const double kappa = -0.005; const double dx = 2.0 / N; const double dt = 0.9 * dx*dx / 2.0; const double invdx2 = 1.0 / (dx*dx); const double dt_tau = dt / tau; // Random initial fields double u_old[N][N]; double v_old[N][N]; double u_new[N][N]; double v_new[N][N]; std::uniform_real_distribution&lt;double&gt; rng(0.,1.); for (int i = 0; i &lt; N; ++i) { for (int j = 0; j &lt; N; ++j) { u_old[i][j] = rng(mt); v_old[i][j] = rng(mt); } } // Solver int Nt = (int) (T / dt); std::cout &lt;&lt; &quot;Nt = &quot; &lt;&lt; Nt &lt;&lt; std::endl; for (int t = 0; t &lt; Nt; ++t) { // evolve inner coordinates for (int i = 1; i &lt; N-1; ++i) { for (int j = 1; j &lt; N-1; ++j) { double delta_u = invdx2 * (u_old[i+1][j] + u_old[i-1][j] + u_old[i][j+1] + u_old[i][j-1] - 4*u_old[i][j]); double delta_v = invdx2 * (v_old[i+1][j] + v_old[i-1][j] + v_old[i][j+1] + v_old[i][j-1] - 4*v_old[i][j]); u_new[i][j] = u_old[i][j] + dt * (aa*delta_u + u_old[i][j] - u_old[i][j]*u_old[i][j]*u_old[i][j] - v_old[i][j] + kappa); v_new[i]After i[j] = v_old[i][j] + dt_tau * (bb * delta_v + u_old[i][j] - v_old[i][j]); } } // neumann boundary conditions for (int i = 0; i &lt; N; ++i) { u_new[i][0] = u_new[i][1]; u_new[i][N-1] = u_new[i][N-2]; v_new[i][0] = v_new[i][1]; v_new[i][N-1] = v_new[i][N-2]; } for (int j = 0; j &lt; N; ++j) { u_new[0][j] = u_new[1][j]; u_new[N-1][j] = u_new[N-2][j]; v_new[0][j] = v_new[1][j]; v_new[N-1][j] = v_new[N-2][j]; } // Swap old and new std::swap(u_new, u_old); std::swap(v_new, v_old); } return u_old[0][0]; } . Results . Setup: . Hardware: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz (AVX) | Julia version: 0.3.10 | C++ Compiler: g++4.8 | . Language Notes Time (s) . Julia | None | 5.993 | . Julia | @inbounds | 3.105 | . Julia | @inbounds + @simd | 3.0603 | . C++ | -O0 -std=c++11 | 22.790 | . C++ | -Ofast -std=c++11 -fno-tree-vectorize -fno-tree-slp-vectorize | 3.2096 | . C++ | -Ofast -std=c++11 | 2.142 | . The best time in C++ is only 1.4x better than the best time in Julia. Inspecting the ASM of Julia + @inbounds + @simd shows that even with these macros Julia is still not generating vector instructions. :(. If I disable vectorisation in the C++ compiler, the times between Julia and C++ are much closer. This suggests that Julia could get even higher performance if it could generate vector instructions. I suppose that newer versions of Julia will improve this in the future. I find these results very impressive nonetheless. It will be worth trying with a newer LLVM version too. . Notes . The thing holding back performance in the Julia code was the use of u[i,j]^3 instead of u[i,j]*u[i,j]*u[i,j]. The former version compiles to a call of pow(double, double) from libm! The latter does what you expect. This is a known bug. Fixed when Julia is built against LLVM 3.6. Version I’m using is built with LLVM v3.4. .",
            "url": "https://jimypbr.github.io/blog/julia/c++/simd/2015/11/21/julia.html",
            "relUrl": "/julia/c++/simd/2015/11/21/julia.html",
            "date": " • Nov 21, 2015"
        }
        
    
  
    
        ,"post19": {
            "title": "Hello",
            "content": "Typical hello world first blog post… .",
            "url": "https://jimypbr.github.io/blog/2015/11/20/hello.html",
            "relUrl": "/2015/11/20/hello.html",
            "date": " • Nov 20, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jimypbr.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jimypbr.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}