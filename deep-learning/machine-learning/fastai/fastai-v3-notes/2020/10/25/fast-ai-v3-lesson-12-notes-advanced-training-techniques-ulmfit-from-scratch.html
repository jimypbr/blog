<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fast.ai v3 Lesson 12 Notes: Advanced training techniques; ULMFiT from scratch | go-seq</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Fast.ai v3 Lesson 12 Notes: Advanced training techniques; ULMFiT from scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My personal notes on Lesson 12 of part 2 of fast.ai v3 - Advanced training techniques; ULMFiT from scratch" />
<meta property="og:description" content="My personal notes on Lesson 12 of part 2 of fast.ai v3 - Advanced training techniques; ULMFiT from scratch" />
<link rel="canonical" href="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html" />
<meta property="og:url" content="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html" />
<meta property="og:site_name" content="go-seq" />
<meta property="og:image" content="https://jimypbr.github.io/blog/images/fastai/image-20190706182251357.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-25T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html","@type":"BlogPosting","headline":"Fast.ai v3 Lesson 12 Notes: Advanced training techniques; ULMFiT from scratch","dateModified":"2020-10-25T00:00:00-05:00","datePublished":"2020-10-25T00:00:00-05:00","image":"https://jimypbr.github.io/blog/images/fastai/image-20190706182251357.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html"},"description":"My personal notes on Lesson 12 of part 2 of fast.ai v3 - Advanced training techniques; ULMFiT from scratch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimypbr.github.io/blog/feed.xml" title="go-seq" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       processEscapes: true
     }
   });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">go-seq</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fast.ai v3 Lesson 12 Notes: Advanced training techniques; ULMFiT from scratch</h1><p class="page-description">My personal notes on Lesson 12 of part 2 of fast.ai v3 - Advanced training techniques; ULMFiT from scratch</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-25T00:00:00-05:00" itemprop="datePublished">
        Oct 25, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      74 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine-learning">machine-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai-v3-notes">fastai-v3-notes</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#overview">Overview</a></li>
<li class="toc-entry toc-h2"><a href="#even-better-image-training-mixuplabel-smoothing">Even Better Image Training: Mixup/Label Smoothing</a>
<ul>
<li class="toc-entry toc-h3"><a href="#mixup">MixUp</a></li>
<li class="toc-entry toc-h3"><a href="#label-smoothing">Label Smoothing</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#training-in-mixed-precision">Training in Mixed Precision</a>
<ul>
<li class="toc-entry toc-h3"><a href="#aside-some-floating-point-revision">Aside: Some Floating Point Revision</a></li>
<li class="toc-entry toc-h3"><a href="#problems-with-half-precision">Problems with Half-Precision</a></li>
<li class="toc-entry toc-h3"><a href="#master-copy-of-weights">Master Copy of Weights</a></li>
<li class="toc-entry toc-h3"><a href="#loss-scaling">Loss Scaling</a></li>
<li class="toc-entry toc-h3"><a href="#accumulate-to-fp32">Accumulate to FP32</a></li>
<li class="toc-entry toc-h3"><a href="#dynamic-loss-scaling">Dynamic Loss Scaling</a></li>
<li class="toc-entry toc-h3"><a href="#summary">Summary</a></li>
<li class="toc-entry toc-h3"><a href="#implementing-mixed-precision-with-apex">Implementing Mixed Precision with APEX</a></li>
<li class="toc-entry toc-h3"><a href="#callback-implementation">Callback Implementation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#xresnet">XResNet</a>
<ul>
<li class="toc-entry toc-h3"><a href="#resnet-stem-trick">ResNet Stem Trick</a></li>
<li class="toc-entry toc-h3"><a href="#zero-batchnorm-trick">Zero BatchNorm Trick</a></li>
<li class="toc-entry toc-h3"><a href="#resblock">ResBlock</a>
<ul>
<li class="toc-entry toc-h4"><a href="#expansionbottleneck-resblock">Expansion/BottleNeck ResBlock</a></li>
<li class="toc-entry toc-h4"><a href="#downsampling-resblock">Downsampling ResBlock</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#putting-it-together">Putting it Together</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#image-classification-transfer-learning--fine-tuning">Image Classification: Transfer Learning / Fine Tuning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#custom-head">Custom Head</a></li>
<li class="toc-entry toc-h3"><a href="#freezing-layers">Freezing Layers</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ulmfit-from-scratch">ULMFiT From Scratch</a>
<ul>
<li class="toc-entry toc-h3"><a href="#preprocess-text">Preprocess Text</a>
<ul>
<li class="toc-entry toc-h4"><a href="#tokenizing">Tokenizing</a></li>
<li class="toc-entry toc-h4"><a href="#numericalize-tokens">Numericalize Tokens</a></li>
<li class="toc-entry toc-h4"><a href="#batching-text-for-rnn-training">Batching Text for RNN Training</a></li>
<li class="toc-entry toc-h4"><a href="#batching-text-for-training-classifiers">Batching Text for Training Classifiers</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#lstm-from-scratch">LSTM From Scratch</a>
<ul>
<li class="toc-entry toc-h4"><a href="#pytorchs-lstm-layer">PyTorch’s LSTM Layer</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#awd-lstm">AWD-LSTM</a>
<ul>
<li class="toc-entry toc-h4"><a href="#rnndropout--variational-dropout">RNNDropout / Variational Dropout</a></li>
<li class="toc-entry toc-h4"><a href="#weight-dropout--dropconnect">Weight Dropout / DropConnect</a></li>
<li class="toc-entry toc-h4"><a href="#embedding-dropout">Embedding Dropout</a></li>
<li class="toc-entry toc-h4"><a href="#the-main-model">The Main Model</a></li>
<li class="toc-entry toc-h4"><a href="#gradient-clippingrescaling">Gradient Clipping/Rescaling</a></li>
<li class="toc-entry toc-h4"><a href="#training--more-regularization">Training + More Regularization</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#ulmfit">ULMFiT</a>
<ul>
<li class="toc-entry toc-h4"><a href="#pretraining-wikitext">Pretraining (Wikitext)</a></li>
<li class="toc-entry toc-h4"><a href="#finetuning-imdb">Finetuning (IMDb)</a></li>
<li class="toc-entry toc-h4"><a href="#classification-imdb">Classification (IMDb)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h2"><a href="#q--a">Q &amp; A</a></li>
<li class="toc-entry toc-h2"><a href="#links-and-references">Links and References</a></li>
</ul><h2 id="overview">
<a class="anchor" href="#overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h2>

<p>This lesson implements some really important training techniques today, all using callbacks:</p>

<ul>
  <li>MixUp: a data augmentation technique that dramatically improves  results, particularly when you have less data, or can train for a longer time.</li>
  <li>Label smoothing: which works particularly well with MixUp, and  significantly improves results when you have noisy labels</li>
  <li>Mixed precision training: which trains models around 3x faster in many situations.</li>
  <li>It also implement <em>XResNet</em>: which is a tweaked version of the classic  resnet architecture that provides substantial improvements. And, even  more important, the development of it provides great insights into what  makes an architecture work well.</li>
  <li>Finally, the lesson show how to implement ULMFiT from scratch, including  building an LSTM RNN, and looking at the various steps necessary to  process natural language data to allow it to be passed to a neural  network.</li>
</ul>

<p><strong><em><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8">Link to Lesson 12 Lecture</a></em></strong></p>

<h2 id="even-better-image-training-mixuplabel-smoothing">
<a class="anchor" href="#even-better-image-training-mixuplabel-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Even Better Image Training: Mixup/Label Smoothing</h2>

<h3 id="mixup">
<a class="anchor" href="#mixup" aria-hidden="true"><span class="octicon octicon-link"></span></a>MixUp</h3>

<p><em>(Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/10b_mixup_label_smoothing.ipynb">10b_mixup_label_smoothing.ipynb</a>)</em></p>

<p>It’s quite possible that we don’t need much data augmentation for images anymore. FastAI’s experiments with a data augmentation called <strong>Mixup</strong>, they  found that they could remove most other data augmentation and get amazingly good results. It’s really simple to do and you can also train with MixUp for a really <em>long time</em> and get really good results.</p>

<ul>
  <li>MixUp comes from the paper: <a href="https://arxiv.org/abs/1710.09412">mixup: Beyond Empirical Risk Minimization [2017]</a>.  This is quite an easy reading paper.</li>
  <li>MixUp was shown to be a very  effective training technique in the paper: <a href="https://arxiv.org/abs/1812.01187?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529">Bag of Tricks for Image Classification with Convolutional Neural Networks [2019]</a>. <em>(This lesson will refer back to this paper a lot so it’s also worth reading fully.)</em>
</li>
</ul>

<p>Here is the table of results for the different training tricks tried in that paper:</p>

<p><img src="/blog/images/fastai/image-20201015000615280.png" alt="image-20201015000615280" style="zoom:50%;"></p>

<p><em>(NB with MixUp they ran for more epochs)</em></p>

<p><strong>What is MixUp?</strong> We are going to take two different images and we are going to <em>combine them</em>. How? By simply making a convex combination of the two. So you do 30% of one image and 70% of the other image:</p>

<p><img src="/blog/images/fastai/image-20201015002514416.png" alt="image-20201015002514416" style="zoom:50%;"></p>

<p><strong>You also have to do MixUp to the labels.</strong> So rather than being a one-hot encoded target, your target would become something like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span> <span class="p">(</span><span class="n">gas</span> <span class="n">pump</span><span class="p">),</span> <span class="mf">0.7</span> <span class="p">(</span><span class="n">dog</span><span class="p">)]</span>
</code></pre></div></div>

<p>When we are generating these mixed up training examples on the fly we we need to pick how much of each image we will use.  Let’s define a <em>mixing proportion</em>, $\lambda$, so our mixed image will be $\lambda x_i + (1-\lambda) x_j$ . We will pick the $\lambda$ randomly each time, however we <em>don’t</em> want to just naively generate this from a uniform distribution. In the MixUp paper they explore how the mixing parameter affects performance and they get this plot (higher is worse):</p>

<p><img src="/blog/images/fastai/image-20200414221039855.png" alt="image-20200414221039855"></p>

<p>For good values we need to sample from a distribution that is more likely to pick numbers near 0 or near 1. A distribution that looks like this is the <strong><em><a href="https://en.wikipedia.org/wiki/Beta_distribution">beta distribution</a></em></strong>.</p>

<p>It is one of the weirder distributions and it’s not very intuitive from its formula, but the shape of it looks like this for two different values of the parameters $\alpha$:</p>

<p><img src="/blog/images/fastai/Sun,%2012%20Apr%202020%20190257.png" alt="img"></p>

<p>The Beta distribution tends to generate number at the edges. If you compare that to the plot of prediction errors above you can see they are inverses of each other.</p>

<p><em>(Aside: <a href="https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution">this SO post</a> about the intuition behind the Beta distribution is quite interesting).</em></p>

<p>Let’s look at the implementation of MixUp…</p>

<p><strong>Original MixUp Algorithm:</strong> In the original article, the authors suggested three things:</p>

<ol>
  <li>Create two separate dataloaders and draw a batch from each at every iteration to mix them up</li>
  <li>Draw a $\lambda$ value following a beta distribution with a parameter $\alpha$ (0.4 is suggested in their article)</li>
  <li>Mix up the two batches with the <em>same value</em> $\lambda$.</li>
  <li>Use one-hot encoded targets</li>
</ol>

<p>While the approach above works very well, it’s not the fastest way we can do this. The main point that slows down this process is wanting two different batches at every iteration (which means loading twice the amount of images and applying to them the other data augmentation  function). To avoid this slow down, we can be a little smarter and mixup a batch with a <em>shuffled version of itself</em> (this way the mixed up images are still different). This was a trick suggested in the MixUp paper.</p>

<p><strong>FastAI MixUp Algorithm:</strong> FastAI employs a few tricks to improve it:</p>

<ol>
  <li>
    <p>Create a single dataloader and draw a single batch, $X$, with labels $y$ from which we can create mixed up images by shuffling this batch.</p>
  </li>
  <li>
    <p>For each item in the batch pick a generate a vector of $\lambda$ values (Beta distribution with $\alpha=0.4$). To avoid potential duplicate mixups fix $\lambda$ values with:</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">λ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">λ</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">λ</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Create a random permutation of the batch $X’$ and labels $y’$.</p>
  </li>
  <li>
    <p>Return the linear combination of the original batch and the random permutation: $\lambda X + (1-\lambda) X’$. Likewise with the labels: $\lambda y + (1-\lambda)y’$.</p>
  </li>
</ol>

<p>The <em>first trick</em> is picking a different $\lambda$ for every image in the batch because fastai found that doing so made the network converge faster.</p>

<p>The <em>second trick</em> is using a single batch and shuffling it for MixUp instead of loading two batches. However, this strategy can create duplicates. Let’s say the batch has two images, we shuffle the batch and first mix Image0 with Image1 with $\lambda_1=0.1$, and then mix Image1 and Image0 with $\lambda=0.9$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image0</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="n">shuffle0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.1</span><span class="p">)</span> <span class="o">=</span> <span class="n">image0</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="n">image1</span> <span class="o">*</span> <span class="mf">0.9</span>
<span class="n">image1</span> <span class="o">*</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="n">shuffle1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.9</span><span class="p">)</span> <span class="o">=</span> <span class="n">image1</span> <span class="o">*</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="n">image0</span> <span class="o">*</span> <span class="mf">0.1</span>
</code></pre></div></div>

<p>These will be the same. Of course, we have to be a bit unlucky but in practice, they saw there was a drop in accuracy by using this without removing those  near-duplicates. To avoid them, the tricks is to replace the vector of  parameters we drew by:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">λ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">λ</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">λ</span><span class="p">)</span>
</code></pre></div></div>

<p>The beta distribution with the two parameters equal is symmetric in  any case, and this way we insure that the biggest coefficient is always  near the first image (the non-shuffled batch).</p>

<p>Here is the <code class="language-plaintext highlighter-rouge">Callback</code> code for MixUp. The <code class="language-plaintext highlighter-rouge">begin_batch</code> method implements the above algorithm:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MixUp</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="n">_order</span> <span class="o">=</span> <span class="mi">90</span> <span class="c1">#Runs after normalization and cuda
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">α</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.4</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">distrib</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="n">α</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="n">α</span><span class="p">]))</span>
    
    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_loss_func</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">loss_func</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">loss_func</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">loss_func</span>
    
    <span class="k">def</span> <span class="nf">begin_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">in_train</span><span class="p">:</span> <span class="k">return</span> <span class="c1">#Only mixup things during training
</span>        <span class="n">λ</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">distrib</span><span class="p">.</span><span class="n">sample</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">yb</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),)).</span><span class="n">squeeze</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">xb</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">λ</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">λ</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">λ</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">λ</span> <span class="o">=</span> <span class="n">unsqueeze</span><span class="p">(</span><span class="n">λ</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
        <span class="n">shuffle</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">yb</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">xb</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">xb1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">yb1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">xb</span><span class="p">[</span><span class="n">shuffle</span><span class="p">],</span><span class="bp">self</span><span class="p">.</span><span class="n">yb</span><span class="p">[</span><span class="n">shuffle</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">xb</span> <span class="o">=</span> <span class="n">lin_comb</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">xb</span><span class="p">,</span> <span class="n">xb1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">λ</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">after_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">loss_func</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_loss_func</span>
    
    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">in_train</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">NoneReduce</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">old_loss_func</span><span class="p">)</span> <span class="k">as</span> <span class="n">loss_func</span><span class="p">:</span>
            <span class="n">loss1</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss2</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">yb1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">lin_comb</span><span class="p">(</span><span class="n">loss1</span><span class="p">,</span> <span class="n">loss2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">λ</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">old_loss_func</span><span class="p">,</span> <span class="s">'reduction'</span><span class="p">,</span> <span class="s">'mean'</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>How do we modify the loss function?</strong> <em>See the <code class="language-plaintext highlighter-rouge">loss_func</code> method above.</em> Like when we coded up the cross-entropy loss, we don’t need to expand the target out into a full categorical distribution, we can instead just write a specialised version of cross-entropy for MixUp:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">new_target</span><span class="p">)</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target2</span><span class="p">)</span>
</code></pre></div></div>

<p>recalling that the cross-entropy formula is: 
\(L =-\sum_i x_i \log p(z_i)\)</p>

<ul>
  <li>PyTorch loss functions like <code class="language-plaintext highlighter-rouge">nn.CrossEntropy</code> have a <code class="language-plaintext highlighter-rouge">reduction</code> attribute to specify how to calculate the loss of a whole batch from the individual losses, e.g. take the mean.</li>
  <li>We want to do this reduction on the batch <em>after</em> the linear combination of the individual losses has been calculated.</li>
  <li>So reduction needs to be turned off for the linear combination, then turned on afterwards.</li>
</ul>

<p><strong>Question:</strong> <em>Is there an intuitive way to understand why MixUp is better than other data augmentation techniques?</em></p>

<blockquote>
  <p>One of the things that’s really nice about MixUp is that it doesn’t require any domain specific thinking about the data augmentation. E.g. can I do vertical/horizontal flipping, how much can we rotate? lossiness: black padding, reflection padding etc. It’s also almost infinite in terms of the number of images it can create.</p>

  <p>There are other similar things:</p>

  <ul>
    <li>
<a href="https://arxiv.org/abs/1708.04552">CutOut</a> - delete a square and replace it with black or random pixels</li>
    <li>
<a href="https://arxiv.org/abs/1905.04899">CutMix</a> - patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches</li>
    <li>Find four different images and put them in four corners.</li>
  </ul>

  <p>These things actually get really good results and are not used so much.</p>
</blockquote>

<h3 id="label-smoothing">
<a class="anchor" href="#label-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Label Smoothing</h3>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?&amp;t=1121">(Jump_to lesson 12 video)</a></em></p>

<p>Another regularization technique that’s often used for classification is <em>label smoothing</em>, which deliberately introduces noise for the labels. It’s designed to make the model a little bit less certain of its decision by changing the target labels: instead of the hard prediction of exactly 1 for the correct class and 0 for all the others, we change the objective to prediction $1-\epsilon$ for the correct class and $\frac{\epsilon}{k-1}$ for all the others, where $\epsilon$ is a small positive number and $k$ is the number of classes.</p>

<p>We can achieve this by updating the <strong>loss</strong> to:</p>

\[loss = (1-\epsilon) \;\mbox{ce}(i) + \epsilon \sum_j \mbox{ce}(j) / (k-1)\]

<p>where $\mbox{ce}(x)$ is the cross-entropy of $x$ (i.e. $-\log(p_x)$), and $i$ is the correct class. Typical value: $\epsilon=0.1$.</p>

<p>This is a really simple, but astonishingly effective way to handle <strong>noisy labels</strong> in your data. For example, in a medical problem where the diagnostic labels are not perfect. It turns out that if you use label smoothing, noisy labels generally aren’t that big an issue. Anecdotally, people have deliberately permuted their labels so they are 50% wrong, and they still get good results with label smoothing. This also could enable you to get training faster to check something works, before investing a lot of time in cleaning up your data.</p>

<p>Noisy labels not as big an issue as you’d think.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LabelSmoothingCrossEntropy</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ε</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ε</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">ε</span><span class="p">,</span><span class="n">reduction</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">log_preds</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">reduce_loss</span><span class="p">(</span><span class="o">-</span><span class="n">log_preds</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>
        <span class="n">nll</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">log_preds</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lin_comb</span><span class="p">(</span><span class="n">loss</span><span class="o">/</span><span class="p">(</span><span class="n">c</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">nll</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ε</span><span class="p">)</span>
</code></pre></div></div>

<p>We can just drop this in as a loss function, replacing the usual cross-entropy.</p>

<p>Additional reading:</p>

<ul>
  <li><a href="https://paperswithcode.com/method/label-smoothing">Label Smoothing [paperswithcode/methods]</a></li>
  <li><a href="https://arxiv.org/abs/1906.02629">When Does Label Smoothing Help? [2019]</a></li>
</ul>

<h2 id="training-in-mixed-precision">
<a class="anchor" href="#training-in-mixed-precision" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training in Mixed Precision</h2>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?&amp;t=1318">(Jump to lesson 12 video)</a></em>; <em>Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/10c_fp16.ipynb">10c_fp16.ipynb</a></em></p>

<p>If you are using a modern accelerator you can train with half precision floating point numbers. These are only 16bit floating point numbers (FP16), instead of the usual single precision 32bit floats (FP32). In theory this should speed things up by 10x, <em>in practice</em> however you get 2-3x speed-ups in deep learning.</p>

<p>Using FP16 cuts memory usage in half so you can double the size of your model and double your batch size. Specialized hardware units in modern accelerators, such as tensor cores, can also execute operations on FP16 faster. On Volta generation NVIDIA cards these tensor cores theoretically give an 8x speed-up (sadly, just in theory).</p>

<p>So training at half precision is better for your memory usage, way faster if you have a Volta GPU (still a tiny bit faster if you don’t since the computations are easiest). How do we use it? In PyTorch you simply have to put <code class="language-plaintext highlighter-rouge">.half()</code> on all your tensors. Problem is that you usually don’t see the same accuracy in the end, because half-precision is not very precise, funnily enough.</p>

<h3 id="aside-some-floating-point-revision">
<a class="anchor" href="#aside-some-floating-point-revision" aria-hidden="true"><span class="octicon octicon-link"></span></a>Aside: Some Floating Point Revision</h3>

<p>Floating point numbers may seem arcane or like a bit of  a dark art, but they are really quite elegant and understandable. One has to first understand that they are basically like scientific notation, except in base 2 instead of base 10:</p>

\[\begin{align}
x &amp;= 0.1101101\times2^4 \\
x &amp;= (-1)^s \times M \times 2^E
\end{align}\]

<p>In the IEEE floating point standard floats are represented using the above formula, where:</p>

<ul>
  <li>The <em>sign</em> $s$ determines if the number of negative ($s=0$) or positive ($s=1$)</li>
  <li>The <em>significant</em> (AKA <em>mantissa</em>) $M$ is a fractional binary number that ranges between $[1, 2-\epsilon]$ (normalized case) or between $[0, 1-\epsilon]$ (denormalized case).</li>
  <li>The <em>exponent</em> $E$ weights the value by a (possibly negative) power of 2.</li>
</ul>

<p>Each of these 3 sections occupy some number of bits. Here are the layouts for 32 bit floats:</p>

<p><img src="/blog/images/fastai/image-20200719155919744.png" alt="image-20200719155919744" style="zoom:50%;"><br><em>(<a href="https://en.wikipedia.org/wiki/File:Float_example.svg">Source</a>)</em></p>

<p>Here are some links to further introductory material:</p>

<ul>
  <li><em>A great explanation of how floats work: <a href="https://www.youtube.com/watch?v=PZRI1IfStY0">YouTube</a>.</em></li>
  <li><em>This video works through adding two floats at the bit level: <a href="https://www.youtube.com/watch?v=Pox8LzIHhR4">YouTube</a></em></li>
</ul>

<h3 id="problems-with-half-precision">
<a class="anchor" href="#problems-with-half-precision" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problems with Half-Precision</h3>

<p>At high precision (FP32, FP64) you have enough breathing room that most of the time you don’t need to worry about the cases where the approximation falls apart and everything goes to hell. At FP16 you have to <em>constantly</em> think about the edge cases.</p>

<p>Let’s look at what FP16 looks like on a bit level:</p>

<p><img src="/blog/images/fastai/image-20200719161555395.png" alt="image-20200719161555395" style="zoom:50%;"></p>

<p><em>(<a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">Source</a>)</em></p>

<ul>
  <li>The exponent has 5 bits, giving it a range [-14, 15].</li>
  <li>Fraction has 10 bits.</li>
  <li>FP16 Range: <code class="language-plaintext highlighter-rouge">2^-14</code> to <code class="language-plaintext highlighter-rouge">2^15</code> roughly</li>
  <li>FP32 Range: <code class="language-plaintext highlighter-rouge">2^-126</code> to <code class="language-plaintext highlighter-rouge">2^127</code>
</li>
  <li>The ‘spaces’ between numbers is <em>increased</em> in FP16. There is a finite number of floats between 1 and 2, and <code class="language-plaintext highlighter-rouge">1 + 0.0001 = 1</code> in FP16. This <em>will cause problems</em> during training.</li>
  <li><em>When <code class="language-plaintext highlighter-rouge">update/param &lt; 2^-11</code>, updates will have no effect.</em></li>
</ul>

<p>You can’t just use half-precision everywhere, because you will almost always get hit by one of the problems above. Instead we do what’s call <strong>mixed precision training</strong>.  This is where you drop down to FP16 in some parts of the training and revert to FP32 to preserve precision in others.</p>

<p>We do the <em>forward pass</em> and the <em>backwards pass</em> in FP16, and pretty much everywhere else we use FP32. For example, when we apply the gradients in the weight update we use full precision. Accumulate in FP32 and store in FP16.</p>

<p>There are still some <strong>problems</strong> remaining if we do this:</p>

<ol>
  <li>Weight update is imprecise. <code class="language-plaintext highlighter-rouge">1+0.0001 = 1</code> =&gt; <em>vanishing gradients</em>.</li>
  <li>Gradients can <em>underflow</em>. Numbers get too low, get replaced by 0 =&gt; <em>vanishing gradients</em>.</li>
  <li>Activations, loss, or reductions can overflow =&gt; <em>Makes NaNs, training diverges</em>.</li>
</ol>

<p>The following subsections show how these are addressed.</p>

<h3 id="master-copy-of-weights">
<a class="anchor" href="#master-copy-of-weights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Master Copy of Weights</h3>

<p>To solve the first problem listed above - <em>weight update is imprecise</em> - we can store a <strong>‘master’ copy</strong> of the weights in FP32. It is this that gets passed to the optimizer:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">master_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div></div>

<p>After the optimizer step you then copy the master weights back into the model weights in FP16: <code class="language-plaintext highlighter-rouge">master.grad.data.copy_(model.grad.data)</code></p>

<p>Then, our training loop will look like:</p>

<ol>
  <li>Compute the output with the FP16 model, then the loss</li>
  <li>Back-propagate the gradients in half-precision.</li>
  <li>Copy the gradients in FP32 precision</li>
  <li>Do the update on the master model (in FP32 precision)</li>
  <li>Copy the master model in the FP16 model.</li>
</ol>

<h3 id="loss-scaling">
<a class="anchor" href="#loss-scaling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss Scaling</h3>

<p>Next we need to tackle the second problem - <em>gradients can underflow</em> when doing backprop in FP16. To avoid the gradients getting zeroed by the FP16 precision, we <strong>multiply the loss by a scale factor</strong>. Typically this factor is something like 512 or 128.</p>

<p>We want to do this because the activation gradient values are typically very small and so fall outside of FP16’s representable range. Here is a histogram of the magnitude of activation gradients:</p>

<p><img src="/blog/images/fastai/image-20200719182040217.png" alt="image-20200719182040217" style="zoom:50%;"></p>

<p>What we want to do is push that distribution to the right and into the representable range of FP16. We could do this by multiplying the loss by 512 or 1024.</p>

<p>We don’t want these 512-scaled gradients to be in the weight update, so after converting them to FP32, we need to ‘descale’ by dividing by the scale factor.</p>

<p>The training loop changes to:</p>

<ol>
  <li>Compute the output with the FP16 model, then the loss.</li>
  <li>Multiply the loss by scale then back-propagate the gradients in half-precision.</li>
  <li>Copy the gradients in FP32 precision then divide them by scale.</li>
  <li>Do the update on the master model (in FP32 precision).</li>
  <li>Copy the master model in the FP16 model.</li>
</ol>

<h3 id="accumulate-to-fp32">
<a class="anchor" href="#accumulate-to-fp32" aria-hidden="true"><span class="octicon octicon-link"></span></a>Accumulate to FP32</h3>

<p>The last problem - <em>Activations, loss, or reductions can overflow</em> - needs to be dealt with in a couple of places.</p>

<p>First, the loss can overflow, so let’s do the reduction calculation that gives the loss in FP32:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># y_pred: fp16
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">.</span><span class="nb">float</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nb">float</span><span class="p">())</span> <span class="c1"># loss is now FP32
</span><span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">scale_factor</span> <span class="o">*</span> <span class="n">loss</span>
</code></pre></div></div>

<p>Another overflow risk is occurs with Batchnorm, which also should do its reduction in FP32. You can recursively go through your model and change all the Batchnorm layers back to FP32 with this function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bn_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm3d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">bn_to_float</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bn_types</span><span class="p">):</span> <span class="n">model</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">children</span><span class="p">():</span>  <span class="n">bn_to_float</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>You can then convert your Pytorch model to half precision with this function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_to_half</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">half</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">bn_to_float</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="dynamic-loss-scaling">
<a class="anchor" href="#dynamic-loss-scaling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dynamic Loss Scaling</h3>

<p>The problem with loss scaling is that it has a magic number, the <code class="language-plaintext highlighter-rouge">scale_factor</code>, that you have to tune. As the model trains, different values may be necessary. Dynamic loss scaling is a technique that adaptively sets the <code class="language-plaintext highlighter-rouge">scale_factor</code> to the right value at runtime. This value will be perfectly fitted to our model and can continue to be dynamically adjusted as the training goes, if it’s still too high, by just halving it each time we overflow. After a while though, training will converge and gradients will start to get smaller, so we also need a mechanism to get this dynamic loss scale larger if it’s safe to do so.</p>

<p><strong>Algorithm:</strong></p>

<ol>
  <li>First initialize <code class="language-plaintext highlighter-rouge">scale_factor</code> with a really high value, e.g. 512.</li>
  <li>Do a forward and backward pass.</li>
  <li>Check if any of the gradients overflowed.</li>
  <li>If any gradients overflowed, half the <code class="language-plaintext highlighter-rouge">scale_factor</code>, and zero the gradients (thus skipping the optimization step).</li>
  <li>If the loop goes 500 steps without an overflow, double the <code class="language-plaintext highlighter-rouge">scale_factor</code>.</li>
</ol>

<p>How do we test for overflow? A useful property of NaNs is that they propagate - add anything to a NaN and the result if NaN. So if we sum a tensor that contains a Nan the result will be NaN. To check if it is NaN we can use the counter-intuitive property that <code class="language-plaintext highlighter-rouge">NaN!=NaN</code> and simply check if the result of the sum equals itself. Here is the code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_overflow</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="nb">sum</span><span class="p">())</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">s</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span> <span class="ow">or</span> <span class="n">s</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span> <span class="ow">or</span> <span class="n">s</span> <span class="o">!=</span> <span class="n">s</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p>Here is a diagram of the mixed precision training loop containing all the above described steps:</p>

<p><img src="/blog/images/fastai/image-20200719163119314.png" alt="image-20200719163119314" style="zoom:50%;"></p>

<p><em>(Source: <a href="https://developer.nvidia.com/blog/video-mixed-precision-techniques-tensor-cores-deep-learning/">NVIDIA - Mixed-Precision Training Techniques Using Tensor Cores for Deep Learning</a>)</em></p>

<p>In conclusion, here are the 3 problems caused by converting the model with FP16 and how they are mitigated:</p>

<ol>
  <li>
<del>Weight update is imprecise</del> =&gt; “Master” weights in FP32</li>
  <li>
<del>Gradients can <em>underflow</em></del> =&gt; Loss (gradient) scaling</li>
  <li>
<del>Activations or loss can overflow</del> =&gt; Accumulate in FP32</li>
</ol>

<p>Sometimes training with half-precision gives better results. More randomness, some regularization. Often the results are similar to what you get with FP32, but just faster.</p>

<h3 id="implementing-mixed-precision-with-apex">
<a class="anchor" href="#implementing-mixed-precision-with-apex" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementing Mixed Precision with APEX</h3>

<p><a href="https://github.com/NVIDIA/apex">APEX</a> is a utility library authored by NVIDIA for doing mixed precision and distributed training in Pytorch.</p>

<p>APEX can convert a model to FP16, keeping the batchnorm’s at FP32, with the function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">apex.fp16_utils</span> <span class="k">as</span> <span class="n">fp16</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fp16</span><span class="p">.</span><span class="n">convert_network</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
</code></pre></div></div>

<p><a href="https://nvidia.github.io/apex/fp16_utils.html">(<code class="language-plaintext highlighter-rouge">apex.fp16_utils</code> docs)</a></p>

<p>From the model parameters (mostly in FP16), APEX can create a master copy in FP32 that we will use for the optimizer step:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_p</span><span class="p">,</span> <span class="n">master_p</span> <span class="o">=</span> <span class="n">fp16</span><span class="p">.</span><span class="n">prep_param_lists</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p>After the backward pass, all gradients must be copied from the model to the master params before the optimizer step can be done in FP32:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fp16</span><span class="p">.</span><span class="n">model_grads_to_master_grads</span><span class="p">(</span><span class="n">model_p</span><span class="p">,</span> <span class="n">master_p</span><span class="p">)</span>
</code></pre></div></div>

<p>After the optimizer step we need to copy back the master parameters to the model parameters for the next update:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fp16</span><span class="p">.</span><span class="n">master_params_to_model_params</span><span class="p">(</span><span class="n">model_params</span><span class="p">,</span> <span class="n">master_params</span><span class="p">)</span>
</code></pre></div></div>

<p>If you want to use parameter groups then we need to do a bit more work than this. Parameter groups allow you to do things like:</p>

<ul>
  <li>Transfer learning and freeze some layers</li>
  <li>Apply discriminative learning rates</li>
  <li>Don’t apply weight decay to some layers (like BatchNorm) or the bias terms</li>
</ul>

<p>Parameter groups are the business of the optimizer not the model, so we need to define a new <code class="language-plaintext highlighter-rouge">prep_param_lists</code> that takes the optimizer and returns the model and master params grouped in a nested list. Then you need too define wrappers for <code class="language-plaintext highlighter-rouge">model_grads_to_master_grads</code> and <code class="language-plaintext highlighter-rouge">master_params_to_model_params</code> that work on these nested lists. <em>It’s straight-forward, but I won’t reproduce it here. It is shown in the notebook: <a href="https://render.githubusercontent.com/view/ipynb?commit=68ee8a707bf612a9c3b98c33d3de5aa1ae73cd30&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f636f757273652d76332f363865653861373037626636313261396333623938633333643364653561613161653733636433302f6e62732f646c322f3130635f667031362e6970796e62&amp;nwo=fastai%2Fcourse-v3&amp;path=nbs%2Fdl2%2F10c_fp16.ipynb&amp;repository_id=152646389&amp;repository_type=Repository#But-we-need-to-handle-param-groups">10c_fp16.ipynb</a></em></p>

<h3 id="callback-implementation">
<a class="anchor" href="#callback-implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Callback Implementation</h3>

<p>Mixed precision training as a callback with dynamic loss scaling:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MixedPrecision</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="n">_order</span> <span class="o">=</span> <span class="mi">99</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_scale</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">flat_master</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_loss_scale</span><span class="o">=</span><span class="mf">2.</span><span class="o">**</span><span class="mi">24</span><span class="p">,</span> <span class="n">div_factor</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span>
                 <span class="n">scale_wait</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">enabled</span><span class="p">,</span> <span class="s">"Mixed precision training requires cudnn."</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flat_master</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">max_loss_scale</span> <span class="o">=</span> <span class="n">flat_master</span><span class="p">,</span><span class="n">dynamic</span><span class="p">,</span><span class="n">max_loss_scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">div_factor</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">scale_wait</span> <span class="o">=</span> <span class="n">div_factor</span><span class="p">,</span><span class="n">scale_wait</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss_scale</span> <span class="o">=</span> <span class="n">max_loss_scale</span> <span class="k">if</span> <span class="n">dynamic</span> <span class="k">else</span> <span class="n">loss_scale</span>

    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">fp16</span><span class="p">.</span><span class="n">convert_network</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model_pgs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">master_pgs</span> <span class="o">=</span> <span class="n">get_master</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">flat_master</span><span class="p">)</span>
        <span class="c1">#Changes the optimizer so that the optimization step is done in FP32.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">opt</span><span class="p">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">master_pgs</span> <span class="c1">#Put those param groups inside our runner.
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">begin_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">xb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">xb</span><span class="p">.</span><span class="n">half</span><span class="p">()</span> <span class="c1">#Put the inputs to half precision
</span>    <span class="k">def</span> <span class="nf">after_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">pred</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span> <span class="c1">#Compute the loss in FP32
</span>    <span class="k">def</span> <span class="nf">after_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">in_train</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">loss</span> <span class="o">*=</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss_scale</span> <span class="c1">#Loss scaling to avoid gradient underflow
</span>
    <span class="k">def</span> <span class="nf">after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#First, check for an overflow
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span> <span class="ow">and</span> <span class="n">grad_overflow</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model_pgs</span><span class="p">):</span>
            <span class="c1">#Divide the loss scale by div_factor, zero the grad (after_step will be skipped)
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">loss_scale</span> <span class="o">/=</span> <span class="bp">self</span><span class="p">.</span><span class="n">div_factor</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="k">return</span> <span class="bp">True</span> <span class="c1">#skip step and zero_grad
</span>        <span class="c1">#Copy the gradients to master and unscale
</span>        <span class="n">to_master_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model_pgs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">master_pgs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">flat_master</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">master_params</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">master_pgs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">master_params</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loss_scale</span><span class="p">)</span>
        <span class="c1">#Check if it's been long enough without overflow
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dynamic</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">count</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale_wait</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">loss_scale</span> <span class="o">*=</span> <span class="bp">self</span><span class="p">.</span><span class="n">div_factor</span>

    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#Zero the gradients of the model since the optimizer is disconnected.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1">#Update the params from master to model.
</span>        <span class="n">to_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model_pgs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">master_pgs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">flat_master</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="xresnet">
<a class="anchor" href="#xresnet" aria-hidden="true"><span class="octicon octicon-link"></span></a>XResNet</h2>

<p><a href="https://youtu.be/vnOpEwmtFJ8?t=1704"><em>(Jump to Lesson 12 video)</em></a>; <em>Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/11_train_imagenette.ipynb">11_train_imagenette.ipynb</a></em></p>

<p>So far all of the image models we’ve used have been boring convolution models. What we really want to be using is a <strong>ResNet model</strong>. We will implement <strong>XResNet</strong>, which is the the mutant/extended version of ResNet. This is a tweaked ResNet taken from the <a href="https://arxiv.org/abs/1812.01187">Bag of tricks paper</a>.</p>

<p>Let’s go through the XResNet modifications…</p>

<h3 id="resnet-stem-trick">
<a class="anchor" href="#resnet-stem-trick" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResNet Stem Trick</h3>

<p><strong>ResNetC</strong> - don’t do a big 7x7 convolution at the start, because it’s inefficient and is just a single linear model. Instead do three 3x3 convs in a row. The receptive field is still going to be about 7x7, but it has a much richer number of things it can learn because it has 3 layers instead of 1. We call these first layers the <em>stem</em>. 
<img src="/blog/images/fastai/image-20200917200518084.png" alt="image-20200917200518084" style="zoom:50%;"></p>

<ul>
  <li>
    <p>The Conv layer takes in a number of input channels <code class="language-plaintext highlighter-rouge">c_in</code> and outputs a number of output channels <code class="language-plaintext highlighter-rouge">c_out</code>.</p>
  </li>
  <li>First layer by default has <code class="language-plaintext highlighter-rouge">c_in=3</code> because normally we have RGB images.</li>
  <li>We set the number of outputs to <code class="language-plaintext highlighter-rouge">c_out=(c_in+1)*8</code>. This gives the second layer an input of 32 channels, which is what the bag of tricks paper recommends.</li>
  <li>The factor of 8 also helps use the GPU architecture more efficiently. This grows / shrinks by itself with the number of input channels, so if you have more inputs then it will have more activations.</li>
</ul>

<p>The first few layers are called the <code class="language-plaintext highlighter-rouge">stem</code> and it looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_in</span><span class="p">,</span> <span class="p">(</span><span class="n">c_out</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># c_in/c_outs for the 3 conv layers
</span><span class="n">stem</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_layer</span><span class="p">(</span><span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
</code></pre></div></div>

<p>Where <code class="language-plaintext highlighter-rouge">conv_layer</code> is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">act_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">ks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">ks</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
  

<span class="k">def</span> <span class="nf">conv_layer</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zero_bn</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">nf</span><span class="p">)</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">bn</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.</span> <span class="k">if</span> <span class="n">zero_bn</span> <span class="k">else</span> <span class="mf">1.</span><span class="p">)</span> <span class="c1"># init batchnorm trick
</span>    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">ks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span> <span class="n">bn</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">act</span><span class="p">:</span> <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">act_fn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">conv_layer</code> is a <code class="language-plaintext highlighter-rouge">nn.Sequential</code> object of:</p>

<ul>
  <li>a <em>convolution</em>
</li>
  <li>followed by a <em>BatchNorm</em>
</li>
  <li>and optionally an <em>activation</em> (default ReLU)</li>
</ul>

<h3 id="zero-batchnorm-trick">
<a class="anchor" href="#zero-batchnorm-trick" aria-hidden="true"><span class="octicon octicon-link"></span></a>Zero BatchNorm Trick</h3>

<p>After the <code class="language-plaintext highlighter-rouge">stem</code> the remainder of the ResNet’s body is an arbitrary number of <code class="language-plaintext highlighter-rouge">ResBlock</code>s.  In the <code class="language-plaintext highlighter-rouge">ResBlock</code> there is one extra trick with the BatchNorm initialization.  <strong>We sometimes initialize the BatchNorm weights to be 0 and other times we initialize it to 1</strong>.</p>

<p>To get why this would be useful, recall the diagram of the standard ResBlock:</p>

<p><img src="/blog/images/fastai/image-20200419214622392.png" alt="image-20200419214622392"></p>

<p>Each ‘weight layer’ in the above is a Conv/BatchNorm. If the input to a ResBlock is <code class="language-plaintext highlighter-rouge">x</code> then its output is <code class="language-plaintext highlighter-rouge">x+block(x)</code>. If we initialize the <em>final BatchNorm layer</em> in the block to 0, then this is the same as multiplying the input by 0, so <code class="language-plaintext highlighter-rouge">block(x)=0</code>. Therefore at the start of training <em>all ResBlocks just return their inputs</em>, and this mimics a network that has fewer layers and is easier to train at the initial stage.</p>

<h3 id="resblock">
<a class="anchor" href="#resblock" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResBlock</h3>

<p>After the <code class="language-plaintext highlighter-rouge">stem</code> the remainder of the ResNet’s body is an arbitrary number of <code class="language-plaintext highlighter-rouge">ResBlock</code>s. The <code class="language-plaintext highlighter-rouge">ResBlock</code> code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">noop</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span>  <span class="c1"># identity operation
</span>
<span class="k">class</span> <span class="nc">ResBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">expansion</span><span class="p">,</span> <span class="n">ni</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># expansion = 1 or 4
</span>        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">nf</span><span class="p">,</span><span class="n">ni</span> <span class="o">=</span> <span class="n">nh</span><span class="o">*</span><span class="n">expansion</span><span class="p">,</span><span class="n">ni</span><span class="o">*</span><span class="n">expansion</span>
        <span class="n">layers</span>  <span class="o">=</span> <span class="p">[</span><span class="n">conv_layer</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
                   <span class="n">conv_layer</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">zero_bn</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">]</span> <span class="k">if</span> <span class="n">expansion</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="p">[</span>
                   <span class="n">conv_layer</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                   <span class="n">conv_layer</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
                   <span class="n">conv_layer</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">zero_bn</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">idconv</span> <span class="o">=</span> <span class="n">noop</span> <span class="k">if</span> <span class="n">ni</span><span class="o">==</span><span class="n">nf</span> <span class="k">else</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">noop</span> <span class="k">if</span> <span class="n">stride</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">act_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">idconv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div></div>

<p>There several different types of ResBlocks in a ResNet and these are all contained in the comlete <code class="language-plaintext highlighter-rouge">ResBlock</code> code above, conditional on parameters <code class="language-plaintext highlighter-rouge">expansion</code> and <code class="language-plaintext highlighter-rouge">stride</code>.</p>

<p>There is the <em>standard</em> ResBlock that has <code class="language-plaintext highlighter-rouge">expansion=1</code> and <code class="language-plaintext highlighter-rouge">stride=1</code> that are stacked together in ResNet18/30, like those in this diagram:</p>

<p><img src="/blog/images/fastai/image-20201013004010663.png" alt="image-20201013004010663" style="zoom:50%;"></p>

<p>Besides the standard there are two other ResBlocks - the <em>Expansion</em> (AKA Bottleneck) ResBlock, and the <em>Downsampling</em> ResBlock. Let’s go through them and how they are tweaked by the Bag of Tricks paper.</p>

<h4 id="expansionbottleneck-resblock">
<a class="anchor" href="#expansionbottleneck-resblock" aria-hidden="true"><span class="octicon octicon-link"></span></a>Expansion/BottleNeck ResBlock</h4>

<p>For ResNet18/34 the ResBlock looks like the diagram on the left below - a tensor comes in with shape <code class="language-plaintext highlighter-rouge">[*, *, 64]</code> and undergoes two 3x3 <code class="language-plaintext highlighter-rouge">conv_layers</code>. However, for the deeper ResNets (e.g. 50+) doing all these 3x3 <code class="language-plaintext highlighter-rouge">conv_layers</code> is expensive and costs memory. Instead we use a <em>BottleNeck</em> that has a 1x1 convolution to squish number of channels down by 4, then we do a <em>single</em> 3x3 convolution, followed by another 1x1 to project it back up to the original shape. Since we are squishing the number of channels down by a factor of 4 in the 3x3 <code class="language-plaintext highlighter-rouge">conv_layer</code>, we <strong><em>expand</em></strong> the normal number of channels in the model by a factor of 4 to get the equivalent size of the convolution as the basic block. See the diagram on the right below:</p>

<p><img src="/blog/images/fastai/image-20201011231816059.png" alt="image-20201011231816059" style="zoom:50%;"></p>

<p><em>(Diagram taken from the original <a href="https://arxiv.org/abs/1512.03385">ResNet paper</a>)</em></p>

<p>In the <code class="language-plaintext highlighter-rouge">ResBlock</code> code, this bottleneck layer is implemented through the <code class="language-plaintext highlighter-rouge">expansion</code> parameter. <code class="language-plaintext highlighter-rouge">expansion</code> is either 1 or 4. We multiple the number of input and output channels by this factor: <code class="language-plaintext highlighter-rouge">nf,ni = nh*expansion,ni*expansion</code>. This factor is 4 for ResNet50+.</p>

<h4 id="downsampling-resblock">
<a class="anchor" href="#downsampling-resblock" aria-hidden="true"><span class="octicon octicon-link"></span></a>Downsampling ResBlock</h4>

<p>At the start of a new group of ResBlocks we typically half the spatial dimensions with a stride 2 convolution and also double the number of channels. The dimensions have now changed so what happens to the identity connection? In the original paper they use a projection matrix to reduce the dimensions, and other implementions I’ve seen use a 1x1 <code class="language-plaintext highlighter-rouge">conv_layer</code> with stride 2.</p>

<p>The way they do it in the bag of tricks paper is to do an <code class="language-plaintext highlighter-rouge">AveragePooling</code> layer with stride 2 to half the grid size, followed by a 1x1 <code class="language-plaintext highlighter-rouge">conv_layer</code> (stride 1) to increase the number of channels. Here is a diagram of the downsampling ResBlock:</p>

<p><img src="/blog/images/fastai/image-20201003175647599.png" alt="image-20201003175647599" style="zoom:50%;"></p>

<p>A further tweak, which is shown above, is putting the stride 2 in the 3x3 <code class="language-plaintext highlighter-rouge">conv_layer</code>. Prior to this, people the stride 2 in the first 1x1 <code class="language-plaintext highlighter-rouge">conv_layer</code>, which is a terrible thing to do because you are just throwing away 3 quarters of the data.</p>

<h3 id="putting-it-together">
<a class="anchor" href="#putting-it-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting it Together</h3>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?t=2390">(Jump to Lesson 12 video)</a></em></p>

<p>Here is the code for creating any ResNet model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">XResNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">expansion</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">nfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_in</span><span class="p">,</span> <span class="p">(</span><span class="n">c_in</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
        <span class="n">stem</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_layer</span><span class="p">(</span><span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>

        <span class="n">nfs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="o">//</span><span class="n">expansion</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">512</span><span class="p">]</span>
        <span class="n">res_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">cls</span><span class="p">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">expansion</span><span class="p">,</span> <span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                                      <span class="n">n_blocks</span><span class="o">=</span><span class="n">l</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">2</span><span class="p">)</span>
                  <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">)]</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">cls</span><span class="p">(</span>
            <span class="o">*</span><span class="n">stem</span><span class="p">,</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="o">*</span><span class="n">res_layers</span><span class="p">,</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nfs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">expansion</span><span class="p">,</span> <span class="n">c_out</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">init_cnn</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="n">expansion</span><span class="p">,</span> <span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">ResBlock</span><span class="p">(</span><span class="n">expansion</span><span class="p">,</span> <span class="n">ni</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="n">nf</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">stride</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">)])</span>
</code></pre></div></div>

<p>Combined with the <code class="language-plaintext highlighter-rouge">ResBlock</code> code that is all that is required for creating any ResNet model. :)</p>

<p>Now we can create all of our ResNets by listing how many blocks we have in each layer and the expansion factor (4 for 50+):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">xresnet18</span> <span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="n">XResNet</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">xresnet34</span> <span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="n">XResNet</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">xresnet50</span> <span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="n">XResNet</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">xresnet101</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="n">XResNet</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">xresnet152</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="n">XResNet</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="image-classification-transfer-learning--fine-tuning">
<a class="anchor" href="#image-classification-transfer-learning--fine-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image Classification: Transfer Learning / Fine Tuning</h2>

<p><em>(<a href="https://youtu.be/vnOpEwmtFJ8?&amp;t=2920">Jump_to lesson 12 video</a>)</em>, <em>(Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/11a_transfer_learning.ipynb">11a_transfer_learning.ipynb</a>)</em></p>

<p>Recall the familiar ‘one-two’ training combo from part 1 of fastai for getting good results on Image classification tasks:</p>

<ol>
  <li>Get pretrained ResNet weights</li>
  <li>Create a new ‘head’ section of the model for your new task.</li>
  <li>Freeze all the layers except the head.</li>
  <li>Run a few cycles of training for the head.</li>
  <li>Unfreeze all the layers and run a few more training cycles.</li>
</ol>

<p>Let’s implement the code required to make that possible.</p>

<h3 id="custom-head">
<a class="anchor" href="#custom-head" aria-hidden="true"><span class="octicon octicon-link"></span></a>Custom Head</h3>

<p>In the notebook they want to use a model pretrained on ImageWoof to fine tune for the Pets dataset. We can save our ImageWoof model to disk as a dictionary of <code class="language-plaintext highlighter-rouge">layer_name: tensor</code>. PyTorch model has this readily available with <code class="language-plaintext highlighter-rouge">st = learn.model.state_dict()</code>.</p>

<p>Let’s go through the process of loading back in the pretrained ImageWoof model. First we need to create a <code class="language-plaintext highlighter-rouge">Learner</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">xresnet18</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt_func</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm_imagenette</span><span class="p">)</span>
</code></pre></div></div>

<p>ImageWoof has 10 activations, so we need to match this so the weights match up: <code class="language-plaintext highlighter-rouge">c_out=10</code>.</p>

<p>We can then load the ImageWoof state dictionary and load the weights into our <code class="language-plaintext highlighter-rouge">Learner</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'imagewoof'</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span>
<span class="n">m</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">st</span><span class="p">)</span>
</code></pre></div></div>

<p>This is now just the recovered ImageWoof model. We want to change it so it can be used on the new dataset, so we take off the last linear layer for 10 classes and replace it with one for the 37 classes of the Pets dataset. We can find the point we want to cut the model by searching for the index <code class="language-plaintext highlighter-rouge">cut</code> that points to the <code class="language-plaintext highlighter-rouge">nn.AdaptiveAvfPool2d</code> layer, which is the penultimate layer before the head. The cut model is then: <code class="language-plaintext highlighter-rouge">m_cut = m[:cut]</code>.</p>

<p>The number of outputs of our new head is 37, what about the inputs? We can determine that easily by just running a batch through the cut model: <code class="language-plaintext highlighter-rouge">ni = m_cut(xb).shape[1]</code>.</p>

<p>We can now create our new head for the Pets model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m_new</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">m_cut</span><span class="p">,</span> <span class="n">AdaptiveConcatPool2d</span><span class="p">(),</span> <span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ni</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">37</span><span class="p">))</span>
</code></pre></div></div>

<p>Where we also use <code class="language-plaintext highlighter-rouge">AdaptiveConcatPool2d</code> which is simply average pooling and max pooling catted together into a <code class="language-plaintext highlighter-rouge">2*ni</code> sized vector. <em>This double pooling is a fastai trick that gives a little boost over just doing one of the other.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdaptiveConcatPool2d</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sz</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">sz</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ap</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">mp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">ap</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>With this simple transfer learning we can get <strong>71%</strong> on Pets after 4 epochs. Without the transfer learning we only get 37%.</p>

<p><strong>All the steps together:</strong> put this whole process in a function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">adapt_model</span><span class="p">(</span><span class="n">learn</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">cut</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">children</span><span class="p">())</span>
               <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">))</span>
    <span class="n">m_cut</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">[:</span><span class="n">cut</span><span class="p">]</span>
    <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">learn</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">m_cut</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">ni</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">m_new</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">m_cut</span><span class="p">,</span> <span class="n">AdaptiveConcatPool2d</span><span class="p">(),</span> <span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ni</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">c_out</span><span class="p">))</span>
    <span class="n">learn</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">m_new</span>
</code></pre></div></div>

<p>Then the weight loading and model adaption is simply becomes:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">xresnet18</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt_func</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm_imagenette</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">mdl_path</span><span class="o">/</span><span class="s">'iw5'</span><span class="p">))</span>
<span class="n">adapt_model</span><span class="p">(</span><span class="n">learn</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="freezing-layers">
<a class="anchor" href="#freezing-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Freezing Layers</h3>

<p>You can freeze layers by turning their gradients off:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s do one-two training combo.</p>

<ol>
  <li>Freezing the body and training the head 3 epochs gets 54%.</li>
  <li>Unfreezing and training the rest of the model for another 5 epochs gets <strong>56%(!)</strong>
</li>
</ol>

<p>It’s better than not fine tuning, but interestingly when we just fine-tuned without the freezing we got a way better result of 71%. Why doesn’t it work?</p>

<blockquote>
  <p>Every time something weird happens in your neural net, it’s almost certainly due to batchnorm. Because batchnorm makes everything weird. :D</p>
</blockquote>

<p>The batchnorm layers in the pretrained model have learned means and stds for a different dataset (ImageWoof). When we trained froze the body and trained the head, the head was learning with a different set of batch norm statistics. When we unfreeze the body the batchnorm statistics can now change, which effectively causes the ground to shift from underneath the later layers that we just changed.</p>

<p><strong>Fix: Don’t freeze the weights in the batchnorm layers when doing partial layer training.</strong></p>

<p>Here’s the function that does the freezing and unfreezing of layers, which skips batchnorm layers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">set_grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">)):</span> <span class="k">return</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<p>We can use PyTorch <code class="language-plaintext highlighter-rouge">apply</code> method to apply this function to our model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">set_grad</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="bp">False</span><span class="p">));</span>
</code></pre></div></div>

<h2 id="ulmfit-from-scratch">
<a class="anchor" href="#ulmfit-from-scratch" aria-hidden="true"><span class="octicon octicon-link"></span></a>ULMFiT From Scratch</h2>

<p><a href="https://youtu.be/vnOpEwmtFJ8?t=4687"><em>(Jump to lesson 12 video)</em></a></p>

<blockquote>
  <p>ULMFiT is transfer learning applied to AWD-LSTM for NLP</p>
</blockquote>

<p>There has been a lot of ground-breaking innovation in the realm of transfer learning applied to NLP recently - e.g. GPT2, BERT. These are all based on Transformers, which are currently very hot, so one could think that LSTMs aren’t used or interesting anymore. However when you look at recent competitive machine learning results (<em>NB recorded 2019</em>), you see ULMFiT beating BERT - from <a href="http://2019.poleval.pl/index.php/results/">poleval2019</a>:</p>

<p><img src="/blog/images/fastai/image-20200820231813649.png" alt="image-20200820231813649" style="zoom:50%;"></p>

<p><strong>Jeremy says…:</strong></p>

<blockquote>
  <p>It’s definitely not true that RNNs are in the past. Transformers and CNNs for text have a lot of problems. They don’t have state. So if you are doing speech recognition, for every sample you look at you have to do an entire analysis of all the sample around it again and again and again. So it’s rediculously wasteful. Wheras RNNs have state. But they are fiddly and hard to deal with when you want to do research and change things. RNNs, and in particular AWD-LSTM, have had a lot of research done on how to regularize them carefully. Stephen Merity did a huge amount of work on all the different way they can be regularized. There is nothing like that outside the RNN world. At the moment my goto choice is still ULMFiT for most real world tasks. I’m not seeing transformers win competitions yet.</p>
</blockquote>

<p>There are <strong>lots of things that are sequences</strong> that aren’t text - <em>genomics, chemical bonding analysis, and drug discovery.</em> People are finding exciting applications of ULMFiT outside of NLP.</p>

<p>Here is a review of the ULMFiT pipeline that we saw in part 1 of fastai v3:</p>

<p><img src="/blog/images/fastai/image-20200820233016330.png" alt="image-20200820233016330" style="zoom:50%;"></p>

<p>We are going to code this up from scratch…</p>

<h3 id="preprocess-text">
<a class="anchor" href="#preprocess-text" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprocess Text</h3>

<p><a href="https://youtu.be/vnOpEwmtFJ8?t=4964"><em>(Jump to lesson 12 video)</em></a></p>

<p><em>Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb">12_text.ipynb</a></em></p>

<p>We will use the IMDB dataset that consists of 50,000 labeled reviews  of movies (positive or negative) and 50,000 unlabelled ones. It contains a <code class="language-plaintext highlighter-rouge">train</code> folder, a <code class="language-plaintext highlighter-rouge">test</code> folder, and an <code class="language-plaintext highlighter-rouge">unsup</code> (unsupervised) folder.</p>

<p>First thing we need to do is create a datablocks <code class="language-plaintext highlighter-rouge">ItemList</code> subclass for text:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_file</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span> 
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">'utf8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="k">return</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    
<span class="k">class</span> <span class="nc">TextList</span><span class="p">(</span><span class="n">ItemList</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">from_files</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">extensions</span><span class="o">=</span><span class="s">'.txt'</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">get_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">extensions</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="n">include</span><span class="p">),</span> <span class="n">path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">Path</span><span class="p">):</span> <span class="k">return</span> <span class="n">read_file</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">i</span>
</code></pre></div></div>

<p>This was easy because we reuse much code we wrote previously. We already have the <code class="language-plaintext highlighter-rouge">get_files</code> function, which now searches for <code class="language-plaintext highlighter-rouge">.txt</code> files instead of images. Then we override <code class="language-plaintext highlighter-rouge">get</code> to now call a function, <code class="language-plaintext highlighter-rouge">read_file</code> that reads a text file. Now we can load the dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">il</span> <span class="o">=</span> <span class="n">TextList</span><span class="p">.</span><span class="n">from_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">,</span> <span class="s">'unsup'</span><span class="p">])</span>
</code></pre></div></div>

<p>If we look at one of the items it will be just the raw text of a IMDB movie review.</p>

<p>We can just throw this in to a model - it needs to be numbers. So we need to <strong>Tokenize</strong> and <strong>Numericalize</strong> it.</p>

<h4 id="tokenizing">
<a class="anchor" href="#tokenizing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tokenizing</h4>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?&amp;t=5070">(Jump_to lesson 12 video)</a></em></p>

<p>We need to tokenize the dataset first, which is splitting a sentence in  individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: <code class="language-plaintext highlighter-rouge">don't</code> for instance is split between <code class="language-plaintext highlighter-rouge">do</code> and <code class="language-plaintext highlighter-rouge">n't</code>. We  will use a processor for this, in conjunction with the <a href="https://spacy.io/">spacy library</a>.</p>

<p><strong>Before tokenizing</strong>, we will apply a bit of preprocessing on the  texts to clean them up (we saw the one up there had some HTML code).  These rules are applied <em>before</em> we split the sentences in tokens.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">default_pre_rules</span> <span class="o">=</span> <span class="p">[</span><span class="n">fixup_text</span><span class="p">,</span> <span class="n">replace_rep</span><span class="p">,</span> <span class="n">replace_wrep</span><span class="p">,</span>
                     <span class="n">spec_add_spaces</span><span class="p">,</span> <span class="n">rm_useless_spaces</span><span class="p">,</span> <span class="n">sub_br</span><span class="p">]</span>

<span class="n">default_spec_tok</span> <span class="o">=</span> <span class="p">[</span><span class="n">UNK</span><span class="p">,</span> <span class="n">PAD</span><span class="p">,</span> <span class="n">BOS</span><span class="p">,</span> <span class="n">EOS</span><span class="p">,</span> <span class="n">TK_REP</span><span class="p">,</span>
                    <span class="n">TK_WREP</span><span class="p">,</span> <span class="n">TK_UP</span><span class="p">,</span> <span class="n">TK_MAJ</span><span class="p">]</span>
</code></pre></div></div>

<p>These are:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">fixup_text</code>: Fixes various messy things seen in documents. For example, HTML artifacts.</li>
  <li>
<code class="language-plaintext highlighter-rouge">replace_rep</code>: Replace repetitions at the character level: <code class="language-plaintext highlighter-rouge">!!!!!</code> -&gt; <code class="language-plaintext highlighter-rouge">TK_REP 5 !</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">replace_wrep</code>: Replace word repetitions: <code class="language-plaintext highlighter-rouge">word word word</code> -&gt; <code class="language-plaintext highlighter-rouge">TK_WREP 3 word</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">spec_add_spaces</code>: Add spaces around <code class="language-plaintext highlighter-rouge">/</code> and <code class="language-plaintext highlighter-rouge">#</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">rm_userless_spaces</code>: If we find more than two spaces in a row, replace them with one space</li>
  <li>
<code class="language-plaintext highlighter-rouge">sub_br</code>: Replaces the <code class="language-plaintext highlighter-rouge">&lt;br /&gt;</code> by <code class="language-plaintext highlighter-rouge">\n</code>
</li>
</ul>

<p>Why do <code class="language-plaintext highlighter-rouge">replace_rep</code> and <code class="language-plaintext highlighter-rouge">replace_wrep</code>? Let’s image a tweet that said: <em>“THIS WAS AMAZING!!!!!!!!!!!!!!!!!!!!!”</em>.  We could treat the exclamation marks as one token, so we would then have a single vocab item that is specifically 21 exclamation marks. You probably wouldn’t see that again so it wouldn’t even end up in your vocab, and if it did it would be so rare that you wouldn’t be able to learn anything interesting about it. It would also absurdly be different from the case where there is 20 or 22 exclamation marks. But some big number of exclamation marks does have a meaning and we know that it is different from the case where there is just a single one. If we instead replace it with ` xxrep 21 ! `, then this is just three tokens where the model can learn that lots of repeating exclamation marks is a general concept that has certain semantics to it.</p>

<p>Another alternative would be to turn our sequence of exclamation marks into 21 tokens in a row, but now we are asking our LSTM to hang onto that state for 21 timesteps, which is a lot more work for it to do and it won’t do as good a job.</p>

<p><em>What we are trying to do in NLP is to make it so that the things in our vocabulary are as meaningful as possible.</em></p>

<p><br></p>

<p><strong>After tokenizing</strong> with <code class="language-plaintext highlighter-rouge">spacey</code> we apply a couple more rules:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">replace_all_caps</code>: Replace tokens in ALL CAPS by their lower version and insert <code class="language-plaintext highlighter-rouge">TK_UP</code> before.</li>
  <li>
<code class="language-plaintext highlighter-rouge">deal_caps</code>: Replace all Captitalized tokens by their lower version and add <code class="language-plaintext highlighter-rouge">TK_MAJ</code> before.</li>
  <li>
<code class="language-plaintext highlighter-rouge">add_eos_bos</code>: Add <em>before-stream</em> (<code class="language-plaintext highlighter-rouge">BOS</code>) and <em>end-of-stream</em> (<code class="language-plaintext highlighter-rouge">EOS</code>) tokens on either side of a list of tokens at the start/end of a document. These tokens turn out to be very important. When the model encounters a <code class="language-plaintext highlighter-rouge">EOS</code> token it knows it is at the end of a document and that the next document is something new. So it will have to learn to reset its state somehow.</li>
</ul>

<p>For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">replace_all_caps</span><span class="p">([</span><span class="s">'I'</span><span class="p">,</span> <span class="s">'AM'</span><span class="p">,</span> <span class="s">'SHOUTING'</span><span class="p">])</span>
<span class="p">[</span><span class="s">'I'</span><span class="p">,</span> <span class="s">'xxup'</span><span class="p">,</span> <span class="s">'am'</span><span class="p">,</span> <span class="s">'xxup'</span><span class="p">,</span> <span class="s">'shouting'</span><span class="p">]</span>


<span class="o">&gt;</span> <span class="n">deal_caps</span><span class="p">([</span><span class="s">'My'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'Jeremy'</span><span class="p">])</span>
<span class="p">[</span><span class="s">'xxmaj'</span><span class="p">,</span> <span class="s">'my'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'xxmaj'</span><span class="p">,</span> <span class="s">'jeremy'</span><span class="p">]</span>
</code></pre></div></div>

<p><strong>Tokenizing with <code class="language-plaintext highlighter-rouge">spacey</code></strong> is quite slow because <code class="language-plaintext highlighter-rouge">spacey</code> does things very carefully. <code class="language-plaintext highlighter-rouge">spacey</code> has a sophisticated parser based tokenizer and it using it will improve your accuracy a lot, so it’s worth using. Luckily tokenizing is embarrassingly parallel so we can speed things up using multi-processing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TokenizeProcessor</span><span class="p">(</span><span class="n">Processor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s">"en"</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">pre_rules</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">post_rules</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">chunksize</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">max_workers</span> <span class="o">=</span> <span class="n">chunksize</span><span class="p">,</span><span class="n">max_workers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="n">blank</span><span class="p">(</span><span class="n">lang</span><span class="p">).</span><span class="n">tokenizer</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">default_spec_tok</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_case</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[{</span><span class="n">ORTH</span><span class="p">:</span> <span class="n">w</span><span class="p">}])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pre_rules</span>  <span class="o">=</span> <span class="n">default_pre_rules</span>  <span class="k">if</span> <span class="n">pre_rules</span>  <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">pre_rules</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">post_rules</span> <span class="o">=</span> <span class="n">default_post_rules</span> <span class="k">if</span> <span class="n">post_rules</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">post_rules</span>

    <span class="k">def</span> <span class="nf">proc_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="n">i</span><span class="p">,</span><span class="n">chunk</span> <span class="o">=</span> <span class="n">args</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="p">[</span><span class="n">compose</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pre_rules</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">chunk</span><span class="p">]</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">d</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">chunk</span><span class="p">)]</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">compose</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post_rules</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">docs</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">items</span><span class="p">):</span> 
        <span class="n">toks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Path</span><span class="p">):</span> <span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="n">read_file</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">items</span><span class="p">]</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span><span class="n">items</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">chunksize</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">chunksize</span><span class="p">))]</span>
        <span class="n">toks</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">proc_chunk</span><span class="p">,</span> <span class="n">chunks</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">max_workers</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">toks</span><span class="p">,</span> <span class="p">[])</span>
    
    <span class="k">def</span> <span class="nf">proc1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">proc_chunk</span><span class="p">([</span><span class="n">item</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">deprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">toks</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">deproc1</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">toks</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">deproc1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tok</span><span class="p">):</span>
        <span class="k">return</span> <span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span>
</code></pre></div></div>

<p>Here is what an example raw input looks like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Comedian Adam Sandler\'s last theatrical release "I Now Pronounce You Chuck and Larry" served as a loud and proud plea for tolerance of the gay community. The former "Saturday Night Live" funnyman\'s new movie "You Don\'t Mess with the Zohan" (*** out o'
</code></pre></div></div>

<p>And here’s what that looks like after tokenization:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'xxbos • xxmaj • comedian • xxmaj • adam • xxmaj • sandler • \'s • last • theatrical • release • " • i • xxmaj • now • xxmaj • pronounce • xxmaj • you • xxmaj • chuck • and • xxmaj • larry • " • served • as • a • loud • and • proud • plea • for • tolerance • of • the • gay • community • . • xxmaj • the • former • " • xxmaj • saturday • xxmaj • night • xxmaj • live • " • funnyman • \'s • new • movie •'
</code></pre></div></div>

<h4 id="numericalize-tokens">
<a class="anchor" href="#numericalize-tokens" aria-hidden="true"><span class="octicon octicon-link"></span></a>Numericalize Tokens</h4>

<p>Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a  processor (not so different from the <code class="language-plaintext highlighter-rouge">CategoryProcessor</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>


<span class="k">class</span> <span class="nc">NumericalizeProcessor</span><span class="p">(</span><span class="n">Processor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_vocab</span><span class="o">=</span><span class="mi">60000</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">max_vocab</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">min_freq</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">,</span><span class="n">max_vocab</span><span class="p">,</span><span class="n">min_freq</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">items</span><span class="p">):</span>
        <span class="c1">#The vocab is defined on the first use.
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">p</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">items</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">o</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="n">freq</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_vocab</span><span class="p">)</span> <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_freq</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">default_spec_tok</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">'otoi'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">otoi</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">,{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">)})</span> 
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">proc1</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">items</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">proc1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">otoi</span><span class="p">[</span><span class="n">o</span><span class="p">]</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">item</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">deprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">deproc1</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">deproc1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">]</span>
</code></pre></div></div>

<p>Tokenizing and Numericalizing text takes a while and so it’s <em>best to do it once and then serialize it.</em></p>

<h4 id="batching-text-for-rnn-training">
<a class="anchor" href="#batching-text-for-rnn-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batching Text for RNN Training</h4>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?&amp;t=5565">(Jump_to lesson 12 video)</a></em></p>

<p>Batching up language model data requires a bit more care than it does with say image data. Let’s take work through batching with an example piece of text:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stream</span> <span class="o">=</span> <span class="s">"""
In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. 
First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.
Then we will study how we build a language model and train it.</span><span class="se">\n</span><span class="s">
"""</span>
</code></pre></div></div>

<p>Let’s use a <strong>batch-size of 6</strong>. This sequences happens to divide into 6 pieces of length 15, so the length of our 6 sequences is 15.</p>

<p><img src="/blog/images/fastai/image-20200830230747354.png" alt="image-20200830230747354" style="zoom:50%;"></p>

<p>Every forward pass we will give our model chunks of text 5 tokens long. This is called the <strong>backpropagation through time (BPTT)</strong>. This is a fancy sounding name, but it just means that every batch we view the RNN as being unfolded 5 times through time as shown below <a href="https://commons.wikimedia.org/wiki/File:Unfold_through_time.png"><em>(source)</em></a>:</p>

<p><a title="Headlessplatter / Public domain" href="https://commons.wikimedia.org/wiki/File:Unfold_through_time.png"><img width="512" alt="Unfold through time" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Unfold_through_time.png/512px-Unfold_through_time.png"></a></p>

<p>So the model would make 5 predictions for each mini-batch, accumulate the errors across the 5 time steps, then update the weights. (In other places this is called <em>Truncated BPTT</em>)</p>

<p>With <code class="language-plaintext highlighter-rouge">bptt=5</code> we go over all the sequences in 3 mini-batches:</p>

<p><img src="/blog/images/fastai/image-20200830231555798.png" alt="image-20200830231555798" style="zoom:50%;"></p>

<p><em>To reconstruct the order of the original text read the top row of all the batches, then the second, and so on.</em></p>

<p>The batch-size is like the number of texts that you train the model on in parallel. Along the rows of the batches, the text is in <em>sequential order</em>. This is essential because the model is creating an internal state that depends on what it is seeing. It needs to be in order.</p>

<p><em>What’s the difference between sequence length, batch-size, and bptt??</em></p>

<ul>
  <li><em>Here is a <a href="https://forums.fast.ai/t/lesson-12-2019-discussion-and-wiki/44009/257">brilliant graphic from Stefano Giomo</a> that illustrates what these all mean.</em></li>
</ul>

<p>Lets create a dataloader for language models. At the beginning of each epoch, it’ll shuffle the articles (if <code class="language-plaintext highlighter-rouge">shuffle=True</code>) and create a big stream by concatenating all of them. We divide this big stream in <code class="language-plaintext highlighter-rouge">bs</code> smaller streams. That we will read in chunks of <code class="language-plaintext highlighter-rouge">bptt</code> length.</p>

<p>What about the source <code class="language-plaintext highlighter-rouge">x</code> and target <code class="language-plaintext highlighter-rouge">y</code> of the language model task? For training our language model using self-supervised learning we want to take in a word and then predict the next word in the sequence. Therefore, the target <code class="language-plaintext highlighter-rouge">y</code> will be exactly the same as <code class="language-plaintext highlighter-rouge">x</code>, but shifted over by one word.</p>

<p>Let’s create this dataloader:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LM_Dataset</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">bptt</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span><span class="n">bs</span><span class="p">,</span><span class="n">bptt</span><span class="p">,</span><span class="n">shuffle</span>
        <span class="n">total_len</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_batch</span> <span class="o">=</span> <span class="n">total_len</span> <span class="o">//</span> <span class="n">bs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batchify</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">n_batch</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">bs</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">source</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">batched_data</span><span class="p">[</span><span class="n">idx</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">]</span>
        <span class="n">seq_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">bptt</span>
        <span class="c1"># x, y (x shifted by 1 word)
</span>        <span class="k">return</span> <span class="n">source</span><span class="p">[</span><span class="n">seq_idx</span><span class="p">:</span><span class="n">seq_idx</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="p">],</span><span class="n">source</span><span class="p">[</span><span class="n">seq_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">seq_idx</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">batchify</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">texts</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">x</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span><span class="p">:</span> <span class="n">texts</span> <span class="o">=</span> <span class="n">texts</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">))]</span>
        <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">batched_data</span> <span class="o">=</span> <span class="n">stream</span><span class="p">[:</span><span class="bp">self</span><span class="p">.</span><span class="n">n_batch</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">].</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_batch</span><span class="p">)</span>
</code></pre></div></div>

<p>If we look at this, if <code class="language-plaintext highlighter-rouge">x</code> is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"xxbos well worth watching , "
</code></pre></div></div>

<p>Then <code class="language-plaintext highlighter-rouge">y</code> would be:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"well worth watching , especially"
</code></pre></div></div>

<p><strong><em>Question</em>:</strong> <em>What are the trade-offs to consider between batch-size and BPTT? For example, BTPP=10 vs BS=100, or BTPP=100 vs BS=10? Both would be passing 1000 tokens at a time to the model. What should you consider when tuning the ratio?</em></p>

<blockquote>
  <p>I don’t know the answer. This would make a super great experiment.</p>

  <p>The batch-size is the the thing that lets it parallelize. So if your batch-size is small it’s just going to be super slow. On the other hand, a large batch size with a short bptt you may end up with less state that’s being backpropagated.</p>

  <p>What the ratio should be - I’m not sure.</p>
</blockquote>

<h4 id="batching-text-for-training-classifiers">
<a class="anchor" href="#batching-text-for-training-classifiers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batching Text for Training Classifiers</h4>

<p><a href="https://youtu.be/vnOpEwmtFJ8?t=5879"><em>(Jump to lesson 12 video)</em></a></p>

<p>When we will want to tackle classification, gathering the data will  be a bit different: first we will label our texts with the folder they  come from. We’ve already done this for image models, and so we can reuse the code we already wrote for that:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proc_cat</span> <span class="o">=</span> <span class="n">CategoryProcessor</span><span class="p">()</span>

<span class="n">il</span> <span class="o">=</span> <span class="n">TextList</span><span class="p">.</span><span class="n">from_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">])</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">SplitData</span><span class="p">.</span><span class="n">split_by_func</span><span class="p">(</span><span class="n">il</span><span class="p">,</span> <span class="n">partial</span><span class="p">(</span><span class="n">grandparent_splitter</span><span class="p">,</span> <span class="n">valid_name</span><span class="o">=</span><span class="s">'test'</span><span class="p">))</span>
<span class="n">ll</span> <span class="o">=</span> <span class="n">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">,</span> <span class="n">parent_labeler</span><span class="p">,</span> <span class="n">proc_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">proc_tok</span><span class="p">,</span> <span class="n">proc_num</span><span class="p">],</span> <span class="n">proc_y</span><span class="o">=</span><span class="n">proc_cat</span><span class="p">)</span>
</code></pre></div></div>

<p>The target/dependent variable could be the sentiment of the document: e.g. <code class="language-plaintext highlighter-rouge">pos</code> or <code class="language-plaintext highlighter-rouge">neg</code>.</p>

<p>Are we finished? Not quite.</p>

<p>When we worked with images, by the time we got to modelling they were all the same size (we resized them to a square). For texts, you can’t ignore that <strong><em>some texts are bigger than others</em></strong>. In order to collate a bunch of texts into a batch we will need to <em>apply padding</em> using some padding token to our documents so that documents collated into the same batches have the same length.</p>

<p>However, if we have a mini-batch with a 1000 word document, a 2000 word document, and then a 20 word document. The 20 word document is going to end up with 1980 padding tokens tacked onto the end. As we go through the RNN we are going to be calculating pointlessly on these padding tokens, which is a waste. So the trick is to <em>sort the data first by length.</em> That way your first mini-batch will contain your really long documents and your last mini-batch will contain your really short documents. This will mean that there won’t be much padding and wasted computation in any of the mini-batches.</p>

<p>In fastai this is done by creating a new type of <strong>Sampler</strong>. Naively, we can create a sampler that just sorts all our documents by length, but this would through away any randomness in constructing our batches - no shuffle. We can instead organize all the documents into <em>buckets</em> such that documents of similar size go in the same bucket. We can then do shuffling inside of those buckets to construct our mini-batches. This sampler is called <code class="language-plaintext highlighter-rouge">SortishSampler</code> in the lesson notebook.</p>

<p>Now we have shuffled the documents using <code class="language-plaintext highlighter-rouge">SortishSampler</code> we need to <strong>collate</strong> them into batches - i.e. stick them together into a batch tensor with a fixed known size. We add the padding token (that as an id of <code class="language-plaintext highlighter-rouge">1</code>) at the end of each  sequence to make them all the same size when batching them. <em>Note that we need padding at the end to be able to use <code class="language-plaintext highlighter-rouge">PyTorch</code> convenience functions that will let us ignore that padding.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pad_collate</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">pad_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_first</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">])</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">max_len</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span> <span class="o">+</span> <span class="n">pad_idx</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">pad_first</span><span class="p">:</span> <span class="n">res</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]):]</span> <span class="o">=</span> <span class="n">LongTensor</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>         <span class="n">res</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">]</span> <span class="o">=</span> <span class="n">LongTensor</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">])</span>
</code></pre></div></div>

<p>So to create the data loader:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bs</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">SortishSampler</span><span class="p">(</span><span class="n">ll</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">ll</span><span class="p">.</span><span class="n">train</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">t</span><span class="p">)][</span><span class="mi">0</span><span class="p">]),</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">ll</span><span class="p">.</span><span class="n">train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">pad_collate</span><span class="p">)</span>
</code></pre></div></div>

<p>An example mini-batch looks like:</p>

<p><img src="/blog/images/fastai/image-20200831171024878.png" alt="image-20200831171024878"></p>

<h3 id="lstm-from-scratch">
<a class="anchor" href="#lstm-from-scratch" aria-hidden="true"><span class="octicon octicon-link"></span></a>LSTM From Scratch</h3>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?&amp;t=6330">(Jump_to lesson 12 video)</a></em></p>

<p>Now we will create an RNN. An RNN is like a network with many many layers. For, say, a document with 2000 words, it would be a network with 2000 layers. Of course, it is never explicitly code it that way and instead just use a for-loop.</p>

<p><img src="/blog/images/fastai/image-20200831204520753.png" alt="image-20200831204520753" style="zoom:50%;"></p>

<p>Between every pair of hidden layers we use the same weight matrix. Problem is, trying to handle 2000 network layers we get vanishing gradients and exploding gradients and it’s really hard to get it to work. We can design more complex networks where the output of one RNN is fed into another RNN (stacked RNNs).</p>

<p>To get this thing to work, we need our hidden layers to do something more than just a matrix multiply. We instead use a <strong>LSTM Cell</strong>:</p>

<p><img src="/blog/images/fastai/image-20200831205326388.png" alt="image-20200831205326388" style="zoom:50%;"></p>

<p>Recall that:</p>

<ul>
  <li>Sigmoid function $\sigma$ - <em>is a smooth function that goes from 0 to 1.</em>
</li>
  <li>Tanh function $\tanh$ - <em>is a smooth function that goes from -1 to 1.</em>
</li>
</ul>

<p>How to read this thing?</p>

<ul>
  <li>
    <p>Starting from the bottom you have the input <code class="language-plaintext highlighter-rouge">x</code> and the hidden layer output from the previous layer <code class="language-plaintext highlighter-rouge">h</code> coming into the cell.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">h</code> are fed into the orange <code class="language-plaintext highlighter-rouge">sigmoid</code> and <code class="language-plaintext highlighter-rouge">tanh</code> layers <em>simultaneously.</em> The same values go into those layers.</p>
  </li>
  <li>
    <p>Each of those layers is basically another little hidden layer. <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">h</code> are multiplied by matrices before going through the <code class="language-plaintext highlighter-rouge">sigmoid</code> or <code class="language-plaintext highlighter-rouge">tanh</code> activations. Each of the layers has its own matrices.</p>
  </li>
</ul>

<p>This diagram also includes equations that make it more clear:</p>

<p><img src="/blog/images/fastai/lstm.jpg" alt="LSTM cell and equations"></p>

<p><em>(picture from <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs</a> by Chris Olah, definitely read this.)</em></p>

<p>Let’s follow the path through the cell:</p>

<ul>
  <li>The <em>forget</em> path goes through a sigmoid and then hits the <em>cell value</em>, $C_{t-1}$.   This is just a rank 2 tensor (with mini-batch) and represents the <em>state</em> or <em>memory</em> component of the LSTM that is passed on and updated through the timesteps.</li>
  <li>We multiply this by the output of the forget <code class="language-plaintext highlighter-rouge">sigmoid</code>. So this gate has the ability to zero-out bits of the <em>Cell state</em>. <em>We can look at some of our words coming in and say - based on that we should zero-out some of the Cell state.</em>
</li>
  <li>We then add the selectively forgotten <em>Cell state</em> to the second little neural net. Here we have the outputs of two layers - a <code class="language-plaintext highlighter-rouge">sigmoid</code> and a tanh - which are multiplied together and their product is then added to the selectively forgotten <em>Cell state</em>.</li>
  <li>This part is where the LSTM chooses how to update the <em>Cell state</em>. This is carried through to the next time step as $C_t$.</li>
  <li>$C_t$ is then also put through another <code class="language-plaintext highlighter-rouge">tanh</code> function and multiplied by our fourth and final mini neural net (another <code class="language-plaintext highlighter-rouge">sigmoid</code>) to create the new output state $h_t$, that is passed on to the next time step. This <code class="language-plaintext highlighter-rouge">sigmoid</code> decides which parts of the <em>Cell state</em> to <em>output</em>.</li>
</ul>

<p>It seems pretty weird, but as code it’s very simple to implement. In coding it, rather than have 4 different matrices for each of the internal mini neural nets, it’s more efficient to just stack them all into one 4x matrix and do one matmul. You can then split the output into equal sized chunks using the <code class="language-plaintext highlighter-rouge">chunk</code> function in PyTorch.</p>

<p>Here is the code for a <strong>LSTM cell</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ni</span><span class="p">,</span> <span class="n">nh</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ih</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">nh</span><span class="p">)</span> <span class="c1"># input2hidden
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">hh</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">nh</span><span class="p">)</span> <span class="c1"># hidden2hidden
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">h</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">state</span>
        <span class="c1">#One big multiplication for all the gates is better than 4 smaller ones
</span>        <span class="n">gates</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ih</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">hh</span><span class="p">(</span><span class="n">h</span><span class="p">)).</span><span class="n">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ingate</span><span class="p">,</span><span class="n">forgetgate</span><span class="p">,</span><span class="n">outgate</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">gates</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
        <span class="n">cellgate</span> <span class="o">=</span> <span class="n">gates</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">tanh</span><span class="p">()</span>

        <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">forgetgate</span><span class="o">*</span><span class="n">c</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">ingate</span><span class="o">*</span><span class="n">cellgate</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">outgate</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div>

<p>Then an <strong>LSTM layer</strong> just applies the cell on all the time steps in order.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LSTMLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="o">*</span><span class="n">cell_args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">cell</span><span class="p">(</span><span class="o">*</span><span class="n">cell_args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
            <span class="n">out</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">state</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">state</span>
</code></pre></div></div>

<p>In practice this is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTMLayer</span><span class="p">(</span><span class="n">LSTMCell</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">300</span><span class="p">),</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">300</span><span class="p">))</span>
</code></pre></div></div>

<p>The hidden state and cell states are initialized with <em>zeros</em> at the start of training.</p>

<p><strong>Aside:</strong> <em>there are lots of other ways you can setup a layer that has the ability to selectively update and selectively forget things. For example, there is a popular alternative called a <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a>, which has one less gate. The thing seems to be giving it some way to make the decision to forget things. Then it has the ability to not push state through all the thousands of time steps.</em></p>

<h4 id="pytorchs-lstm-layer">
<a class="anchor" href="#pytorchs-lstm-layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>PyTorch’s LSTM Layer</h4>

<p>We can now use the Pytorch’s <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">own LSTM layer</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> 
               <span class="n">hidden_size</span><span class="p">,</span>
               <span class="n">num_layers</span><span class="p">,</span>
               <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">bs</span><span class="p">,</span> <span class="n">bptt</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">70</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">bptt</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
     <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>

<span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">c1</span><span class="p">)</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<p>It’s worth going over the dimensions because they confused me a lot.</p>

<p>Firstly, remember that this PyTorch module is <strong>not</strong> an individual LSTM cell, rather it is potentially multiple LSTM layers over multple timesteps. With PyTorch’s <code class="language-plaintext highlighter-rouge">LSTM</code> you can specify the <code class="language-plaintext highlighter-rouge">num_layers</code> and we also run it with <code class="language-plaintext highlighter-rouge">bptt</code> timesteps. The following diagram from this <a href="https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm">SO post</a> shows how you should picture this:</p>

<p><img src="/blog/images/fastai/image-20200906143942918.png" alt="image-20200906143942918" style="zoom:50%;"></p>

<p>The depth is the <code class="language-plaintext highlighter-rouge">num_layers</code> and time is <code class="language-plaintext highlighter-rouge">bptt</code>.</p>

<p>The dimensions of the inputs:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">input</code>: <code class="language-plaintext highlighter-rouge">(batch, bptt, input_size)</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">h_0</code>: <code class="language-plaintext highlighter-rouge">(batch, num_layers, hidden_size)</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">c_0</code>: <code class="language-plaintext highlighter-rouge">(batch, num_layers, hidden_size)</code>
</li>
</ul>

<p>The dimensions of the outputs:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">output</code>: <code class="language-plaintext highlighter-rouge">(batch, seq_len, hidden_size)</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">h_n</code>: <code class="language-plaintext highlighter-rouge">(batch, num_layers, hidden_size)</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">c_n</code>: <code class="language-plaintext highlighter-rouge">(batch, num_layers, hidden_size)</code>
</li>
</ul>

<p><em>(NB this is while using <code class="language-plaintext highlighter-rouge">batch_first=True</code>)</em></p>

<h3 id="awd-lstm">
<a class="anchor" href="#awd-lstm" aria-hidden="true"><span class="octicon octicon-link"></span></a>AWD-LSTM</h3>

<p>We want to use the <strong>AWD-LSTM</strong> from <a href="https://arxiv.org/abs/1708.02182">Stephen Merity et al. [2017]</a>. In this paper, the authors thought about all the ways we can regularize and optimize the LSTM model for NLP. In their paper they test all the different kinds of way they can apply <strong>dropout</strong> to the LSTM.</p>

<ul>
  <li>AWD-LSTM <em>stands for:</em> (<strong>A</strong>verage Stochastic Gradient Descent)(<strong>W</strong>eight-<strong>D</strong>ropped)-<strong>LSTM</strong>…</li>
</ul>

<p><strong>Dropout</strong> consists of replacing some coefficients by <code class="language-plaintext highlighter-rouge">0</code> with probablility <code class="language-plaintext highlighter-rouge">p</code>. To ensure that the average of the weights remains constant, we apply a correction factor to the weights that aren’t nullified with value <code class="language-plaintext highlighter-rouge">1/(1-p)</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dropout_mask</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="o">*</span><span class="n">sz</span><span class="p">).</span><span class="n">bernoulli_</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">).</span><span class="n">div_</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
</code></pre></div></div>

<p>This looks like:</p>

<p><img src="/blog/images/fastai/image-20200902205807471.png" alt="image-20200902205807471" style="zoom:50%;"></p>

<p>Once with have a dropout mask <code class="language-plaintext highlighter-rouge">mask</code>, applying the dropout to <code class="language-plaintext highlighter-rouge">x</code> is simply done by <code class="language-plaintext highlighter-rouge">x = x * mask</code>.</p>

<p>With RNN NLP tasks, a tensor <code class="language-plaintext highlighter-rouge">x</code> will have three dimensions:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">batch_size</code>: <em>the number of texts we are training on in parallel</em>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">bptt</code>: <em>the number of tokens we are looking at at a time.</em>
</li>
  <li>The <code class="language-plaintext highlighter-rouge">emb_size</code>: <em>the size of the vector representation of a token.</em>
</li>
</ul>

<p>With (2, 3, 5) this could look like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.1620</span><span class="p">,</span>  <span class="mf">0.0125</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5448</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8244</span><span class="p">,</span>  <span class="mf">0.3781</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.2661</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7103</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5006</span><span class="p">,</span>  <span class="mf">0.5024</span><span class="p">,</span>  <span class="mf">0.7515</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.6798</span><span class="p">,</span>  <span class="mf">0.1970</span><span class="p">,</span>  <span class="mf">0.0260</span><span class="p">,</span>  <span class="mf">0.5037</span><span class="p">,</span>  <span class="mf">0.2735</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">0.0661</span><span class="p">,</span>  <span class="mf">1.2567</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2873</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1245</span><span class="p">,</span>  <span class="mf">0.0959</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5627</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0315</span><span class="p">,</span>  <span class="mf">0.9382</span><span class="p">,</span>  <span class="mf">0.8043</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2791</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.2626</span><span class="p">,</span>  <span class="mf">1.8968</span><span class="p">,</span>  <span class="mf">0.5332</span><span class="p">,</span>  <span class="mf">0.6908</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3327</span><span class="p">]]])</span>
</code></pre></div></div>

<h4 id="rnndropout--variational-dropout">
<a class="anchor" href="#rnndropout--variational-dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>RNNDropout / Variational Dropout</h4>

<p>For each document in the batch we want to have a unique dropout mask, but we also don’t want to randomly apply it on the vocab dimension, so that every token has a different dropout mask. We have a sequence length of say 5, then recall that the RNN will only do a single forward/backward/update pass with those 5.</p>

<p>Therefore the model needs to be the same for all those sequences and so <em>we need to apply the same dropout mask on the entire sequence</em>, otherwise it’s just broken.</p>

<p>The standard <code class="language-plaintext highlighter-rouge">dropout</code> can’t do this for us so we need to create our own layer  called <code class="language-plaintext highlighter-rouge">RNNDropout</code>. We want to have different dropout masks for each member of the batch (document), but replicate it over their sequences. We can elegantly do this with a clever broadcasting trick, where you specify that the dropout mask has shape <code class="language-plaintext highlighter-rouge">(x.size(0), 1, x.size(2))</code>. That way when you multiply by the mask, it will be replicated down the sequence dimension.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RNNDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="o">=</span><span class="n">p</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span> <span class="ow">or</span> <span class="bp">self</span><span class="p">.</span><span class="n">p</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">:</span> <span class="k">return</span> <span class="n">x</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">dropout_mask</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)),</span> <span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">m</span>
</code></pre></div></div>

<p>The output looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">dp</span> <span class="o">=</span> <span class="n">RNNDropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">dp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.6986</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2667</span><span class="p">,</span>  <span class="mf">0.5033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7446</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.5847</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1816</span><span class="p">,</span>  <span class="mf">2.8067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1114</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.4601</span><span class="p">,</span>  <span class="mf">0.6553</span><span class="p">,</span>  <span class="mf">1.4082</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6506</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">1.7197</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0774</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1192</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.9075</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1597</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3051</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.5267</span><span class="p">,</span>  <span class="mf">1.0956</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3911</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.6468</span><span class="p">,</span>  <span class="mf">1.8105</span><span class="p">,</span>  <span class="mf">0.6677</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6478</span><span class="p">,</span>  <span class="mf">0.8841</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5549</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.1027</span><span class="p">,</span>  <span class="mf">0.7640</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4295</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">2.8382</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.6135</span><span class="p">,</span>  <span class="mf">0.1007</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.7681</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1549</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4484</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.5333</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.0678</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4169</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2658</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.3604</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2424</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">3.5358</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.7004</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5376</span><span class="p">]]])</span>
</code></pre></div></div>

<p>Note the positions of the 0s.</p>

<h4 id="weight-dropout--dropconnect">
<a class="anchor" href="#weight-dropout--dropconnect" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Dropout / DropConnect</h4>

<p><strong>Weight dropout</strong> applies dropout not on the activations, <em>but on the weights themselves</em>. But otherwise the aim of regularization is the same as normal dropout. This is also called <strong>DropConnect</strong>. Here this is applied to the weights of the inner LSTM hidden to hidden matrix - $U^i, U^f, U^o, U^g$.</p>

<p>The <a href="http://proceedings.mlr.press/v28/wan13.html">DropConnect paper</a> says:</p>

<blockquote>
  <p>DropConnect is the generalization of Dropout in which each connection, instead of each output unit as in Dropout, can be dropped with probability <code class="language-plaintext highlighter-rouge">p</code>.</p>
</blockquote>

<p>Since there are many more weights than activations to disable, DropConnect creates many more ways of altering the model.<a href="https://stats.stackexchange.com/questions/201569/what-is-the-difference-between-dropout-and-drop-connect"><em>(Also see this Stackoverflow question on the difference between the two.)</em></a></p>

<p>The <strong>downside</strong> is that it requires us to keep a copy of the weights we are using DropConnect on, so the memory requirement will increase for our model. For normal dropout, on the other hand, we just need to store a mask of the activations, of which there are fewer than the weights.</p>

<p>Here’s how to implement it in PyTorch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">WEIGHT_HH</span> <span class="o">=</span> <span class="s">'weight_hh_l0'</span>

<span class="k">class</span> <span class="nc">WeightDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">],</span>
                 <span class="n">layer_names</span><span class="o">=</span><span class="p">[</span><span class="n">WEIGHT_HH</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_p</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">layer_names</span> <span class="o">=</span> <span class="n">module</span><span class="p">,</span><span class="n">weight_p</span><span class="p">,</span><span class="n">layer_names</span>
        
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_names</span><span class="p">:</span>
            <span class="c1">#Makes a copy of the weights of the selected layers.
</span>            <span class="n">w</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">,</span> <span class="n">layer</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">f'</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">_raw'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">data</span><span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_setweights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_names</span><span class="p">:</span>
            <span class="n">raw_w</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">f'</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">_raw'</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">raw_w</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_setweights</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">warnings</span><span class="p">.</span><span class="n">catch_warnings</span><span class="p">():</span>
            <span class="c1">#To avoid the warning that comes because the weights aren't flattened.
</span>            <span class="n">warnings</span><span class="p">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</code></pre></div></div>

<p>It’s not implemented as another layer, like normal dropout is, rather more like a <em>wrapper layer</em>. It takes a <code class="language-plaintext highlighter-rouge">module</code> and a list of <code class="language-plaintext highlighter-rouge">layer_names</code> in its constructor; these are the <code class="language-plaintext highlighter-rouge">layer_names</code> that you want to apply DropConnect to. You can see in the constructor that it stashes the original weight matrix as <code class="language-plaintext highlighter-rouge">"{layer}_raw"</code>. Then in the <code class="language-plaintext highlighter-rouge">forward</code> method it calls <code class="language-plaintext highlighter-rouge">_setweights</code> that loads the stashed weights then applies dropout to the weights and saves the result as <code class="language-plaintext highlighter-rouge">"{layer}"</code>.</p>

<h4 id="embedding-dropout">
<a class="anchor" href="#embedding-dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Embedding Dropout</h4>

<p>The next type of dropout is <strong>Embedding Dropout</strong>. This apples dropout to full rows of the embedding matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EmbeddingDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector."</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">embed_p</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_p</span> <span class="o">=</span> <span class="n">emb</span><span class="p">,</span><span class="n">embed_p</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">padding_idx</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad_idx</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_p</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">dropout_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_p</span><span class="p">)</span>
            <span class="n">masked_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">mask</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">masked_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
          <span class="n">masked_embed</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
          
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">masked_embed</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad_idx</span><span class="p">,</span>
        									 <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">max_norm</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">norm_type</span><span class="p">,</span> 
        									 <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">sparse</span><span class="p">)</span>
</code></pre></div></div>

<p>In practice this looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">enc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">enc_dp</span> <span class="o">=</span> <span class="n">EmbeddingDropout</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">tst_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,(</span><span class="mi">6</span><span class="p">,))</span>
<span class="o">&gt;&gt;</span> <span class="n">enc_dp</span><span class="p">(</span><span class="n">tst_input</span><span class="p">)</span>

<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.2812</span><span class="p">,</span>  <span class="mf">2.0295</span><span class="p">,</span>  <span class="mf">2.9520</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3344</span><span class="p">,</span>  <span class="mf">3.9739</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.3218</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0442</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0116</span><span class="p">,</span>  <span class="mf">0.4450</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9943</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.3218</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0442</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0116</span><span class="p">,</span>  <span class="mf">0.4450</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9943</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9694</span><span class="p">,</span>  <span class="mf">0.0261</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3517</span><span class="p">,</span>  <span class="mf">1.7962</span><span class="p">,</span>  <span class="mf">2.3478</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">EmbeddingBackward</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>

<p>This is dropping out entire words at a time.</p>

<h4 id="the-main-model">
<a class="anchor" href="#the-main-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Main Model</h4>

<p>With all that in place we can code up our LSTM model.</p>

<p>Here is a pseudo-code outline of the forward pass of the model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>        <span class="c1"># Tokenized + numericalized data, (bs, bptt)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (bs, bptt, emb_sz)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">EmbeddingDropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">RNNDropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Loop through LSTMs
</span>    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lstm_layers</span><span class="p">:</span>
        <span class="n">WeightDropout</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">new_h</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
        <span class="n">h</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_h</span>

        <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="n">last_layer</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">RNNDropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>  <span class="c1"># (bs, bptt, hidden_sz)
</span></code></pre></div></div>

<p><em>To see what’s happening at every step of AWD-LSTM visualized with excel, I recommend reading this excellent medium post -  <a href="https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b">Understanding building blocks of ULMFiT [Kerem Turgutlu]</a>.</em></p>

<p>Here is the PyTorch implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AWD_LSTM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."</span>
    <span class="n">initrange</span><span class="o">=</span><span class="mf">0.1</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_sz</span><span class="p">,</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">n_hid</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                 <span class="n">pad_token</span><span class="p">,</span> <span class="n">hidden_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">input_p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
                 <span class="n">embed_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">emb_sz</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">n_hid</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="n">emb_sz</span><span class="p">,</span><span class="n">n_hid</span><span class="p">,</span><span class="n">n_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_sz</span><span class="p">,</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">pad_token</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emb_dp</span> <span class="o">=</span> <span class="n">EmbeddingDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">,</span> <span class="n">embed_p</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">rnns</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">emb_sz</span> <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">n_hid</span><span class="p">,</span> 
                             <span class="n">hidden_size</span><span class="o">=</span><span class="p">(</span><span class="n">n_hid</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">emb_sz</span><span class="p">),</span> 
                             <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">rnns</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">WeightDropout</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">weight_p</span><span class="p">)</span> <span class="k">for</span> <span class="n">rnn</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">rnns</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">emb</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">initrange</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">initrange</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_dp</span> <span class="o">=</span> <span class="n">RNNDropout</span><span class="p">(</span><span class="n">input_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dps</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">RNNDropout</span><span class="p">(</span><span class="n">hidden_p</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">sl</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">bs</span><span class="o">!=</span><span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="c1"># Embedding layer
</span>        <span class="n">raw_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">emb_dp</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
        
        <span class="c1"># Loop through LSTM layers
</span>        <span class="n">new_hidden</span><span class="p">,</span><span class="n">raw_outputs</span><span class="p">,</span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[],[],[]</span>
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="p">(</span><span class="n">rnn</span><span class="p">,</span><span class="n">hid_dp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rnns</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dps</span><span class="p">)):</span>
            <span class="n">raw_output</span><span class="p">,</span> <span class="n">new_h</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">raw_output</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
            <span class="n">new_hidden</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_h</span><span class="p">)</span>
            <span class="n">raw_outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">raw_output</span> <span class="o">=</span> <span class="n">hid_dp</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">raw_output</span><span class="p">)</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">to_detach</span><span class="p">(</span><span class="n">new_hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">raw_outputs</span><span class="p">,</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">_one_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
        <span class="s">"Return one hidden state."</span>
        <span class="n">nh</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_hid</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">emb_sz</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()).</span><span class="n">new</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">bs</span><span class="p">,</span> <span class="n">nh</span><span class="p">).</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"Reset the hidden states. Init with zeros"</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="p">.</span><span class="n">_one_hidden</span><span class="p">(</span><span class="n">l</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">_one_hidden</span><span class="p">(</span><span class="n">l</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_layers</span><span class="p">)]</span>
</code></pre></div></div>

<p>The intermediate LSTM layer outputs are also stored, <code class="language-plaintext highlighter-rouge">raw_outputs</code>, these are needed later for regularization. The last LSTM layer has  <code class="language-plaintext highlighter-rouge">hidden_sz=emb_sz</code>. This is because we are trying to make a word prediction so we want it to output a vector that fits in our word embedding space.</p>

<p>We then need another layer to decode the output of the last <code class="language-plaintext highlighter-rouge">LSTM</code> and tells us what the next word will be. <code class="language-plaintext highlighter-rouge">LinearDecoder</code> is simply a fully connected linear layer that transforms the output of the last LSTM layer to token predictions in our vocabulary - it’s a <em>classifier</em> for words. It uses the same embedding matrix as the encoding layer (<em>tied weights/tied_encoder</em>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">n_hid</span><span class="p">,</span> <span class="n">output_p</span><span class="p">,</span> <span class="n">tie_encoder</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_dp</span> <span class="o">=</span> <span class="n">RNNDropout</span><span class="p">(</span><span class="n">output_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hid</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">tie_encoder</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">tie_encoder</span><span class="p">.</span><span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span> <span class="n">init</span><span class="p">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">raw_outputs</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_dp</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">raw_outputs</span><span class="p">,</span> <span class="n">outputs</span>
</code></pre></div></div>

<p>We also apply <code class="language-plaintext highlighter-rouge">RNNDropout</code> to the input to <code class="language-plaintext highlighter-rouge">LinearDecoder</code>.</p>

<p>We will create this linear layer with <code class="language-plaintext highlighter-rouge">nn.Linear(emb_sz, vocab_size)</code>. Recall the last LSTM layer will output something with shape <code class="language-plaintext highlighter-rouge">(bs, bptt, emb_sz)</code>, so the output of the <code class="language-plaintext highlighter-rouge">LinearDecoder</code> will be a tensor of shape <code class="language-plaintext highlighter-rouge">(bs, bptt, vocab_size)</code>. We can then apply cross-entropy loss to this word prediction to determine how correct the model’s prediction was.</p>

<p>Now we can stack all of this together to make our language model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequentialRNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="s">"A sequential module that passes the reset call to its children."</span>
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">children</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="s">'reset'</span><span class="p">):</span> <span class="n">c</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
              
              
<span class="k">def</span> <span class="nf">get_language_model</span><span class="p">(</span><span class="n">vocab_sz</span><span class="p">,</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">n_hid</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                       <span class="n">pad_token</span><span class="p">,</span> <span class="n">output_p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hidden_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                       <span class="n">input_p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">embed_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                       <span class="n">tie_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    
    <span class="n">rnn_enc</span> <span class="o">=</span> <span class="n">AWD_LSTM</span><span class="p">(</span><span class="n">vocab_sz</span><span class="p">,</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">n_hid</span><span class="o">=</span><span class="n">n_hid</span><span class="p">,</span>
                       <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
                       <span class="n">hidden_p</span><span class="o">=</span><span class="n">hidden_p</span><span class="p">,</span> <span class="n">input_p</span><span class="o">=</span><span class="n">input_p</span><span class="p">,</span>
                       <span class="n">embed_p</span><span class="o">=</span><span class="n">embed_p</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="n">weight_p</span><span class="p">)</span>
    
    <span class="n">enc</span> <span class="o">=</span> <span class="n">rnn_enc</span><span class="p">.</span><span class="n">emb</span> <span class="k">if</span> <span class="n">tie_weights</span> <span class="k">else</span> <span class="bp">None</span>
    
    <span class="k">return</span> <span class="n">SequentialRNN</span><span class="p">(</span><span class="n">rnn_enc</span><span class="p">,</span>
                         <span class="n">LinearDecoder</span><span class="p">(</span><span class="n">vocab_sz</span><span class="p">,</span> <span class="c1"># output=word
</span>                                       <span class="n">emb_sz</span><span class="p">,</span>   <span class="c1"># input=emb vector
</span>                                       <span class="n">output_p</span><span class="p">,</span>
                                       <span class="n">tie_encoder</span><span class="o">=</span><span class="n">enc</span><span class="p">,</span>
                                       <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="gradient-clippingrescaling">
<a class="anchor" href="#gradient-clippingrescaling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Clipping/Rescaling</h4>

<p>With the model in place we can now focus on the training and regularization.</p>

<p>AWD-LSTM paper also recommends <strong>gradient clipping/rescaling</strong>. This is a <em>super good idea</em> for training because it lets you train at higher learning rates and avoid gradients blowing out. This can be especially bad in RNNs because we unroll the RNN and effectively replicated the weight matrix and repeatedly multiply it. This can cause things to grow or shrink exponentially.</p>

<p>The idea is simple - if the gradient gets too large, then we rescale it. We do this by <em>rescaling the norm of the gradient tensor</em> to a hyperparameter <code class="language-plaintext highlighter-rouge">clip</code>:
\(\mathbf{g} \leftarrow c \cdot \mathbf{g}/\|\mathbf{g}\|\)
PyTorch has a <a href="https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html"><code class="language-plaintext highlighter-rouge">clip_grad_norm_</code></a> function that does this. Here it is wrapped in a callback:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GradientClipping</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">clip</span> <span class="o">=</span> <span class="n">clip</span>
    <span class="k">def</span> <span class="nf">after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">clip</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span>
                                     <span class="bp">self</span><span class="p">.</span><span class="n">clip</span><span class="p">)</span>
</code></pre></div></div>

<p>The value used for <code class="language-plaintext highlighter-rouge">clip</code> here is <code class="language-plaintext highlighter-rouge">0.1</code>.</p>

<p>An alternative form is often used where you simply <em>clamp</em> the gradients  between some <code class="language-plaintext highlighter-rouge">[-clip, clip]</code>, where <code class="language-plaintext highlighter-rouge">clip</code> may have some value like 5 or 100. This is provided by PyTorch’s <a href="https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_"><code class="language-plaintext highlighter-rouge">clip_grad_value_</code></a>.</p>

<p>Gradient clipping addresses only the numerical stability of training deep neural network models and does not offer any general  improvement in performance. This is likely even more pertinent when using FP16.</p>

<p><em>More info: <a href="https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/">How to Avoid Exploding Gradients With Gradient Clipping [Machine Learning Mastery]</a></em></p>

<h4 id="training--more-regularization">
<a class="anchor" href="#training--more-regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training + More Regularization</h4>

<p>At the loss calculation stage AWD-LSTM applies two more types of regularization.</p>

<p>The first is <strong>Activation Regularization (AR):</strong> this is an L2 penalty (like weight decay) except on final <em>activations</em>, instead of on weights. We add to the loss an L2 penalty (times hyperparameter $\alpha$) on the last activations of the AWD LSTM (with dropout applied).</p>

<p>The second is <strong>Temporal Activation Regularization (TAR):</strong> we add to the loss an L2 penalty (times hyperparameter $\beta$) on the difference between two consecutive (in terms of words) raw outputs. This checks how much does each activation change by from time step to time step, then takes the square of that. This regularizes the RNN so that it tries not to have things that massively change from time step to time step, because if it’s doing then then it’s probably not a good sign.</p>

<p>Code for the <code class="language-plaintext highlighter-rouge">RNNTrainer</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RNNTrainer</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">β</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">α</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">β</span> <span class="o">=</span> <span class="n">α</span><span class="p">,</span><span class="n">β</span>
    
    <span class="k">def</span> <span class="nf">after_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#Save the extra outputs for later and only returns the true output (decoded into words).
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">raw_out</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pred</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="bp">self</span><span class="p">.</span><span class="n">pred</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">after_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#AR and TAR
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">α</span> <span class="o">!=</span> <span class="mf">0.</span><span class="p">:</span>  <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">α</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nb">float</span><span class="p">().</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">β</span> <span class="o">!=</span> <span class="mf">0.</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">raw_out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">h</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">β</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">h</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nb">float</span><span class="p">().</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
                
    <span class="k">def</span> <span class="nf">begin_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#Shuffle the texts at the beginning of the epoch
</span>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dl</span><span class="p">.</span><span class="n">dataset</span><span class="p">,</span> <span class="s">"batchify"</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">dl</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">batchify</span><span class="p">()</span>
</code></pre></div></div>

<p>We set up our loss function normal <code class="language-plaintext highlighter-rouge">cross_entropy</code> and a accuracy metrics. We just need to make sure that then batch and sequence dimensions are all flattened into one dimension:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cross_entropy_flat</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">bs</span><span class="p">,</span><span class="n">sl</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span> <span class="o">*</span> <span class="n">sl</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span> <span class="o">*</span> <span class="n">sl</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">accuracy_flat</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">bs</span><span class="p">,</span><span class="n">sl</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">accuracy</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span> <span class="o">*</span> <span class="n">sl</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span> <span class="o">*</span> <span class="n">sl</span><span class="p">))</span>
</code></pre></div></div>

<p>Now we are ready to go:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emb_sz</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="n">nl</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_language_model</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="n">nl</span><span class="p">,</span> <span class="n">tok_pad</span><span class="p">,</span>
                           <span class="n">input_p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">output_p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                           <span class="n">embed_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hidden_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>


<span class="n">cbs</span> <span class="o">=</span> <span class="p">[</span><span class="n">partial</span><span class="p">(</span><span class="n">AvgStatsCallback</span><span class="p">,</span><span class="n">accuracy_flat</span><span class="p">),</span>
       <span class="n">CudaCallback</span><span class="p">,</span> <span class="n">Recorder</span><span class="p">,</span>
       <span class="n">partial</span><span class="p">(</span><span class="n">GradientClipping</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
       <span class="n">partial</span><span class="p">(</span><span class="n">RNNTrainer</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">β</span><span class="o">=</span><span class="mf">1.</span><span class="p">),</span>
       <span class="n">ProgressCallback</span><span class="p">]</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">cross_entropy_flat</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-3</span><span class="p">,</span>
                <span class="n">cb_funcs</span><span class="o">=</span><span class="n">cbs</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">adam_opt</span><span class="p">())</span>

<span class="n">learn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="ulmfit">
<a class="anchor" href="#ulmfit" aria-hidden="true"><span class="octicon octicon-link"></span></a>ULMFiT</h3>

<p><a href="https://youtu.be/vnOpEwmtFJ8?t=7408"><em>(Jump to Lesson 12 video)</em></a></p>

<h4 id="pretraining-wikitext">
<a class="anchor" href="#pretraining-wikitext" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pretraining (Wikitext)</h4>

<p><em>(Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb">12b_lm_pretrain.ipynb</a>)</em></p>

<p>We now have a language model trainer using AWD-LSTM. We can now use what we have above to train a language model using Wikitext. This is covered in the course notebook.</p>

<p>Here’s how you can download WikiText103:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-<span class="o">{</span>version<span class="o">}</span><span class="nt">-v1</span>.zip <span class="nt">-P</span> <span class="o">{</span>path<span class="o">}</span>
unzip <span class="nt">-q</span> <span class="nt">-n</span> <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span><span class="nt">-v1</span>.zip  <span class="nt">-d</span> <span class="o">{</span>path<span class="o">}</span>
<span class="nb">mv</span> <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span>/wiki.train.tokens <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span>/train.txt
<span class="nb">mv</span> <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span>/wiki.valid.tokens <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span>/valid.txt
<span class="nb">mv</span> <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span>/wiki.test.tokens <span class="o">{</span>path<span class="o">}</span>/wikitext-<span class="o">{</span>version<span class="o">}</span>/test.txt
</code></pre></div></div>

<p>Split it into articles:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">istitle</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="s">r'^ = [^=]* = $'</span><span class="p">,</span> <span class="n">line</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">read_wiki</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">articles</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">current_article</span> <span class="o">=</span> <span class="s">''</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
        <span class="n">current_article</span> <span class="o">+=</span> <span class="n">line</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">lines</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s">' </span><span class="se">\n</span><span class="s">'</span> <span class="ow">and</span> <span class="n">istitle</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]):</span>
            <span class="n">current_article</span> <span class="o">=</span> <span class="n">current_article</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'&lt;unk&gt;'</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span>
            <span class="n">articles</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_article</span><span class="p">)</span>
            <span class="n">current_article</span> <span class="o">=</span> <span class="s">''</span>
    <span class="n">current_article</span> <span class="o">=</span> <span class="n">current_article</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'&lt;unk&gt;'</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span>
    <span class="n">articles</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_article</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">articles</span>
</code></pre></div></div>

<p>Create training and validation datasets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span> <span class="o">=</span> <span class="n">TextList</span><span class="p">(</span><span class="n">read_wiki</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">'train.txt'</span><span class="p">),</span> <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span> 
<span class="n">valid</span> <span class="o">=</span> <span class="n">TextList</span><span class="p">(</span><span class="n">read_wiki</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">'valid.txt'</span><span class="p">),</span> <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">SplitData</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">valid</span><span class="p">)</span>
</code></pre></div></div>

<p>Then, as before, tokenize and numericalize, then <code class="language-plaintext highlighter-rouge">databunchify</code>. You can then train it on a GPU for about <em>5 hours</em>. Language models take a <em>long time to train</em>, but luckily we only need to do it once and then we can reuse that model for much faster fine-tuning.</p>

<h4 id="finetuning-imdb">
<a class="anchor" href="#finetuning-imdb" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finetuning (IMDb)</h4>

<p><em>(Notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb">12c_ulmfit.ipynb</a>)</em></p>

<p>You can download a small model pretrained on <a href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">wikitext 103</a> using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget http://files.fast.ai/models/wt103_tiny.tgz <span class="nt">-P</span> <span class="o">{</span>path<span class="o">}</span>
<span class="nb">tar </span>xf <span class="o">{</span>path<span class="o">}</span>/wt103_tiny.tgz <span class="nt">-C</span> <span class="o">{</span>path<span class="o">}</span>
</code></pre></div></div>

<p>This language model can be used to <em>fine-tune</em> on other NLP tasks. For example the IMDb dataset. If we now create the dataloaders for IMDb, like before, it will will create a vocabulary based on that data alone. <em>This won’t be the same vocabulary as wikitext language model.</em> There will be different tokens and tokens will be numericalized differently.</p>

<p>We somehow need to match our pretrained weights to the new  vocabulary. This is done on the embeddings and the decoder (since the  weights between embeddings and decoders are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order.</p>

<p>We just go through each vocab item in the IMDb vocab, find out if it is in the Wikitext103 vocab, and if it is we copy Wikitext103’s embedding over. Anytime there is a word in IMDb and NOT in Wikitext103 we just set its embedding term to the mean bias and mean weight.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">house_wgt</span>  <span class="o">=</span> <span class="n">old_wgts</span><span class="p">[</span><span class="s">'0.emb.weight'</span><span class="p">][</span><span class="n">idx_house_old</span><span class="p">]</span>
<span class="n">house_bias</span> <span class="o">=</span> <span class="n">old_wgts</span><span class="p">[</span><span class="s">'1.decoder.bias'</span><span class="p">][</span><span class="n">idx_house_old</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">match_embeds</span><span class="p">(</span><span class="n">old_wgts</span><span class="p">,</span> <span class="n">old_vocab</span><span class="p">,</span> <span class="n">new_vocab</span><span class="p">):</span>
    <span class="n">wgts</span> <span class="o">=</span> <span class="n">old_wgts</span><span class="p">[</span><span class="s">'0.emb.weight'</span><span class="p">]</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">old_wgts</span><span class="p">[</span><span class="s">'1.decoder.bias'</span><span class="p">]</span>
    <span class="n">wgts_m</span><span class="p">,</span><span class="n">bias_m</span> <span class="o">=</span> <span class="n">wgts</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span><span class="n">bias</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">new_wgts</span> <span class="o">=</span> <span class="n">wgts</span><span class="p">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_vocab</span><span class="p">),</span> <span class="n">wgts</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">new_bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_vocab</span><span class="p">))</span>
    <span class="n">otoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">old_vocab</span><span class="p">)}</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">new_vocab</span><span class="p">):</span> 
        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">otoi</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">otoi</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="n">new_wgts</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">new_bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">wgts</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">bias</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span> <span class="n">new_wgts</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">new_bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">wgts_m</span><span class="p">,</span><span class="n">bias_m</span>
    <span class="n">old_wgts</span><span class="p">[</span><span class="s">'0.emb.weight'</span><span class="p">]</span>    <span class="o">=</span> <span class="n">new_wgts</span>
    <span class="n">old_wgts</span><span class="p">[</span><span class="s">'0.emb_dp.emb.weight'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_wgts</span>
    <span class="n">old_wgts</span><span class="p">[</span><span class="s">'1.decoder.weight'</span><span class="p">]</span>    <span class="o">=</span> <span class="n">new_wgts</span>
    <span class="n">old_wgts</span><span class="p">[</span><span class="s">'1.decoder.bias'</span><span class="p">]</span>      <span class="o">=</span> <span class="n">new_bias</span>
    <span class="k">return</span> <span class="n">old_wgts</span>
</code></pre></div></div>

<p>We then repeat the same process as before to train a fine-tuned language model for IMdB.</p>

<h4 id="classification-imdb">
<a class="anchor" href="#classification-imdb" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification (IMDb)</h4>

<p><em><a href="https://youtu.be/vnOpEwmtFJ8?t=7554">(Jump_to lesson 12 video)</a></em></p>

<p>Now we can tackle the IMDb classifier task - positive/negative reviews. We load the data the same way as before, but using the language model <code class="language-plaintext highlighter-rouge">vocab</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proc_tok</span><span class="p">,</span><span class="n">proc_num</span><span class="p">,</span><span class="n">proc_cat</span> <span class="o">=</span> <span class="n">TokenizeProcessor</span><span class="p">(),</span><span class="n">NumericalizeProcessor</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">),</span><span class="n">CategoryProcessor</span><span class="p">()</span>

<span class="n">il</span> <span class="o">=</span> <span class="n">TextList</span><span class="p">.</span><span class="n">from_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">])</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">SplitData</span><span class="p">.</span><span class="n">split_by_func</span><span class="p">(</span><span class="n">il</span><span class="p">,</span> <span class="n">partial</span><span class="p">(</span><span class="n">grandparent_splitter</span><span class="p">,</span> <span class="n">valid_name</span><span class="o">=</span><span class="s">'test'</span><span class="p">))</span>
<span class="n">ll</span> <span class="o">=</span> <span class="n">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">,</span> <span class="n">parent_labeler</span><span class="p">,</span> <span class="n">proc_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">proc_tok</span><span class="p">,</span> <span class="n">proc_num</span><span class="p">],</span> <span class="n">proc_y</span><span class="o">=</span><span class="n">proc_cat</span><span class="p">)</span>

<span class="n">bs</span><span class="p">,</span><span class="n">bptt</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span><span class="mi">70</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">clas_databunchify</span><span class="p">(</span><span class="n">ll</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
</code></pre></div></div>

<p>We again use AWD-LSTM. Reminder: its forward pass outputs: <code class="language-plaintext highlighter-rouge">raw_outputs, outputs, mask</code>. (Where <code class="language-plaintext highlighter-rouge">mask</code> is the mask for the padding tokens).</p>

<p><strong>Concat Pooling:</strong> We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. (Also ignore the  padding in the last element/average/maximum). We concatenate all these together into one vector:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Pooling</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">raw_outputs</span><span class="p">,</span><span class="n">outputs</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mask</span><span class="p">.</span><span class="nb">long</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">[:,:,</span><span class="bp">None</span><span class="p">],</span> <span class="mi">0</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">avg_pool</span><span class="p">.</span><span class="n">div_</span><span class="p">(</span><span class="n">lengths</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">avg_pool</span><span class="p">.</span><span class="n">dtype</span><span class="p">)[:,</span><span class="bp">None</span><span class="p">])</span>
        <span class="n">max_pool</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">[:,:,</span><span class="bp">None</span><span class="p">],</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)).</span><span class="nb">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">output</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span><span class="n">lengths</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">max_pool</span><span class="p">,</span> <span class="n">avg_pool</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#Concat pooling.
</span>        <span class="k">return</span> <span class="n">output</span><span class="p">,</span><span class="n">x</span>
</code></pre></div></div>

<p><em>People used to just used the final activation. ULMFiT uses all the activations between the layers and it works better.</em></p>

<p>This gives us the <strong>PoolingLinearClassifier</strong>, which is just a sequence of batchnorm dropout linear layers that consumes the output of Concat Pooling. This will output a vector that is <code class="language-plaintext highlighter-rouge">(batch, n_classes)</code> sized, which can be then  passed to a loss function (e.g. <code class="language-plaintext highlighter-rouge">cross_entropy</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bn_drop_lin</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">actn</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_in</span><span class="p">)]</span> <span class="k">if</span> <span class="n">bn</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span> <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">actn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">actn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span>
  

<span class="k">class</span> <span class="nc">PoolingLinearClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"Create a linear classifier with pooling."</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">drops</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">mod_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">activs</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">actn</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">drops</span><span class="p">,</span> <span class="n">activs</span><span class="p">):</span>
            <span class="n">mod_layers</span> <span class="o">+=</span> <span class="n">bn_drop_lin</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">actn</span><span class="o">=</span><span class="n">actn</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">mod_layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">raw_outputs</span><span class="p">,</span><span class="n">outputs</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mask</span><span class="p">.</span><span class="nb">long</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">[:,:,</span><span class="bp">None</span><span class="p">],</span> <span class="mi">0</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">avg_pool</span><span class="p">.</span><span class="n">div_</span><span class="p">(</span><span class="n">lengths</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">avg_pool</span><span class="p">.</span><span class="n">dtype</span><span class="p">)[:,</span><span class="bp">None</span><span class="p">])</span>
        <span class="n">max_pool</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">[:,:,</span><span class="bp">None</span><span class="p">],</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)).</span><span class="nb">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">output</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span><span class="n">lengths</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">max_pool</span><span class="p">,</span> <span class="n">avg_pool</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#Concat pooling.
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>Now we just need to run the LSTM through the IMDb reviews one <code class="language-plaintext highlighter-rouge">bptt</code> at a time, recording the <code class="language-plaintext highlighter-rouge">raw_output</code>, <code class="language-plaintext highlighter-rouge">output</code>, and <code class="language-plaintext highlighter-rouge">mask</code> as it goes. (These all will go into the <code class="language-plaintext highlighter-rouge">PoolingLinearClassifier</code> afterwards, whose subsequent output will go into a <code class="language-plaintext highlighter-rouge">cross_entropy</code> loss). The code for this is the <strong>Sentence Encoder:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SentenceEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">bptt</span><span class="p">,</span> <span class="n">pad_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="n">bptt</span><span class="p">,</span><span class="n">module</span><span class="p">,</span><span class="n">pad_idx</span>

    <span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arrs</span><span class="p">,</span> <span class="n">bs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pad_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">si</span><span class="p">],</span><span class="n">bs</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">arrs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">si</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">sl</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">bs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">raw_outputs</span><span class="p">,</span><span class="n">outputs</span><span class="p">,</span><span class="n">masks</span> <span class="o">=</span> <span class="p">[],[],[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="p">):</span>
            <span class="n">r</span><span class="p">,</span><span class="n">o</span><span class="p">,</span><span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">[:,</span><span class="n">i</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bptt</span><span class="p">,</span> <span class="n">sl</span><span class="p">)])</span>
            <span class="n">masks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pad_tensor</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">raw_outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">raw_outputs</span><span class="p">,</span> <span class="n">bs</span><span class="p">),</span><span class="bp">self</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">bs</span><span class="p">),</span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Here is the <strong>Full AWD-LSTM Classifier</strong> for IMDb:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_text_classifier</span><span class="p">(</span><span class="n">vocab_sz</span><span class="p">,</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">n_hid</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">pad_token</span><span class="p">,</span> <span class="n">bptt</span><span class="p">,</span> <span class="n">output_p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hidden_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                        <span class="n">input_p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">embed_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">drops</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"To create a full AWD-LSTM"</span>
    <span class="n">rnn_enc</span> <span class="o">=</span> <span class="n">AWD_LSTM1</span><span class="p">(</span><span class="n">vocab_sz</span><span class="p">,</span> <span class="n">emb_sz</span><span class="p">,</span> <span class="n">n_hid</span><span class="o">=</span><span class="n">n_hid</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
                        <span class="n">hidden_p</span><span class="o">=</span><span class="n">hidden_p</span><span class="p">,</span> <span class="n">input_p</span><span class="o">=</span><span class="n">input_p</span><span class="p">,</span> <span class="n">embed_p</span><span class="o">=</span><span class="n">embed_p</span><span class="p">,</span> <span class="n">weight_p</span><span class="o">=</span><span class="n">weight_p</span><span class="p">)</span>
    <span class="n">enc</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">(</span><span class="n">rnn_enc</span><span class="p">,</span> <span class="n">bptt</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">drops</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  <span class="n">drops</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">emb_sz</span><span class="p">]</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">+</span> <span class="p">[</span><span class="n">n_out</span><span class="p">]</span> 
    <span class="n">drops</span> <span class="o">=</span> <span class="p">[</span><span class="n">output_p</span><span class="p">]</span> <span class="o">+</span> <span class="n">drops</span>
    <span class="k">return</span> <span class="n">SequentialRNN</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">PoolingLinearClassifier</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">drops</span><span class="p">))</span>
</code></pre></div></div>

<p>To do <strong>transfer learning</strong> with the pretrained weight we follow the same procedure we saw before with ResNet. We load the fine-tuned weights into the model with <code class="language-plaintext highlighter-rouge">module.load_state_dict</code> method from PyTorch. Then we freeze that part of the model and initially train just the head (the <code class="language-plaintext highlighter-rouge">PoolingLinearClassifier</code>). We then unfreeze the whole model and continue to train that for a few more epochs.</p>

<p>In the end this gives 92% on IMDb, which was SOTA a few years ago.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<p>There are two more lessons left in FastAI v3:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=3TqN_M1L4ts&amp;t=834s">Lesson 13 (2019) - Basics of Swift for Deep Learning</a></li>
  <li><a href="https://www.youtube.com/watch?v=8wd8zFzTG38">Lesson 14 (2019) - Swift: C interop; Protocols; Putting it all together</a></li>
</ul>

<p>I don’t plan on producing extensive notes on these two lectures since are focussed on using Swift for deep learning with <em>Swift for Tensorflow</em>. There is excellent discussion on the weaknesses of Python for deep learning and the space for disruption by a better language like Swift or Julia. While this is quite an interesting topic, it’s not so useful for me to invest my energy writing about this at this time (each of these notes was hard work!), and I want to move onto other projects. These lectures are also very code heavy and provide a coding introduction to Swift. It wouldn’t be worth reproducing that here when there are myriad better tutorials available elsewhere. I’m also conscious that it’s over a year since these lectures were published, and a year hence it’s not clear to me whether Swift for Tensorflow has gained much momentum in the community. Julia, in the meantime, seems like it might be a more promising rival to Python.</p>

<h2 id="q--a">
<a class="anchor" href="#q--a" aria-hidden="true"><span class="octicon octicon-link"></span></a>Q &amp; A</h2>

<ul>
  <li>
    <p><em>What do you mean by keeping a scientific journal?</em></p>

    <blockquote>
      <p>When you look at the great scientists in history they all had great scientific journal practices.</p>

      <p>In my case it’s a piece of software called Windows Notepad, and I paste things into it at the bottom, and when I want to find something I press Ctrl-F. It just needs to be something that has a record of what you’re doing and what the results are. Because scientists that make the break throughs generally make the break-throughs because they look at something that shouldn’t be and they say “Oh! That’s odd! What’s going on?”</p>

      <p>For example, the discovery of the noble gases was because a scientist saw one little bubble left in the beaker, and they were pretty sure there shouldn’t be a bubble there. Most people would have just ignored it, but they studied the bubble and discovered the noble gases.</p>

      <p>Another example is penicilin.</p>

      <p>I find that in deep learning this is true as well. I spent a lot of time studying batch normalization and transfer learning because a few years ago in Keras I was getting terrible transfer learning results for something I thought should be much more accurate. I thought - “Oh, that’s odd.” - and I spent weeks changing everything I could, and then, almost randomly, tried changing batch norm.</p>

      <p>All this fiddling around - 90% doesn’t lead anywhere, but it’s the other 10% that you won’t be able to figure out, unless you can go back confirm that the result of an experiment really did happen.</p>

      <p>You can record the dates, github commits, logs, command lines, whatever to ensure that you could go back and reproduce the experiment later on.</p>
    </blockquote>
  </li>
  <li>
    <p><em>Why are you against using cross-validation for deep learning?</em></p>

    <blockquote>
      <p>Cross-validation is a very useful technique for getting a reasonably sized validation set if you don’t have enough data to otherwise create a reasonably sized validation set.</p>

      <p>It was particularly popular in the days when most studies were say 50-60 rows. If you have a few 1000 rows it’s just pointless - the statistical significance is going to be there regardless. I’m not against it, it’s just most of the time you don’t need it. Because if you have a 1000 things in the validation set and you only care whether it’s plus or minus 1% it’s totally pointless.</p>

      <p>Have a look to see how much your validation set accuracy is varying from run to run and if it’s too much that you can’t make the decisions you need to make, then you can add cross-validation.</p>
    </blockquote>
  </li>
  <li>
    <p><em>What are you best tips for debugging deep learning?</em></p>

    <blockquote>
      <p>Don’t make mistakes in the first place(!)</p>

      <p>The only way to do that is to make your code so simple that it can’t possibly have a mistake and check every single intermediate result along the way to make sure it doesn’t have a mistake.</p>

      <p>Otherwise you could end up spending a month of your time like I did last month. A month ago I got 94.1% accuracy on ImageNet. Then I started trying various tweaks, but none of the tweaks seemed to help. As a sanity check I decided to repeat the previous training and I couldn’t repeat it - I was getting 93.5%. (Every training run took 6 hours cost me $150 on AWS!).</p>

      <p>So it’s a big process to even realise that it’s broken. When you’ve written deep learning code wrong, it gets broken in ways you don’t even notice.</p>

      <p>You need to be a great scientist to do deep learning - keep excellent journals of your results. I could go back to my journal to see when I got the 94.1% result, so I could revert <code class="language-plaintext highlighter-rouge">fastai</code> back to a commit of that time and reran and successfully reproduce the result.</p>

      <p>I could then bisect the changes made to <code class="language-plaintext highlighter-rouge">fastai</code> in the meantime until I found the bug in the mixed-precision module. The bug was subtle and didn’t show up until epoch 50!  Finding it cost $5000…</p>

      <p>The tiny difference is so insignificant that noone using fastai noticed the error and it was only noticible when trying to get a SOTA imagenet result.</p>

      <p>These types of ‘soft-bugs’ are common in deep learning, really hard to detect and tedious to track down!!</p>
    </blockquote>
  </li>
  <li>
    <p><em>In NLP, do you do any other preprocessing such as removing stop words, stemming, or lemmatization?</em></p>

    <blockquote>
      <p>In traditional NLP those are importanting things to do. Stop words are things like: ‘a’, ‘on’, ‘the’. Stemming is getting rid of the ‘ing’ suffix and stuff like that. These are pretty universal in NLP… It’s an absolutely terrible idea. Never do this.</p>

      <p>Why would you remove information from your neural net that might be useful? It is useful. Your use of stop words tells you a lot about your style of language. For example, you’ll often have a lot fewer articles if you are really angry and speaking quickly. The tense you are talking about is important, and stemming gets rid of it.</p>

      <p>All these kinds of preprocessors are in the past. In general, preprocessing data in neural nets - the rule of thumb is to leave it as raw as you can.</p>
    </blockquote>
  </li>
</ul>

<h2 id="links-and-references">
<a class="anchor" href="#links-and-references" aria-hidden="true"><span class="octicon octicon-link"></span></a>Links and References</h2>

<ul>
  <li><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&amp;feature=youtu.be">Lesson 12 video lecture</a></li>
  <li><a href="https://paperswithcode.com/method/label-smoothing">Label Smoothing [paperswithcode/methods]</a></li>
  <li><a href="https://developer.nvidia.com/blog/video-mixed-precision-techniques-tensor-cores-deep-learning/">Mixed Precision Training [NVIDIA Blog]</a></li>
  <li>A great explanation of <a href="https://www.youtube.com/watch?v=PZRI1IfStY0">How floats work [YouTube]</a>.</li>
  <li>This video works through <a href="https://www.youtube.com/watch?v=Pox8LzIHhR4">Adding two floats at the bit level [YouTube]</a>
</li>
  <li><a href="https://machinelearningmastery.com/gentle-introduction-backpropagation-time/">Backpropagation Through Time Blog post [Machine Learning Mastery]</a></li>
  <li>
<a href="https://forums.fast.ai/t/lesson-12-2019-discussion-and-wiki/44009/257">FastAI forum post by Stefano Giomo</a> the demystifies the difference between sequence length, BPTT, and batch size in RNNs.</li>
  <li>RNN Refresher: <a href="http://joshvarty.github.io/VisualizingRNNs/">Visualizing RNNs [Josh Varty]</a>
</li>
  <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks [Chris Olah]</a></li>
  <li><a href="https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b">Understanding building blocks of ULMFiT [Kerem Turgutlu; Medium]</a></li>
  <li>Lesson Notebooks:
    <ul>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/10_augmentation.ipynb">10_augmentation.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/10b_mixup_label_smoothing.ipynb">10b_mixup_label_smoothing.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/10c_fp16.ipynb">10c_fp16.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/11_train_imagenette.ipynb">11_train_imagenette.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/11a_transfer_learning.ipynb">11a_transfer_learning.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb">12_text.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb">12b_lm_pretrain.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb">12c_ulmfit.ipynb</a></li>
    </ul>
  </li>
  <li>Papers:
    <ul>
      <li>Fast Image Augmentation: <a href="https://arxiv.org/abs/1710.09412">Mixup paper [2017]</a>, <a href="https://arxiv.org/abs/1708.04552">CutOut paper [2017]</a>, <a href="https://arxiv.org/abs/1905.04899">CutMix [2019]</a>
</li>
      <li><a href="https://arxiv.org/abs/1906.02629">When Does Label Smoothing Help? [2019]</a></li>
      <li><a href="https://arxiv.org/abs/1512.03385">Original ResNet paper [2015]</a></li>
      <li><a href="https://arxiv.org/abs/1812.01187?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529">Bag of Tricks paper [2019]</a></li>
      <li>AWD-LSTM - <a href="https://arxiv.org/abs/1708.02182">Regularizing and Optimizing LSTM Language Models, Merity et al. [2017]</a>
</li>
    </ul>
  </li>
</ul>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jimypbr/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/10/25/fast-ai-v3-lesson-12-notes-advanced-training-techniques-ulmfit-from-scratch.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jim Briggs&#39; blog about ML, software, etc</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
