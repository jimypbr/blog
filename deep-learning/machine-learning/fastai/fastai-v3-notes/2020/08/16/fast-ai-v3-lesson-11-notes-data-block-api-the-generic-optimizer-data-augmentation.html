<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fast.ai v3 Lesson 11 Notes: Data Block API, the Generic Optimizer, Data Augmentation | go-seq</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Fast.ai v3 Lesson 11 Notes: Data Block API, the Generic Optimizer, Data Augmentation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My personal notes on Lesson 11 of part 2 of fast.ai v3 - Data Block API, the Generic Optimzier, and Data Augmentation" />
<meta property="og:description" content="My personal notes on Lesson 11 of part 2 of fast.ai v3 - Data Block API, the Generic Optimzier, and Data Augmentation" />
<link rel="canonical" href="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html" />
<meta property="og:url" content="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html" />
<meta property="og:site_name" content="go-seq" />
<meta property="og:image" content="https://jimypbr.github.io/blog/images/fastai/image-20190706182251357.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-16T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html","@type":"BlogPosting","headline":"Fast.ai v3 Lesson 11 Notes: Data Block API, the Generic Optimizer, Data Augmentation","dateModified":"2020-08-16T00:00:00-05:00","datePublished":"2020-08-16T00:00:00-05:00","image":"https://jimypbr.github.io/blog/images/fastai/image-20190706182251357.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html"},"description":"My personal notes on Lesson 11 of part 2 of fast.ai v3 - Data Block API, the Generic Optimzier, and Data Augmentation","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimypbr.github.io/blog/feed.xml" title="go-seq" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       processEscapes: true
     }
   });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">go-seq</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fast.ai v3 Lesson 11 Notes: Data Block API, the Generic Optimizer, Data Augmentation</h1><p class="page-description">My personal notes on Lesson 11 of part 2 of fast.ai v3 - Data Block API, the Generic Optimzier, and Data Augmentation</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-16T00:00:00-05:00" itemprop="datePublished">
        Aug 16, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      48 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine-learning">machine-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai-v3-notes">fastai-v3-notes</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#overview">Overview</a></li>
<li class="toc-entry toc-h2"><a href="#layer-wise-sequential-unit-variance-lsuv">Layer-wise Sequential Unit-Variance (LSUV)</a></li>
<li class="toc-entry toc-h2"><a href="#imagenette-dataset">Imagenette Dataset</a></li>
<li class="toc-entry toc-h2"><a href="#data-block-api-foundations">Data Block API Foundations</a>
<ul>
<li class="toc-entry toc-h3"><a href="#datablock-api-motivation">DataBlock API Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#step-1---imagelist">Step 1 - ImageList</a></li>
<li class="toc-entry toc-h3"><a href="#step-2---split-validation-set">Step 2 - Split Validation Set</a></li>
<li class="toc-entry toc-h3"><a href="#step-3---labelling">Step 3 - Labelling</a></li>
<li class="toc-entry toc-h3"><a href="#step-4---databunch">Step 4 - DataBunch</a></li>
<li class="toc-entry toc-h3"><a href="#all-the-steps">All the steps</a></li>
<li class="toc-entry toc-h3"><a href="#new-cnn-model">New CNN Model</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#universal-optimizer">Universal Optimizer</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-optimizer-class">The Optimizer Class</a></li>
<li class="toc-entry toc-h3"><a href="#weight-decay">Weight Decay</a></li>
<li class="toc-entry toc-h3"><a href="#momentum">Momentum</a>
<ul>
<li class="toc-entry toc-h4"><a href="#aside-pythons-wonderful-kwargs">Aside: Python’s Wonderful kwargs</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#weight-decay--batch-norm-a-surprising-result">Weight Decay + Batch Norm: A Surprising Result</a></li>
<li class="toc-entry toc-h3"><a href="#momentum-experiments">Momentum Experiments</a></li>
<li class="toc-entry toc-h3"><a href="#adam-algorithm">Adam Algorithm</a></li>
<li class="toc-entry toc-h3"><a href="#lamb-algorithm">LAMB Algorithm</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#data-augmentation">Data Augmentation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#resizing">Resizing</a></li>
<li class="toc-entry toc-h3"><a href="#flipping-rotating-cropping">Flipping, Rotating, Cropping</a></li>
<li class="toc-entry toc-h3"><a href="#perspective-transform">Perspective Transform</a></li>
<li class="toc-entry toc-h3"><a href="#batch-data-augmentation">Batch Data Augmentation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#1-generate-the-grid">1. Generate the Grid</a></li>
<li class="toc-entry toc-h4"><a href="#step-2-affine-multiplication">Step 2: Affine Multiplication</a></li>
<li class="toc-entry toc-h4"><a href="#step-3-interpolate">Step 3: Interpolate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#links-and-references">Links and References</a></li>
</ul><h2 id="overview">
<a class="anchor" href="#overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h2>

<p>This lesson starts with introducing a simple initialization technique called <em>Layer-wise Sequential Unit Variance (LSUV)</em>. This technique iteratively sets the weights or each layer in your model so their outputs are normally distributed, without needing to derive any fiddly formulae for each different activation you are using.</p>

<p>Next the lesson shows how to implement fastai’s <em>Data Block API</em>.</p>

<p>After that, the lesson gets into optimization. It implements Optimizer and StatefulOptimizer and shows that nearly all optimizers used in modern deep learning training are just special cases of these classes. They use it to add weight decay, momentum, Adam, and LAMB optimizers.</p>

<p>Finally, the lesson looks at data augmentation, specifically for images. It shows that data augmentation can also be done on the GPU, which speeds things up quite dramatically.</p>

<p><a href="https://youtu.be/hPQKzsjTyyQ"><em>Link to the lesson 11 video</em></a></p>

<ul>
  <li><em>(Update 2020-08-22: fixed typos, fixed incorrect explanation of compose function, expanded on AdamW, included trust ratio in LAMB.)</em></li>
</ul>

<h2 id="layer-wise-sequential-unit-variance-lsuv">
<a class="anchor" href="#layer-wise-sequential-unit-variance-lsuv" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layer-wise Sequential Unit-Variance (LSUV)</h2>

<p>It’s really fiddly to get unit variances throughout the layers of the network.  If you change one thing like your activation function or the add dropout, change the amount of dropout then you’d have to alter the initialization again to adjust for this. If the variance of a layer is just a little bit different to 1, then it will get exponentially worse in the subsequent layers.  You would need to analytically workout how to reinitialize things.</p>

<p>There is a better way. In the paper <a href="https://arxiv.org/abs/1511.06422">All you need is a good init [2015]</a> - the authors created a way to let the computer figure out how to reinitialize everything. This technique is called <strong><em>Layer-wise Sequential Unit-Variance (LSUV)</em>.</strong></p>

<p>The algorithm is very simple:</p>

<ul>
  <li>
    <p>Loop through every layer <code class="language-plaintext highlighter-rouge">l</code> in the network</p>

    <ul>
      <li>While stdev of layer’s output <code class="language-plaintext highlighter-rouge">h.std()</code> is not approximately 1.0:
        <ul>
          <li>Do a forward pass with a mini-batch</li>
          <li>Get the layer’s output tensor: <code class="language-plaintext highlighter-rouge">h</code>
</li>
          <li>Update the layer’s weights: <code class="language-plaintext highlighter-rouge">W_l = W_l / Var(h).sqrt()</code>
</li>
        </ul>
      </li>
      <li>While the mean of the layer’s output: <code class="language-plaintext highlighter-rouge">h.mean()​</code> is not approximately 0.0:
        <ul>
          <li>Do a forward pass with a mini-batch</li>
          <li>Get the layer’s output tensor: <code class="language-plaintext highlighter-rouge">h</code>
</li>
          <li>Update the layer’s bias: <code class="language-plaintext highlighter-rouge">bias_l = bias_l - h.mean()</code>
</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Here is the PyTorch code to do LSUV using PyTorch hooks to record the statistics of the target module in the model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">cond</span><span class="p">):</span>
    <span class="c1"># recursively walk through the layers in pytorch model
</span>    <span class="c1"># returning a list of all that satisfy `cond`
</span>    <span class="k">if</span> <span class="n">cond</span><span class="p">(</span><span class="n">m</span><span class="p">):</span> <span class="k">return</span> <span class="p">[</span><span class="n">m</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">find_modules</span><span class="p">(</span><span class="n">o</span><span class="p">,</span><span class="n">cond</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">m</span><span class="p">.</span><span class="n">children</span><span class="p">()],</span> <span class="p">[])</span>
  

<span class="k">def</span> <span class="nf">lsuv_module</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">mdl</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">Hook</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">append_stat</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">mdl</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">h</span><span class="p">.</span><span class="n">std</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-3</span><span class="p">:</span> <span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">h</span><span class="p">.</span><span class="n">std</span>
    <span class="k">while</span> <span class="n">mdl</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">h</span><span class="p">.</span><span class="n">mean</span><span class="p">)</span>  <span class="o">&gt;</span> <span class="mf">1e-3</span><span class="p">:</span> <span class="n">m</span><span class="p">.</span><span class="n">bias</span> <span class="o">-=</span> <span class="n">h</span><span class="p">.</span><span class="n">mean</span>

    <span class="n">h</span><span class="p">.</span><span class="n">remove</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">h</span><span class="p">.</span><span class="n">mean</span><span class="p">,</span><span class="n">h</span><span class="p">.</span><span class="n">std</span>
  

<span class="n">mdl</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">mods</span> <span class="o">=</span> <span class="n">find_modules</span><span class="p">(</span><span class="n">learn</span><span class="p">.</span><span class="n">model</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span><span class="n">ConvLayer</span><span class="p">))</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mods</span><span class="p">:</span> 
    <span class="k">print</span><span class="p">(</span><span class="n">lsuv_module</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">mdl</span><span class="p">))</span>

<span class="c1">## output:
## (2.1287371865241766e-08, 1.0)
## (2.5848953200124924e-08, 1.0)
## (-5.820766091346741e-10, 0.9999999403953552)
## (-2.6775524020195007e-08, 1.0)
## (2.2351741790771484e-08, 1.0)
</span></code></pre></div></div>

<p>Let’s visualize the layers with the histograms like we did in the last lesson.</p>

<p><strong>No LSUV, normal init:</strong> the histograms and proportions of non-zeros of the weights over time during training:</p>

<p><img src="/blog/images/fastai/Wed,%2001%20Apr%202020%20193929.png" alt="img"></p>

<p><img src="/blog/images/fastai/Wed,%2001%20Apr%202020%20194537.png" alt="img"></p>

<p><strong>With LSUV:</strong> the histograms and proportions of non-zeros of the weights over time during training:</p>

<p><img src="/blog/images/fastai/Wed,%2001%20Apr%202020%20200007.png" alt="img"></p>

<p><img src="/blog/images/fastai/Wed,%2001%20Apr%202020%20200038.png" alt="img"></p>

<p>LSUV is something you run on all the layers at the beginning before starting training. You can also take more than one mini-batch.</p>

<p>Links, interesting forum posts:</p>

<ul>
  <li>
    <p><a href="https://forums.fast.ai/t/implementing-the-empirical-initialization-from-all-you-need-is-a-good-init/42284">https://forums.fast.ai/t/implementing-the-empirical-initialization-from-all-you-need-is-a-good-init/42284</a></p>
  </li>
  <li>
    <p><a href="https://forums.fast.ai/t/lsuv-improvement/49571">https://forums.fast.ai/t/lsuv-improvement/49571</a></p>
  </li>
</ul>

<h2 id="imagenette-dataset">
<a class="anchor" href="#imagenette-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Imagenette Dataset</h2>

<p><a href="https://youtu.be/hPQKzsjTyyQ?t=576"><em>(Jump to lesson 11 video)</em></a></p>

<p>We are getting great results very fast on MNIST. It’s time to put away MNIST and try a dataset that’s a bit harder. We aren’t quite ready to take on the <a href="http://www.image-net.org/">ImageNet</a> dataset, because ImageNet is very large and takes several days to train on one GPU. We need something that has a faster feedback loop than that for practising, learning, or researching.</p>

<p>Another image dataset is <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>, but this one consists of 32x32 images. It turns out that small images have very different characteristics to large images. Under 96x96 stuff behaves very differently. So stuff that works well on CIFAR-10, tends not to work well on larger images.</p>

<p>The same authors of the ‘All you need is a good init’ paper, Dmytro Mishkin et al., showed in another paper <a href="https://arxiv.org/abs/1606.02228">‘Systematic evaluation of CNN advances on the ImageNet [2017]’</a> that:</p>

<blockquote>
  <p>… the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.</p>
</blockquote>

<p>If we do experiments with a dataset of 128x128 sized images the results will be applicable to larger images and we can save heap of time too. But simply resizing Imagenet down to 128x128 still takes too long because there are loads of different classes.</p>

<p>To fill this gap in the market, Jeremy has created <a href="https://github.com/fastai/imagenette">Imagenette</a>, which has normal sized images that are trainable in a sane amount of time. Imagenette consists of 3 datasets, which are all subsets of Imagenet.</p>

<ol>
  <li>
<strong>Imagenette:</strong> A subset of 10 <em>easily</em> classified classes from Imagenet.</li>
  <li>
<strong>Imagewoof:</strong> A subset of 10 classes from Imagenet that <em>aren’t easy</em> to classify (all dog breeds).</li>
  <li>
<strong>Image网 (or Imagewang):</strong> Imagenette and Imagewoof combined with some twists to make it into a tricky semi-supervised unbalanced classification problem.</li>
</ol>

<p>Each of the datasets is available in 3 sizes:</p>

<ul>
  <li>Full size.</li>
  <li>Shortest length resized to 160px (aspect ratio maintained).</li>
  <li>Shortest length resized to 320px (aspect ratio maintained).</li>
</ul>

<p><strong><em>Jeremy says…</em></strong></p>

<blockquote>
  <p>A big part of a getting good at using deep learning in your domain is knowing how to create small, workable, useful datasets…</p>

  <p>Try to come up with a toy problem or two that will give you insight into your full problem.</p>
</blockquote>

<h2 id="data-block-api-foundations">
<a class="anchor" href="#data-block-api-foundations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Block API Foundations</h2>

<p><a href="https://youtu.be/hPQKzsjTyyQ?t=1036"><em>(Jump to lesson 11 video)</em></a></p>

<p>Imagenette isn’t big, but it’s too big to fit into RAM. We need to read it in one image at a time. We need to design and build fastai’s <em>Data Block API</em> from scratch.</p>

<p>What does the raw image data look like? Here is the directory structure of Imagenette and the number of images for each class:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>imagenette2-160
├── train
│   ├── n01440764 [963 entries]
│   ├── n02102040 [955 entries]
│   ├── n02979186 [993 entries]
│   ├── n03000684 [858 entries]
│   ├── n03028079 [941 entries]
│   ├── n03394916 [956 entries]
│   ├── n03417042 [961 entries]
│   ├── n03425413 [931 entries]
│   ├── n03445777 [951 entries]
│   └── n03888257 [960 entries]
└── val 
    ├── n01440764 [387 entries]
    ├── n02102040 [395 entries]
    ├── n02979186 [357 entries]
    ├── n03000684 [386 entries]
    ├── n03028079 [409 entries]
    ├── n03394916 [394 entries]
    ├── n03417042 [389 entries]
    ├── n03425413 [419 entries]
    ├── n03445777 [399 entries]
    └── n03888257 [390 entries]
</code></pre></div></div>

<p>There is are <code class="language-plaintext highlighter-rouge">train</code> and <code class="language-plaintext highlighter-rouge">val</code> directories. Each contains a subdirectory of JPEG images. The name of these subdirectories comes from Imagenet and is an encoding of the different categories and subcategories of objects. See the <a href="http://image-net.org/explore">ImageNet explorer</a> to get what I mean. The classes are all fairly balanced too.</p>

<p>All the images are roughly the same size, but have their own dimensions and are rectangular.</p>

<p>The first class <code class="language-plaintext highlighter-rouge">n01440764</code> is a <em>tench</em>, which is a kind of fish:</p>

<p><img src="/blog/images/fastai/Sun,%2005%20Apr%202020%20211704.png" alt="img"></p>

<p>What does the data of this image look like? If we read it into python with the image library <code class="language-plaintext highlighter-rouge">PIL</code> and convert it to a <code class="language-plaintext highlighter-rouge">numpy</code> array, we can see it is an array with shape <code class="language-plaintext highlighter-rouge">(160, 237, 3)</code> and its numbers look like:</p>

<p><img src="/blog/images/fastai/image-20200405212534524.png" alt="image-20200405212534524"></p>

<p>It’s an RGB image, with each pixel represented by 3 integers between 0 and 255, which say what colour the pixel is.</p>

<p>The first thing to build before the Datablack API is good way of grabbing all the files we need for training and validating our model. For image files, there are a number of different file types available. We can easily get a list of all the standard image file extensions from the python module <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types"><code class="language-plaintext highlighter-rouge">mimetypes</code></a>.</p>

<p><img src="/blog/images/fastai/image-20200412121636032.png" alt="image-20200412121636032"></p>

<p>With the list of image file extensions we can do a walk through the dataset’s directory <code class="language-plaintext highlighter-rouge">path</code> to grab all these files.  Here is the function used in fastai to recursively walk the directory path:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">extensions</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">extensions</span> <span class="o">=</span> <span class="n">setify</span><span class="p">(</span><span class="n">extensions</span><span class="p">)</span>
    <span class="n">extensions</span> <span class="o">=</span> <span class="p">{</span><span class="n">e</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">extensions</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">recurse</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">p</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">f</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">walk</span><span class="p">(</span><span class="n">path</span><span class="p">)):</span> 
            <span class="c1"># returns (dirpath, dirnames, filenames)
</span>            <span class="k">if</span> <span class="n">include</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> 
                <span class="n">d</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">d</span> <span class="k">if</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">include</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">d</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">d</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">o</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'.'</span><span class="p">)]</span>
            <span class="n">res</span> <span class="o">+=</span> <span class="n">_get_files</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">extensions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="p">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">scandir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="p">.</span><span class="n">is_file</span><span class="p">()]</span>
        <span class="k">return</span> <span class="n">_get_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">extensions</span><span class="p">)</span>
</code></pre></div></div>

<p>Where the helper function <code class="language-plaintext highlighter-rouge">_get_files</code> takes a list of files in a directory and selects only the files that have the right <code class="language-plaintext highlighter-rouge">extension</code>. <code class="language-plaintext highlighter-rouge">get_files</code> also uses the fast low level file functions <code class="language-plaintext highlighter-rouge">os.walk</code> and <code class="language-plaintext highlighter-rouge">os.scandir</code>. These functions connect to library functions written in C and are orders of magnitudes faster than using something like python’s <code class="language-plaintext highlighter-rouge">glob</code> module.</p>

<h3 id="datablock-api-motivation">
<a class="anchor" href="#datablock-api-motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataBlock API Motivation</h3>

<p><a href="https://youtu.be/hPQKzsjTyyQ?t=1714"><em>(Jump to lesson 11 video)</em></a></p>

<p>Why does FastAI have a DataBlock API? The API attempts to systematically define all the steps  necessary to prepare data for a deep learning model, and create a mix and match rec
ipe book for combining these steps.</p>

<p>To prepare for modeling, the following steps need to be performed:</p>

<ul>
  <li>Get the source items</li>
  <li>Splitting the items into training and validation sets
    <ul>
      <li>e.g. random fraction, folder name, CSV, …</li>
    </ul>
  </li>
  <li>Labelling the items,
    <ul>
      <li>e.g. from folder name, file name/re, CSV, …</li>
    </ul>
  </li>
  <li>Processing the items (such as normalization)</li>
  <li>
<em>(Optional)</em> Doing some Augmentation</li>
  <li>Transform items into tensors</li>
  <li>Make data into batches (<code class="language-plaintext highlighter-rouge">DataLoader</code>)</li>
  <li>
<em>(Optional)</em> Transform per batch</li>
  <li>Combine the <code class="language-plaintext highlighter-rouge">DataLoader</code>s together into a <code class="language-plaintext highlighter-rouge">DataBunch</code>
</li>
  <li>
<em>(Optional)</em> Add a test set</li>
</ul>

<h3 id="step-1---imagelist">
<a class="anchor" href="#step-1---imagelist" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 1 - ImageList</h3>

<p>We need to get the source items and store them in some kind of collection data structure. We already created the <code class="language-plaintext highlighter-rouge">ListContainer</code> data structure for storing things in a list in a previous lecture that we can build upon here. What we want to do is not store the loaded source data in our list, rather store the filenames of the source data in a list and load things into memory when they are needed.</p>

<p>We create a base class called <code class="language-plaintext highlighter-rouge">ItemList</code> that has a <code class="language-plaintext highlighter-rouge">get</code> method, which subclasses override, and this <code class="language-plaintext highlighter-rouge">get</code> method should load and return what you put in there. An <em>item</em> is some data point, it could be an image, text sequence, whatever. For the case of the <code class="language-plaintext highlighter-rouge">ImageList</code> subclass, <code class="language-plaintext highlighter-rouge">get</code> will read the image file and return a <code class="language-plaintext highlighter-rouge">PIL</code> image object.</p>

<p>In summary, <code class="language-plaintext highlighter-rouge">ItemList</code>:</p>

<ul>
  <li>
    <p>Is a list of items and a path where they came from</p>
  </li>
  <li>
    <p><em>Optionally</em> has a list of transforms, which are functions.</p>
  </li>
  <li>
    <p>The list of transforms is <code class="language-plaintext highlighter-rouge">composed</code> and applied every time you <code class="language-plaintext highlighter-rouge">get</code> and item. So you get back a transformed item every time.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ItemList</span><span class="p">(</span><span class="n">ListContainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">items</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s">'.'</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">path</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">tfms</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">),</span><span class="n">tfms</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s">f'</span><span class="si">{</span><span class="nb">super</span><span class="p">().</span><span class="n">__repr__</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s">Path: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">path</span><span class="si">}</span><span class="s">'</span>
    
    <span class="k">def</span> <span class="nf">new</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">items</span><span class="p">,</span> <span class="n">cls</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">cls</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">cls</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">__class__</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">path</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">tfms</span><span class="p">)</span>
    
    <span class="k">def</span>  <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span> <span class="k">return</span> <span class="n">i</span>
    <span class="k">def</span> <span class="nf">_get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span> <span class="k">return</span> <span class="n">compose</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">tfms</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">().</span><span class="n">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="nb">list</span><span class="p">):</span> <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_get</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_get</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

</code></pre></div></div>

<p><strong><em>Aside:</em> <code class="language-plaintext highlighter-rouge">compose</code> <em>function:</em></strong> takes a list of functions and combines them into a pipeline that chains the outputs of the first function to input of the second and so on. In other words, a deep neural network is just a composition of functions (layers). <strong>NB</strong>, if you compose a list of functions then the order they are applied is <em>right-to-left</em>.
As a one-liner:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compose</span><span class="p">([</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div></div>

<p>Here is the implementation for <code class="language-plaintext highlighter-rouge">ImageList</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ImageList</span><span class="p">(</span><span class="n">ItemList</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">from_files</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">extensions</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">extensions</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">extensions</span> <span class="o">=</span> <span class="n">image_extensions</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">get_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">extensions</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">,</span> <span class="n">include</span><span class="o">=</span><span class="n">include</span><span class="p">),</span> <span class="n">path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
</code></pre></div></div>

<p>It defines the class method <code class="language-plaintext highlighter-rouge">from_files</code> to get a list of image files from a <code class="language-plaintext highlighter-rouge">path</code>. It uses the <code class="language-plaintext highlighter-rouge">image_extensions</code> and searches the <code class="language-plaintext highlighter-rouge">path</code> using our <code class="language-plaintext highlighter-rouge">get_files</code> function. Its <code class="language-plaintext highlighter-rouge">get</code> method is overridden and returns a <code class="language-plaintext highlighter-rouge">PIL</code> image object.</p>

<p>What about transforms? The first transform we create is <code class="language-plaintext highlighter-rouge">make_rgb</code>. When loading in images, if an image is grayscale then <code class="language-plaintext highlighter-rouge">PIL</code> will read it in as a rank 2 tensor, when we want it to be rank 3. So the <code class="language-plaintext highlighter-rouge">make_rgb</code> transform calls the <code class="language-plaintext highlighter-rouge">PIL</code> method to convert it to RGB:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_rgb</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">item</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>

<span class="n">il</span> <span class="o">=</span> <span class="n">ImageList</span><span class="p">.</span><span class="n">from_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">make_rgb</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-2---split-validation-set">
<a class="anchor" href="#step-2---split-validation-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 2 - Split Validation Set</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=2175">(Jump to lesson 11 video)</a></em></p>

<p>Next we want to split the data into train and validation sets. For Imagenette training and validation sets have already been created for us and live in different directories. These are the <code class="language-plaintext highlighter-rouge">train</code> and <code class="language-plaintext highlighter-rouge">val</code> subdirectories. The path of an image in the dataset is something like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>imagenette2-160/val/n02102040/n02102040_850.JPEG
</code></pre></div></div>

<p>The parent of the image is its label, and the parent of its parent (grandparent) denotes whether it is in the training or validation set. Therefore we will create a splitter function that splits on an image path’s grandparent:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grandparent_splitter</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">valid_name</span><span class="o">=</span><span class="s">'valid'</span><span class="p">,</span> <span class="n">train_name</span><span class="o">=</span><span class="s">'train'</span><span class="p">):</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">fname</span><span class="p">.</span><span class="n">parent</span><span class="p">.</span><span class="n">parent</span><span class="p">.</span><span class="n">name</span>
    <span class="k">return</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">gp</span><span class="o">==</span><span class="n">valid_name</span> <span class="k">else</span> <span class="bp">False</span> <span class="k">if</span> <span class="n">gp</span><span class="o">==</span><span class="n">train_name</span> <span class="k">else</span> <span class="bp">None</span>
</code></pre></div></div>

<p>Let’s go further and encapsulate this into a <code class="language-plaintext highlighter-rouge">SplitData</code> class that can apply a splitter function to any kind of <code class="language-plaintext highlighter-rouge">ItemList</code> object:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SplitData</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">valid</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">train</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">valid</span> <span class="o">=</span> <span class="n">train</span><span class="p">,</span><span class="n">valid</span>
        
    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
        <span class="c1"># This is needed if we want to pickle SplitData and be able to load it back without recursion errors
</span>        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">train</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">data</span><span class="p">:</span><span class="n">Any</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> 
    
    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">split_by_func</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">il</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="n">lists</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">il</span><span class="p">.</span><span class="n">new</span><span class="p">,</span> <span class="n">split_by_func</span><span class="p">(</span><span class="n">il</span><span class="p">.</span><span class="n">items</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s">f'</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="si">}</span><span class="se">\n</span><span class="s">Train: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">train</span><span class="si">}</span><span class="se">\n</span><span class="s">Valid: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">valid</span><span class="si">}</span><span class="se">\n</span><span class="s">'</span>
</code></pre></div></div>

<p>This has a <code class="language-plaintext highlighter-rouge">split_by_func</code>, which uses <code class="language-plaintext highlighter-rouge">ItemList.new</code> to coerce the train and test items back into their original <code class="language-plaintext highlighter-rouge">ItemList</code> type. So in the end we will get two <code class="language-plaintext highlighter-rouge">ImageList</code> objects for training and validation image sets.</p>

<p>This looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SplitData
  Train: ImageList (12894 items)
    [PosixPath('/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_9403.JPEG'),           
     PosixPath('/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_6402.JPEG'), 
     PosixPath('/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_4446.JPEG'), 
     PosixPath('/home/ubuntu/.fastai/data/imagenette-160/train/n03888257/n03888257_13476.JPEG')...]
    Path: /home/ubuntu/.fastai/data/imagenette-160
  
  Valid: ImageList (500 items)
    [PosixPath('/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00016387.JPEG'),   
     PosixPath('/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00034544.JPEG'), 
     PosixPath('/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00009593.JPEG'), 
     PosixPath('/home/ubuntu/.fastai/data/imagenette-160/val/n03888257/ILSVRC2012_val_00020698.JPEG')...]
    Path: /home/ubuntu/.fastai/data/imagenette-160
</code></pre></div></div>

<h3 id="step-3---labelling">
<a class="anchor" href="#step-3---labelling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 3 - Labelling</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=2368">(Jump to lesson 11 video)</a></em></p>

<p>Labelling is a little more tricky because it has to be done <em>after</em> splitting, at it uses <em>training</em> set information to apply to the <em>validation</em> set. To do this we need to create something called a <em>Processor</em>. For example, we could have a processor whose job it was to <strong>encoded the label strings into numbers</strong>:</p>

<ul>
  <li>“tench” =&gt; 0</li>
  <li>“french horn” =&gt; 1</li>
</ul>

<p>We would need the training set to have the same mapping as the validation set. So we need to create a <em>vocabulary</em> which encodes our classes to numbers and tells us the order they are in. We create this vocabulary from the training set and use that to transform the training and the validation sets.</p>

<p>Other examples of processors would be <strong>processing texts</strong> to <strong>tokenize</strong> and <strong>numericalize</strong> them. Text in the validation set should be numericalized the same way as the training set. Or in another case with <strong>tabular data</strong>, where we wish to <strong>fill missing values</strong> with, for instance, the median computed on the training set. The median is stored in the inner state of the <em>Processor</em> and applied on the validation set.</p>

<p>Here we label according to the folders of the images, so simply <code class="language-plaintext highlighter-rouge">fn.parent.name</code>. We label the training set first with a newly created <code class="language-plaintext highlighter-rouge">CategoryProcessor</code> so that it computes its inner <code class="language-plaintext highlighter-rouge">vocab</code> on that set. Then we label the validation set using the same processor, which means it uses the same <code class="language-plaintext highlighter-rouge">vocab</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Processor</span><span class="p">():</span> 
    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">items</span><span class="p">):</span> <span class="k">return</span> <span class="n">items</span>

<span class="k">class</span> <span class="nc">CategoryProcessor</span><span class="p">(</span><span class="n">Processor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="o">=</span><span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">items</span><span class="p">):</span>
        <span class="c1">#The vocab is defined on the first use.
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">uniqueify</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">otoi</span>  <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">)}</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">proc_</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">items</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">proc_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>  <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">otoi</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">deprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">deproc_</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">deproc_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">proc_x</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">proc_y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">LabeledData</span><span class="p">.</span><span class="n">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">.</span><span class="n">train</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">proc_x</span><span class="o">=</span><span class="n">proc_x</span><span class="p">,</span> <span class="n">proc_y</span><span class="o">=</span><span class="n">proc_y</span><span class="p">)</span>
    <span class="n">valid</span> <span class="o">=</span> <span class="n">LabeledData</span><span class="p">.</span><span class="n">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">.</span><span class="n">valid</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">proc_x</span><span class="o">=</span><span class="n">proc_x</span><span class="p">,</span> <span class="n">proc_y</span><span class="o">=</span><span class="n">proc_y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SplitData</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">valid</span><span class="p">)</span>


<span class="n">ll</span> <span class="o">=</span> <span class="n">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">,</span> <span class="n">parent_labeler</span><span class="p">,</span> <span class="n">proc_y</span><span class="o">=</span><span class="n">CategoryProcessor</span><span class="p">())</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">LabeledData</code> is an object that has two <code class="language-plaintext highlighter-rouge">ItemList</code> objects: <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>. In this case <code class="language-plaintext highlighter-rouge">x</code> is an <code class="language-plaintext highlighter-rouge">ImageList</code> (basically a list of file paths) and <code class="language-plaintext highlighter-rouge">y</code> is a <code class="language-plaintext highlighter-rouge">ItemList</code> (a generic container, here it contains labels: <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, etc).</p>

<p>This output <code class="language-plaintext highlighter-rouge">ll</code> looks like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SplitData
 Train: LabeledData
  x: ImageList (12894 items)
  [...]
  Path: ...
  y: ItemList (12894 items)
  [0,0,0,0,0,0...]
  Path: ...

 Valid: LabeledData
  x: ImageList (500 items)
  [...]
  Path: ...
  y: ItemList (500 items)
  [0,0,0,0,0,0...]
  Path: ...
</code></pre></div></div>

<p><strong>Question:</strong> <em>How do could we handle unseen labels?</em></p>

<blockquote>
  <p>You could group together rare labels into a single label called ‘other’/’unknown’</p>
</blockquote>

<h3 id="step-4---databunch">
<a class="anchor" href="#step-4---databunch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 4 - DataBunch</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=3226">(Jump_to lesson 11 video)</a></em></p>

<p>A <code class="language-plaintext highlighter-rouge">DataBunch</code> has a training dataloader and a validation data loader. Here is the class:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataBunch</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">train_dl</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">valid_dl</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">c_in</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">c_out</span> <span class="o">=</span> <span class="n">train_dl</span><span class="p">,</span><span class="n">valid_dl</span><span class="p">,</span><span class="n">c_in</span><span class="p">,</span><span class="n">c_out</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">train_ds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">train_dl</span><span class="p">.</span><span class="n">dataset</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">valid_ds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">valid_dl</span><span class="p">.</span><span class="n">dataset</span>
    

<span class="k">def</span> <span class="nf">databunchify</span><span class="p">(</span><span class="n">sd</span><span class="p">:</span> <span class="n">SplitData</span><span class="p">,</span> <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="n">sd</span><span class="p">.</span><span class="n">train</span><span class="p">,</span> <span class="n">sd</span><span class="p">.</span><span class="n">valid</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">DataBunch</span><span class="p">(</span><span class="o">*</span><span class="n">dls</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="n">c_out</span><span class="p">)</span>

<span class="n">SplitData</span><span class="p">.</span><span class="n">to_databunch</span> <span class="o">=</span> <span class="n">databunchify</span>
</code></pre></div></div>

<h3 id="all-the-steps">
<a class="anchor" href="#all-the-steps" aria-hidden="true"><span class="octicon octicon-link"></span></a>All the steps</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=3360">(Jump_to lesson 11 video)</a></em></p>

<p>Here’s the fully dataloading pipeline using the Data Block API: grab the path, untar the data, list the transforms, get item list, split the data, label the data, create a databunch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">untar_data</span><span class="p">(</span><span class="n">datasets</span><span class="p">.</span><span class="n">URLs</span><span class="p">.</span><span class="n">IMAGENETTE_160</span><span class="p">)</span>
<span class="n">tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_rgb</span><span class="p">,</span> <span class="n">ResizeFixed</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">to_byte_tensor</span><span class="p">,</span> <span class="n">to_float_tensor</span><span class="p">]</span>

<span class="n">il</span> <span class="o">=</span> <span class="n">ImageList</span><span class="p">.</span><span class="n">from_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">tfms</span><span class="p">)</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">SplitData</span><span class="p">.</span><span class="n">split_by_func</span><span class="p">(</span><span class="n">il</span><span class="p">,</span> <span class="n">partial</span><span class="p">(</span><span class="n">grandparent_splitter</span><span class="p">,</span> <span class="n">valid_name</span><span class="o">=</span><span class="s">'val'</span><span class="p">))</span>
<span class="n">ll</span> <span class="o">=</span> <span class="n">label_by_func</span><span class="p">(</span><span class="n">sd</span><span class="p">,</span> <span class="n">parent_labeler</span><span class="p">,</span> <span class="n">proc_y</span><span class="o">=</span><span class="n">CategoryProcessor</span><span class="p">())</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ll</span><span class="p">.</span><span class="n">to_databunch</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="new-cnn-model">
<a class="anchor" href="#new-cnn-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>New CNN Model</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=3360">(Jump_to lesson 11 video)</a></em></p>

<p>Let’s train a CNN using our databunch.</p>

<p>Get the callbacks:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cbfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">partial</span><span class="p">(</span><span class="n">AvgStatsCallback</span><span class="p">,</span><span class="n">accuracy</span><span class="p">),</span>
        <span class="n">CudaCallback</span><span class="p">]</span>
</code></pre></div></div>

<p>Next we need to normalize all the images for training. With colour images we need to normalize all three channels so we need means and standard deviations for each of channels. We can get these statistics from a batch/batches from the <em>training set</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normalize_chan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mean</span><span class="p">[...,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">])</span> <span class="o">/</span> <span class="n">std</span><span class="p">[...,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">]</span>

<span class="n">_m</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.47</span><span class="p">,</span> <span class="mf">0.48</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">])</span>
<span class="n">_s</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">])</span>
<span class="n">norm_imagenette</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">normalize_chan</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">_m</span><span class="p">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">std</span><span class="o">=</span><span class="n">_s</span><span class="p">.</span><span class="n">cuda</span><span class="p">())</span>

<span class="n">cbfs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">BatchTransformXCallback</span><span class="p">,</span> <span class="n">norm_imagenette</span><span class="p">))</span>
</code></pre></div></div>

<p>We build our model using <a href="https://arxiv.org/abs/1812.01187">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>, in particular: we don’t use a big conv 7x7 at first but three 3x3 convs, and don’t go directly from 3 channels to 64 but progressively add those. The first 3 layers are <em>very important</em>. Back in the old days people would use 5x5 and 7x7 kernels for the first layer. However the Bag of Tricks paper shows that this isn’t a good idea, which refers to many previous citations and competition winning models. The message is clear - 3x3 kernels give you more bang for your buck. You get deeper, you get the same receptive field, and it’s also faster because you have less working going on. The 7x7 conv layer also is over 5 times slower than a single 3x3 as well.</p>

<p><em>(Recall - a conv_layer composes a Conv2d, Generalized ReLU, and a normalization (e.g. batchnorm))</em></p>

<p>The <strong>first layer</strong> is a 3x3 kernel and a known number of channels <code class="language-plaintext highlighter-rouge">data.c_in</code>, which in this case is 3 (RGB). What about the number of outputs? The kernel has <code class="language-plaintext highlighter-rouge">9*c_in</code> numbers. We want to make sure that our kernal has something useful to do. You don’t want more numbers coming out than are coming in, because its a waste of time. We set the number of outputs to the closest power of 2 below <code class="language-plaintext highlighter-rouge">9*c_in</code>. (For <code class="language-plaintext highlighter-rouge">9*3</code> that is 16). The <em>stride</em> of the first layer is also 1, so the first layer doesn’t downsample.</p>

<p>Then for the <strong>next two layers</strong> we successively mutiply the number of outputs by 2 and set stride to 2.</p>

<p><em>(Anywhere you see something that isn’t a 3x3 kernel - have a big think as to whether it makes sense.)</em></p>

<p>We use 4 conv_layers in the body of the model with sizes: <code class="language-plaintext highlighter-rouge">nfs = [64,64,128,256]</code></p>

<p>Here is the code for the model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="k">def</span> <span class="nf">prev_pow_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">2</span><span class="o">**</span><span class="n">math</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">get_cnn_layers</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">nfs</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span> <span class="k">return</span> <span class="n">layer</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">c_in</span>
    <span class="n">l2</span> <span class="o">=</span> <span class="n">prev_pow_2</span><span class="p">(</span><span class="n">l1</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">layers</span> <span class="o">=</span>  <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">l1</span>  <span class="p">,</span> <span class="n">l2</span>  <span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
               <span class="n">f</span><span class="p">(</span><span class="n">l2</span>  <span class="p">,</span> <span class="n">l2</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
               <span class="n">f</span><span class="p">(</span><span class="n">l2</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">l2</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)]</span>
    <span class="n">nfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">l2</span><span class="o">*</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="n">nfs</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">nfs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nfs</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">Lambda</span><span class="p">(</span><span class="n">flatten</span><span class="p">),</span> 
               <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nfs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">c_out</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">layers</span>

<span class="k">def</span> <span class="nf">get_cnn_model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">nfs</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">get_cnn_layers</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">nfs</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
</code></pre></div></div>

<p>We run this with cosine 1cycle annealing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sched</span> <span class="o">=</span> <span class="n">combine_scheds</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.7</span><span class="p">],</span> <span class="n">cos_1cycle_anneal</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.05</span><span class="p">))</span>

<span class="n">learn</span><span class="p">,</span><span class="n">run</span> <span class="o">=</span> <span class="n">get_learn_run</span><span class="p">(</span><span class="n">nfs</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">conv_layer</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">cbfs</span><span class="o">+</span><span class="p">[</span>
    <span class="n">partial</span><span class="p">(</span><span class="n">ParamScheduler</span><span class="p">,</span> <span class="s">'lr'</span><span class="p">,</span> <span class="n">sched</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>This gives performance 72.6% for Imagenette, which is not bad and on the right track.</p>

<h2 id="universal-optimizer">
<a class="anchor" href="#universal-optimizer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Universal Optimizer</h2>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=4074">(Jump to lesson 11 video)</a></em></p>

<p>Every other deep learning library treats every optimizer algorithm as a totally different object. But this is an artificial categorization - there is however only <em>one optimizer</em> and lots of stuff you can add to it.</p>

<p>We are going to implement this paper: <a href="https://arxiv.org/abs/1904.00962">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a>. We will implement this equation set from the paper:</p>

<p><img src="/blog/images/fastai/image-20200612115122485.png" alt="image-20200612115122485" height="500"></p>

<p>This looks scary because of the mathematical notation and greek symbols, but we will find when we turn it into code it is actually very simple. All these terms are separable parts or <em>‘steppers’</em> of a more general optimizer class.</p>

<p>All experiments will be done with our CNN model using the Imagenette dataset.</p>

<h3 id="the-optimizer-class">
<a class="anchor" href="#the-optimizer-class" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Optimizer Class</h3>

<p>Let’s build own <code class="language-plaintext highlighter-rouge">Optimizer</code> class. It needs to have a <code class="language-plaintext highlighter-rouge">zero_grad</code> method to set the gradients of the parameters to zero  and a <code class="language-plaintext highlighter-rouge">step</code> method that does some kind of step. The thing we will do differently from all other libraries is that the functionality of <code class="language-plaintext highlighter-rouge">step</code> will be abstracted out into a composition of <code class="language-plaintext highlighter-rouge">stepper</code> functions. The <code class="language-plaintext highlighter-rouge">Optimizer</code> class will simply have a list of <code class="language-plaintext highlighter-rouge">steppers</code> to iterate through.</p>

<p>In order to optimize something we need to know what all the parameter tensors are in a model. However we might want to say: <em>“the last two layers should have a different learning rate to the rest of the layers.”</em> We can instead decide group different parameters into <code class="language-plaintext highlighter-rouge">param_groups</code>, which would basically be a list of lists. Each parameter group can have its own set of hyperparameters (e.g. learning rate, weight decay, etc) and each parameter group will have its own dictionary to store these hyperparameters.</p>

<p>Code for the <code class="language-plaintext highlighter-rouge">Optimizer</code> class, with a way of getting default hyperparameters for the steppers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">steppers</span><span class="p">,</span> <span class="o">**</span><span class="n">defaults</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">steppers</span> <span class="o">=</span> <span class="n">listify</span><span class="p">(</span><span class="n">steppers</span><span class="p">)</span>
        <span class="n">maybe_update</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">steppers</span><span class="p">,</span> <span class="n">defaults</span><span class="p">,</span> <span class="n">get_defaults</span><span class="p">)</span>
        <span class="c1"># might be a generator
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="c1"># ensure params is a list of lists
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hypers</span> <span class="o">=</span> <span class="p">[{</span><span class="o">**</span><span class="n">defaults</span><span class="p">}</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">grad_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># return flattened list of parameters from all layers
</span>        <span class="k">return</span> <span class="p">[(</span><span class="n">p</span><span class="p">,</span><span class="n">hyper</span><span class="p">)</span> <span class="k">for</span> <span class="n">pg</span><span class="p">,</span><span class="n">hyper</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">hypers</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pg</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">hyper</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad_params</span><span class="p">():</span>
            <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">detach_</span><span class="p">()</span>
            <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">hyper</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad_params</span><span class="p">():</span> <span class="n">compose</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">steppers</span><span class="p">,</span> <span class="o">**</span><span class="n">hyper</span><span class="p">)</span>
          

<span class="k">def</span> <span class="nf">maybe_update</span><span class="p">(</span><span class="n">os</span><span class="p">,</span> <span class="n">dest</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">os</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">f</span><span class="p">(</span><span class="n">o</span><span class="p">).</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dest</span><span class="p">:</span> <span class="n">dest</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

<span class="k">def</span> <span class="nf">get_defaults</span><span class="p">(</span><span class="n">d</span><span class="p">):</span> <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="s">'_defaults'</span><span class="p">,{})</span>
</code></pre></div></div>

<p>This is basically the gist of PyTorch’s <code class="language-plaintext highlighter-rouge">optim.Optimizer</code>, but with the <code class="language-plaintext highlighter-rouge">steppers</code>.  A <code class="language-plaintext highlighter-rouge">stepper</code> is a function that forms part of the optimizer recipe. An example of <code class="language-plaintext highlighter-rouge">stepper</code> is <code class="language-plaintext highlighter-rouge">sgd_step</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sgd_step</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="s">"""
  SGD step
  p : parameters
  lr : learning rate
  """</span>
	<span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">p</span>
</code></pre></div></div>

<p>In other words we can create an optimizers like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt_func</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">steppers</span><span class="o">=</span><span class="p">[</span><span class="n">sgd_step</span><span class="p">])</span>
</code></pre></div></div>

<p>When we call <code class="language-plaintext highlighter-rouge">step</code>, it loops through all our parameters and composes all our steppers then calls that composition on the parameters.</p>

<h3 id="weight-decay">
<a class="anchor" href="#weight-decay" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Decay</h3>

<p><em>(This subsection is combines explanations from the <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/09_optimizers.ipynb">09_optimizers.ipynb</a> notebook and this <a href="https://www.fast.ai/2018/07/02/adam-weight-decay/#understanding-adamw-weight-decay-or-l2-regularization">fastai blog post</a>)</em></p>

<p>By letting our model learn high parameters, it might fit all the data points in the training set with an over-complex function that has very sharp changes, which will lead to overfitting.</p>

<p><img src="/blog/images/fastai/overfit.png" alt="Fitting vs over-fitting" style="zoom: 50%;"></p>

<p>Weight decay comes from the idea of <em>L2 regularization</em>, which consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.</p>

<p>Classic L2 regularization consists of adding the sum of all the weights squared to the loss multiplied by a hyperparameter, <code class="language-plaintext highlighter-rouge">wd</code>. The intuition is that large weight values get ‘exploded’ when they are squared which will contribute to a much larger loss. The optimizer will therefore shy away from such regions of parameter space. In theory, this would be like adding this big sum to the total loss at the end of the forward pass:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_with_wd</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">wd</span> <span class="o">*</span> <span class="n">all_weights</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></div>

<p>This is never how this is implemented in practice however. The sum would require a massive reduction of all the weight tensors at every update step, which would be expensive and potentially numerically unstable (more so with lower precision). We only need the derivative of that wrt to each of the weights, and remembering that $\frac{\partial}{\partial w_j} \sum_i w_i^2 = 2 w_j$, you can see that adding the big sum to the loss is equivalent to locally updating the gradients of the parameters like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">wd</span> <span class="o">*</span> <span class="n">weight</span>
</code></pre></div></div>

<p>For the case of vanilla SGD this is equivalent to updating the parameters with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">wd</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</code></pre></div></div>

<p>This technique is called <strong>weight decay</strong>, as each weight is decayed by a factor <code class="language-plaintext highlighter-rouge">lr * wd</code>, as it’s shown in this last formula.</p>

<p>This is a slightly confusing thing - <strong>Aren’t L2 regularization and Weight decay the same thing?</strong> – <em>Not exactly. Only in the case of vanilla SGD are they the same.</em></p>

<p>For algorithms such as momentum, RMSProp, and Adam, the update has some additional formulas around the gradient. For SGD with momentum the formula is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">moving_avg</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">moving_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">.</span><span class="n">grad</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">moving_avg</span>
</code></pre></div></div>

<p>If we did L2 regularization this would become:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">moving_avg</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">moving_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">wd</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">moving_avg</span>
</code></pre></div></div>

<p>Whereas with weight decay it would be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">moving_avg</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">moving_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">.</span><span class="n">grad</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">moving_avg</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">wd</span> <span class="o">*</span> <span class="n">w</span>
</code></pre></div></div>

<p>We can see that the part subtracted from w linked to regularization isn’t the same in the two methods, and the <code class="language-plaintext highlighter-rouge">wd</code> is polluted by the <code class="language-plaintext highlighter-rouge">(1-alpha)</code> factor. When using something more complicated like the Adam optimizer, it gets even more polluted. Most libraries use the first formulation, but as it was pointed out in <a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Regularization</a> by Ilya Loshchilov and Frank Hutter, it is better to use the second one with the Adam optimizer, which is why the fastai library made it its default. This implemation of Adam and decoupled weight decay is often called <strong>AdamW</strong>.</p>

<p><img src="/blog/images/fastai/compare_acc.png" alt="Accuracy with L2 regularization or weight decay" style="zoom:67%;"></p>

<p><em>The above is a comparison between the two done by <a href="https://www.fast.ai/2018/07/02/adam-weight-decay/#results-of-adamw-experiments-does-it-work">Jeremy and Sylvain</a>. The weight decay formulation gives slightly better results.</em></p>

<p>Weight decay is also super simple to implement too - you simply subtract <code class="language-plaintext highlighter-rouge">lr*wd*weight</code> from the weights before the optimizer step. We could create some abstract base class for stepper or just use a function in python:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">weight_decay</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">wd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>

<span class="n">weight_decay</span><span class="p">.</span><span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</code></pre></div></div>

<p>In python you can attach an attribute to any object in python including functions. Here we attach a dictionary <code class="language-plaintext highlighter-rouge">_defaults</code> for default hyper-parameter values. Alternatively, if you were using an abstract base class you would just have a class attribute <code class="language-plaintext highlighter-rouge">_defaults</code> to get the sam effect.</p>

<p>Similarly, if you wanted to use L2 regularization then the implementation is also simply - add <code class="language-plaintext highlighter-rouge">wd*weight</code> to the gradients:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">l2_reg</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="n">wd</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">)</span> <span class="c1"># add is actually scaled-add
</span>    <span class="k">return</span> <span class="n">p</span>
  
<span class="n">l2_reg</span><span class="p">.</span><span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="momentum">
<a class="anchor" href="#momentum" aria-hidden="true"><span class="octicon octicon-link"></span></a>Momentum</h3>

<p>Momentum will require an optimizers that has some <em>state</em> because it needs to remember what it did in the last update to do the current update.</p>

<p>Momentum requires to add some state. We need to save the moving average of the gradients to be able to do the step and store this inside the optimizer state. We need to track, for every single parameter, what happened last time. This is actually quite a bit of state - if you had 10 million activations in your network, you now have 10 million more floats that you have to store.</p>

<p>To implement this we need to create a new subclass of <code class="language-plaintext highlighter-rouge">Optimizer</code> which maintains a <code class="language-plaintext highlighter-rouge">state</code> attribute which can store running <code class="language-plaintext highlighter-rouge">Stat</code>s of things, which are updated every <code class="language-plaintext highlighter-rouge">step</code>. A <code class="language-plaintext highlighter-rouge">Stat</code> is an object that has two methods and an attribute:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">init_state</code>, that returns the initial state (a tensor of 0. for the moving average of gradients)</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">update</code>, that updates the state with the new gradient value. Takes a state dict and returns an updated state dict.</p>
  </li>
  <li>We also read the <code class="language-plaintext highlighter-rouge">_defaults</code> values of those objects, to allow them to provide default values to hyper-parameters.</li>
</ul>

<p>The <code class="language-plaintext highlighter-rouge">StatefulOptimizer</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">StatefulOptimizer</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">steppers</span><span class="p">,</span> <span class="n">stats</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">defaults</span><span class="p">):</span>
      	<span class="bp">self</span><span class="p">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">listify</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
        <span class="n">maybe_update</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">,</span> <span class="n">defaults</span><span class="p">,</span> <span class="n">get_defaults</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">steppers</span><span class="p">,</span> <span class="o">**</span><span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="p">{}</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      	<span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">hyper</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">:</span>
                <span class="c1"># Create a state for p and 
</span>                <span class="c1"># call all the statistics to initalize it
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="n">maybe_update</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="n">o</span><span class="p">.</span><span class="n">init_state</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">stat</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">:</span> 
                <span class="n">state</span> <span class="o">=</span> <span class="n">stat</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">**</span><span class="n">hyper</span><span class="p">)</span>
            
            <span class="c1"># run the steppers
</span>            <span class="n">compose</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">steppers</span><span class="p">,</span> <span class="o">**</span><span class="n">state</span><span class="p">,</span> <span class="o">**</span><span class="n">hyper</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span>
</code></pre></div></div>

<p>For momentum we are mainting an moving average of the parameter gradients. The <code class="language-plaintext highlighter-rouge">Stat</code> for this would be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AverageGrad</span><span class="p">(</span><span class="n">Stat</span><span class="p">):</span>
    <span class="c1"># with NO dampening
</span>    <span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
		
    <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"grad_avg"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)}</span>
		
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">mom</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">state</span><span class="p">[</span><span class="s">"grad_avg"</span><span class="p">].</span><span class="n">mul_</span><span class="p">(</span><span class="n">mom</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>	
</code></pre></div></div>

<p>With this we can now implement MomentumSGD a new stepper, <code class="language-plaintext highlighter-rouge">momentum_step</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">momentum_step</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">grad_avg</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">p</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span><span class="p">,</span> <span class="n">grad_avg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
  
  
<span class="n">sgd_mom_opt</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">StatefulOptimizer</span><span class="p">,</span>
                      <span class="n">steppers</span><span class="o">=</span><span class="p">[</span><span class="n">momentum_step</span><span class="p">,</span>
                                <span class="n">weight_decay</span><span class="p">],</span>
                      <span class="n">stats</span><span class="o">=</span><span class="n">AverageGrad</span><span class="p">(),</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="aside-pythons-wonderful-kwargs">
<a class="anchor" href="#aside-pythons-wonderful-kwargs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Aside: Python’s Wonderful kwargs</h4>

<p>One of the features of python that makes this work is the wonderfully flexible way that python handles parameters and lists of keyword arguments. All the different <code class="language-plaintext highlighter-rouge">stepper</code> functions take a weight tensor plus some individual set of positional arguments. It would be complicated as hell trying to call a list of <code class="language-plaintext highlighter-rouge">stepper</code> functions with a list of which of all their positional arguments. However if you stick on a <code class="language-plaintext highlighter-rouge">**kwargs</code> to a <code class="language-plaintext highlighter-rouge">stepper</code>’s parameter list then it enables you to throw a dictionary of <em>all</em> the parameters name/value pairs to <em>all</em> the <code class="language-plaintext highlighter-rouge">stepper</code> functions, and when it comes time to call the <code class="language-plaintext highlighter-rouge">stepper</code> it will simply take what it needs from <code class="language-plaintext highlighter-rouge">kwargs</code> and ignore everything else!</p>

<p>This trivial example shows what I mean:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">bar</span><span class="p">,</span> <span class="n">lol</span><span class="p">,</span> <span class="n">baz</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">bar</span><span class="p">,</span> <span class="n">lol</span><span class="p">,</span> <span class="n">baz</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">boo</span><span class="p">(</span><span class="n">biz</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">biz</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"lol"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">"baz"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">"biz"</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
<span class="n">foo</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
<span class="n">boo</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

<span class="c1">## This outputs:
## 1 2 3
## 5
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">params</code> has all of the kwargs for all of the functions. Functions <code class="language-plaintext highlighter-rouge">foo</code> and <code class="language-plaintext highlighter-rouge">boo</code> only take what they need from <code class="language-plaintext highlighter-rouge">params</code>. The only thing you need to be careful of here is that you don’t have any stepper functions whose parameters share the same name, but are semantically different things. You could perhaps have a check on <code class="language-plaintext highlighter-rouge">params</code> to throw and exception if a key is overwritten to prevent this silent bug.</p>

<h3 id="weight-decay--batch-norm-a-surprising-result">
<a class="anchor" href="#weight-decay--batch-norm-a-surprising-result" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Decay + Batch Norm: A Surprising Result</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=5115">Jump to lesson 11 video</a></em></p>

<p>Weight decay scales the weights by a factor of <code class="language-plaintext highlighter-rouge">(1-wd)</code>, however batch norm is invariant to weight scaling, so weight decay followed by batch norm effectively undoes the weight decay.</p>

<p>This was pointed out in the paper: <a href="https://arxiv.org/abs/1706.05350">L2 Regularization versus Batch and Weight Normalization</a>.</p>

<p>Empirically, however, it has been found that weight decay and batch norm is actually anyway better than batch norm and no weight decay. This <a href="https://blog.janestreet.com/l2-regularization-and-batch-norm/">blog post</a> explores this for vanillia SGD:</p>

<blockquote>
  <p>…without an L2 penalty or other constraint on weight scale, introducing batch norm will introduce a large decay in the effective learning rate over time. But an L2 penalty counters this.</p>
</blockquote>

<p>This paper - <a href="https://arxiv.org/abs/1810.12281">Three Mechanisms of Weight Decay Regularization</a> - identifies three different ways weight decay exerts a regularization effect, depending on the different optimization algorithm and architecture.</p>

<p><em>In reality, we don’t really know why weight decay works, but empirically it seems to help and basically all models use it. :-)</em></p>

<h3 id="momentum-experiments">
<a class="anchor" href="#momentum-experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Momentum Experiments</h3>

<p>Momentum is also interesting, and we really don’t understand it works.</p>

<p>Let’s create some fake data series of 200 normally distributed points and plot the moving average of this series with different <code class="language-plaintext highlighter-rouge">beta</code> values: <code class="language-plaintext highlighter-rouge">[0.5, 0.7, 0.9, 0.99]</code></p>

<p>The regular momentum:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mom1</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">avg</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">avg</span><span class="o">=</span><span class="n">yi</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">avg</span> <span class="o">+</span> <span class="n">yi</span>
    <span class="k">return</span> <span class="n">res</span>
</code></pre></div></div>

<p>Here is a plot of the data (blue) and moving average (red):</p>

<p><img src="/blog/images/fastai/image-20200614172145628.png" alt="image-20200614172145628" style="zoom:50%;"></p>

<p><em>With very little momentum (small <code class="language-plaintext highlighter-rouge">beta</code>) it is very bumpy/highly variant. When you get up to larger values of momentum it shoots off and the new values its seeing can’t slow it down. So <strong>you have to be really careful when it comes to high momentum values.</strong></em></p>

<p><br></p>

<p>This is a rather naive implementation. We can fix it by instead using a <strong>Exponentially Weighted Moving Average</strong> (or <strong>EWMA</strong>, also called <code class="language-plaintext highlighter-rouge">lerp</code> in PyTorch):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ewma</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">beta</span><span class="o">*</span><span class="n">v1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">v2</span>
  
<span class="k">def</span> <span class="nf">mom2</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">avg</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">avg</span><span class="o">=</span><span class="n">yi</span>
    <span class="n">avg</span> <span class="o">=</span> <span class="n">ewma</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg</span>
</code></pre></div></div>

<p>This helps to <em>dampen</em> the incoming data point which stops it being so bumpy for lower momentum values. . Plotting the same again:</p>

<p><img src="/blog/images/fastai/image-20200614172508617.png" alt="image-20200614172508617" style="zoom:50%;"></p>

<p><em>This works much better. So we’re done? - Not quite.</em></p>

<p><br></p>

<p>What if the thing we are trying to match isn’t just random, but is some function like a polynomial. We’ve also added an outlier at the start.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lin_comb</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">beta</span><span class="o">*</span><span class="n">v1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">v2</span>
  
<span class="k">def</span> <span class="nf">mom2</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">avg</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">avg</span><span class="o">=</span><span class="n">yi</span>
    <span class="n">avg</span> <span class="o">=</span> <span class="n">lin_comb</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg</span>
</code></pre></div></div>

<p>Let’s see how EWMA does here:</p>

<p><img src="/blog/images/fastai/image-20200614172530045.png" alt="image-20200614172530045" style="zoom:50%;"></p>

<p><em>The outlier at the start causes trouble with the higher momentum values. The first item is massively biasing the start.</em></p>

<p><br></p>

<p>We need to do something called <strong><em>Debiasing</em></strong> (aka bias correction). We want to make sure that no observation is weighted too highly. Normal way of doing EWMA gives the first point far too much weight. These first points are all zero, so the running averages are all biased low. Add a correction factor <code class="language-plaintext highlighter-rouge">dbias</code>: $x_i = x_i/(1 - \beta^{i+1})$. When $i$ is large this correction factor tends to 1 - it only pushes up the initial values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mom3</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">avg</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">avg</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">avg</span> <span class="o">=</span> <span class="n">lin_comb</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="o">**</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Plot that:</p>

<p><img src="/blog/images/fastai/image-20200614172604064.png" alt="image-20200614172604064" style="zoom:50%;"></p>

<p><em>This is pretty good. It debiases pretty well even if we have a bad starting point. You can see why <code class="language-plaintext highlighter-rouge">beta=0.9</code> is a <strong>popular value</strong>.</em></p>

<p><br></p>

<h3 id="adam-algorithm">
<a class="anchor" href="#adam-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adam Algorithm</h3>

<p>Let’s use what we’ve learned to implement the optimizer <strong>Adam</strong>. The algorithm definition from the <a href="https://arxiv.org/abs/1412.6980">Adam paper (2014)</a> is:</p>

<p><img src="/blog/images/fastai/image-20200614221103906.png" alt="image-20200614221103906" style="zoom:50%;"></p>

<p>If we look inside the while loop, and stare at the maths there is nothing in there we haven’t seen already. $g$ is the gradients of the weights, $m$ is the EWMA of the gradients, and $v$ is the EMWA of the square of the gradients. $m$ and $v$ are then debiased, as we have seen above.</p>

<p><em>Adam is just dampened debiased momentum divided by dampened debiased root sum of squared gradients.</em></p>

<p>To implement Adam we will need to implement the following:</p>

<ul>
  <li>EWMA of the gradients - a <code class="language-plaintext highlighter-rouge">Stat</code> subclass.</li>
  <li>EWMA of the square of the gradients - a <code class="language-plaintext highlighter-rouge">Stat</code> subclass.</li>
  <li>A debiasing function. This will need to know which step we are on.</li>
  <li>A step counter - a <code class="language-plaintext highlighter-rouge">Stat</code> subclass</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AverageGrad</span><span class="p">(</span><span class="n">Stat</span><span class="p">):</span>
    <span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dampening</span> <span class="o">=</span> <span class="n">dampening</span>
        
    <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">'grad_avg'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)}</span>
      
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">mom</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">state</span><span class="p">[</span><span class="s">'mom_damp'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">mom</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dampening</span> <span class="k">else</span> <span class="mf">1.</span>
        <span class="n">state</span><span class="p">[</span><span class="s">'grad_avg'</span><span class="p">].</span><span class="n">mul_</span><span class="p">(</span><span class="n">mom</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s">'mom_damp'</span><span class="p">],</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>
      
   
<span class="k">class</span> <span class="nc">AverageSqrGrad</span><span class="p">(</span><span class="n">Stat</span><span class="p">):</span>
    <span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">sqr_mom</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dampening</span> <span class="o">=</span> <span class="n">dampening</span>
        
    <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">'sqr_avg'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)}</span>
      
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">sqr_mom</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">state</span><span class="p">[</span><span class="s">'sqr_damp'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">sqr_mom</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">dampening</span> <span class="k">else</span> <span class="mf">1.</span>
        <span class="n">state</span><span class="p">[</span><span class="s">'sqr_avg'</span><span class="p">].</span><span class="n">mul_</span><span class="p">(</span><span class="n">sqr_mom</span><span class="p">).</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s">'sqr_damp'</span><span class="p">],</span>
                                                <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">,</span>
                                                <span class="n">sp</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>
      
      
<span class="k">class</span> <span class="nc">StepCount</span><span class="p">(</span><span class="n">Stat</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span> <span class="k">return</span> <span class="p">{</span><span class="s">'step'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">state</span>

      
<span class="k">def</span> <span class="nf">debias_term</span><span class="p">(</span><span class="n">mom</span><span class="p">,</span> <span class="n">damp</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="c1"># if we don't use dampening (damp=1) we need to divide by 1-mom because
</span>    <span class="c1"># that term is missing everywhere
</span>    <span class="k">return</span> <span class="n">damp</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mom</span><span class="o">**</span><span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">mom</span><span class="p">)</span>
</code></pre></div></div>

<p>Adam as a <code class="language-plaintext highlighter-rouge">stepper</code> is now:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">adam_step</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">mom</span><span class="p">,</span> <span class="n">mom_damp</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">sqr_mom</span><span class="p">,</span> <span class="n">sqr_damp</span><span class="p">,</span> <span class="n">grad_avg</span><span class="p">,</span> <span class="n">sqr_avg</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">debias1</span> <span class="o">=</span> <span class="n">debias_term</span><span class="p">(</span><span class="n">mom</span><span class="p">,</span> <span class="n">mom_damp</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">debias2</span> <span class="o">=</span> <span class="n">debias_term</span><span class="p">(</span><span class="n">sqr_mom</span><span class="p">,</span> <span class="n">sqr_damp</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">/</span> <span class="n">debias1</span><span class="p">,</span>
                    <span class="n">grad_avg</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">sqr_avg</span><span class="o">/</span><span class="n">debias2</span><span class="p">).</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
<span class="n">adam_step</span><span class="p">.</span><span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">adam_opt</span><span class="p">(</span><span class="n">xtra_step</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">StatefulOptimizer</span><span class="p">,</span>
                   <span class="n">steppers</span><span class="o">=</span><span class="p">[</span><span class="n">adam_step</span><span class="p">,</span><span class="n">weight_decay</span><span class="p">]</span><span class="o">+</span><span class="n">listify</span><span class="p">(</span><span class="n">xtra_step</span><span class="p">),</span>
                   <span class="n">stats</span><span class="o">=</span><span class="p">[</span><span class="n">AverageGrad</span><span class="p">(</span><span class="n">dampening</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                          <span class="n">AverageSqrGrad</span><span class="p">(),</span>
                          <span class="n">StepCount</span><span class="p">()],</span>
                   <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<p>Note that the weight decay and Adam step are totally decoupled. This is an  implemention the <strong>AdamW</strong> algorithm, mentioned above in the weight decay subsection. First you decay the weights, then you do the Adam step.</p>

<p><br></p>

<p>The epsilon <code class="language-plaintext highlighter-rouge">eps</code> in Adam is super important to think about. What if we set <code class="language-plaintext highlighter-rouge">eps=1</code>? Most of the time the gradients are going to be smaller than 1 and the squared gradients are going to be much smaller than 1. So <code class="language-plaintext highlighter-rouge">eps=1</code> is going to be much bigger than <code class="language-plaintext highlighter-rouge">(sqr_avg/debias2).sqrt()</code>, so <code class="language-plaintext highlighter-rouge">eps</code> will dominate and the optimizer will be pretty close to being SGD with debiased-dampened momentum.</p>

<p>Whereas, if <code class="language-plaintext highlighter-rouge">eps=1e-7</code> then we are really using the <code class="language-plaintext highlighter-rouge">(sqr_avg/debias2).sqrt()</code> term. If you have some activation that has had a very small squared gradients for a while, the value of this term could well be <code class="language-plaintext highlighter-rouge">1e-6</code>. Dividing by this is equivalent to multiplying by a million, which would kill your optimizer. The trick getting Adam and friends working well is a value between <code class="language-plaintext highlighter-rouge">eps=1e-3</code> and <code class="language-plaintext highlighter-rouge">eps=1e-1</code>.</p>

<p>Most people use <code class="language-plaintext highlighter-rouge">1e-7</code>, which is equivalent to multiplying by 10 million. Here <code class="language-plaintext highlighter-rouge">eps</code> is basically just a small hack number put in to avoid a possible division by zero. We can instead treat <code class="language-plaintext highlighter-rouge">eps</code> as a kind of smoothing factor that enables the optimizer to behave more like momentum SGD sometimes and normal Adam at other times.</p>

<h3 id="lamb-algorithm">
<a class="anchor" href="#lamb-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>LAMB Algorithm</h3>

<p><em><a href="https://youtu.be/hPQKzsjTyyQ&amp;t=6038">Jump to lesson 11 video</a></em></p>

<p>It’s then super easy to implement a new optimizer. This is LAMB from a <a href="https://arxiv.org/abs/1904.00962">very recent paper (2019)</a>:</p>

\[\begin{align}
g_{t}^{l} &amp;= \nabla L(w_{t-1}^{l}, x_{t}) \\
m_{t}^{l} &amp;= \beta_{1} m_{t-1}^{l} + (1-\beta_{1}) g_{t}^{l} \\
v_{t}^{l} &amp;= \beta_{2} v_{t-1}^{l} + (1-\beta_{2}) g_{t}^{l} \odot g_{t}^{l} \\
m_{t}^{l} &amp;= m_{t}^{l} / (1 - \beta_{1}^{t}) \\
v_{t}^{l} &amp;= v_{t}^{l} / (1 - \beta_{2}^{t}) \\
r_{1} &amp;= \|w_{t-1}^{l}\|_{2} \\
s_{t}^{l} &amp;= \frac{m_{t}^{l}}{\sqrt{v_{t}^{l}} + \epsilon} + \lambda w_{t-1}^{l} \\ 
r_{2} &amp;= \| s_{t}^{l} \|_{2} \\
\eta^{l} &amp;= \eta * r_{1}/r_{2} \\ 
w_{t}^{l} &amp;= w_{t-1}^{l} - \eta_{l} * s_{t}^{l} \\
\end{align}\]

<p>This is stuff we’ve seen before in Adam plus a few extras:</p>

<ul>
  <li>$m$ and $v$ are the debiased dampened momentum and the debiased dampened square of the gradients exactly like Adam.</li>
  <li>$|w^l_{t-1}|_2$ is the <em>layerwise</em> l2-norm of the weights in layer $l$.</li>
  <li>The learning rate $\eta^l$ is adapted individually for every layer.</li>
  <li>It requires the same amount of state as Adam.</li>
</ul>

<p>As code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lamb_step</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">mom</span><span class="p">,</span> <span class="n">mom_damp</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span>
              <span class="n">sqr_mom</span><span class="p">,</span> <span class="n">sqr_damp</span><span class="p">,</span> <span class="n">grad_avg</span><span class="p">,</span>
              <span class="n">sqr_avg</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">wd</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">debias1</span> <span class="o">=</span> <span class="n">debias</span><span class="p">(</span><span class="n">mom</span><span class="p">,</span>     <span class="n">mom_damp</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">debias2</span> <span class="o">=</span> <span class="n">debias</span><span class="p">(</span><span class="n">sqr_mom</span><span class="p">,</span> <span class="n">sqr_damp</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    
    <span class="n">r1</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">sqrt</span><span class="p">()</span>  <span class="c1"># layerwise L2 norm
</span>    
    <span class="n">step</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_avg</span><span class="o">/</span><span class="n">debias1</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="n">sqr_avg</span><span class="o">/</span><span class="n">debias2</span><span class="p">).</span><span class="n">sqrt</span><span class="p">()</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">wd</span><span class="o">*</span><span class="n">p</span><span class="p">.</span><span class="n">data</span>
    
    <span class="n">r2</span> <span class="o">=</span> <span class="n">step</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">sqrt</span><span class="p">()</span>	<span class="c1"># layerwise L2
</span>    
    <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="n">r1</span><span class="o">/</span><span class="n">r2</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">step</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
  
<span class="n">lamb_step</span><span class="p">.</span><span class="n">_defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">lamb_opt</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">StatefulOptimizer</span><span class="p">,</span> 
                   <span class="n">steppers</span><span class="o">=</span><span class="n">lamb_step</span><span class="p">,</span> 
                   <span class="n">stats</span><span class="o">=</span><span class="p">[</span><span class="n">AverageGrad</span><span class="p">(</span><span class="n">dampening</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                          <span class="n">AverageSqrGrad</span><span class="p">(),</span> 
                          <span class="n">StepCount</span><span class="p">()],</span>
                   <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<p>The ratio <code class="language-plaintext highlighter-rouge">r1/r2</code> is called the <em>‘trust ratio’</em> by the original authors. In most implementations this trust ratio’s upper value is clipped to make LAMB more stable. This upper bound is typically set to 10 or 100.</p>

<h2 id="data-augmentation">
<a class="anchor" href="#data-augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Augmentation</h2>

<p><a href="https://youtu.be/hPQKzsjTyyQ?t=6878"><em>(Jump to lesson 11 video)</em></a></p>

<p>Up to this point we have created our datablocks API and optimizers and we have these running nicely together in a <code class="language-plaintext highlighter-rouge">Learner</code> class (which replaces the <code class="language-plaintext highlighter-rouge">Runner</code> class seen in prior lessons). With this we can train a reasonably good Imagenette model with a CNN (<a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/09b_learner.ipynb">09b_learner.ipynb</a>). But Imagenette is a bit short of data, so to make an even better model we should use <strong>data  augmentation</strong>.</p>

<p>It’s important when doing data augmentation to <em>look at or listen to or understand your augmented data to make sure the new data is of good enough quality and it representative of the original data. Don’t just chuck it into a model and hope for the best.</em> Let’s look at an example where this can create problems with <strong><em>resizing images</em></strong>.</p>

<h3 id="resizing">
<a class="anchor" href="#resizing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resizing</h3>

<p>Let’s load up some imagenette:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">untar_data</span><span class="p">(</span><span class="n">datasets</span><span class="p">.</span><span class="n">URLs</span><span class="p">.</span><span class="n">IMAGENETTE</span><span class="p">)</span>
<span class="n">tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_rgb</span><span class="p">,</span> <span class="n">ResizeFixed</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">to_byte_tensor</span><span class="p">,</span> <span class="n">to_float_tensor</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">get_il</span><span class="p">(</span><span class="n">tfms</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">ImageList</span><span class="p">.</span><span class="n">from_files</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">tfms</span><span class="p">)</span>

<span class="n">il</span> <span class="o">=</span> <span class="n">get_il</span><span class="p">(</span><span class="n">tfms</span><span class="p">)</span>
</code></pre></div></div>

<p>Our transforms here are:</p>

<ol>
  <li>Convert image to RGB</li>
  <li>Resize to 128x128</li>
  <li>Convert from Pillow object (bytes) to byte tensor</li>
  <li>Convert to float tensor</li>
</ol>

<p>Here is an image from the <em>tench</em> class with the <code class="language-plaintext highlighter-rouge">ResizeFixed(128)</code> transform:</p>

<p><img src="/blog/images/fastai/image-20200714124958546.png" alt="image-20200714124958546" style="zoom:50%;"></p>

<p>However, here is what the original looks like:</p>

<p><img src="/blog/images/fastai/image-20200714125023158.png" alt="image-20200714125023158" style="zoom:50%;"></p>

<p>Notice how the fish’s scale texture and the texture of the net is completely lost during the resizing.  This resizing method may be chucking out useful textures that are key to identifying certain classes. <em>Be careful of resampling methods, you can quickly lose some textures!</em></p>

<p><em>(Perhaps one could try making the resampling method used in a resizing random as a method of data augmentation?)</em></p>

<p>There are many resampling methods. Be critical about the resizing. Look at the augmented data and make sure that you aren’t losing key information like textures. <code class="language-plaintext highlighter-rouge">Pillow</code> has many different resizing methods. They recommend  <code class="language-plaintext highlighter-rouge">ANTIALIAS</code> as a good default. Let’s look at the different resampling methods offered by Pillow:</p>

<table>
  <thead>
    <tr>
      <th>
<img src="/blog/images/fastai/image-20200714130050576.png" alt="image-20200714130050576" style="zoom:50%;"><br><code class="language-plaintext highlighter-rouge">ANTIALIAS</code>
</th>
      <th>
<img src="/blog/images/fastai/image-20200714130119721.png" alt="image-20200714130119721" style="zoom:50%;"><br><code class="language-plaintext highlighter-rouge">BICUBIC</code>
</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
<img src="/blog/images/fastai/image-20200714130142388.png" alt="image-20200714130142388" style="zoom:50%;"><br><code class="language-plaintext highlighter-rouge">NEAREST</code>
</td>
      <td>
<img src="/blog/images/fastai/image-20200714130202590.png" alt="image-20200714130202590" style="zoom:50%;"><br><code class="language-plaintext highlighter-rouge">BICUBIC/NEAREST</code>
</td>
    </tr>
  </tbody>
</table>

<p><code class="language-plaintext highlighter-rouge">NEAREST</code> is the only one that preserves the textures. There are a lot of aliasing artifacts however. The last one, <code class="language-plaintext highlighter-rouge">BICUBLIC/NEAREST</code>, does a resize to <code class="language-plaintext highlighter-rouge">256x256</code> with <code class="language-plaintext highlighter-rouge">BICUBIC</code> then another resize to <code class="language-plaintext highlighter-rouge">128x128</code> with <code class="language-plaintext highlighter-rouge">NEAREST</code> to achieve a pretty good compromise.</p>

<p><strong>Topical:</strong> This <a href="https://twitter.com/poolio/status/1280689329908543488">recent tweet</a> shows the difference between image resize methods in tensorflow and pytorch. <em>“Something to check when porting and comparing models between frameworks”</em></p>

<h3 id="flipping-rotating-cropping">
<a class="anchor" href="#flipping-rotating-cropping" aria-hidden="true"><span class="octicon octicon-link"></span></a>Flipping, Rotating, Cropping</h3>

<p>Flipping is a great data augmentation for vision. A very important point to make here is that <em>doing image transforms on bytes is much <strong>faster</strong> than doing them on floats</em>, because bytes are 4 times smaller than floats. If you are flipping an image, flipping bytes is identical to flipping floats in terms of the outcome. You should definitely everything you can while your image is still bytes (a <code class="language-plaintext highlighter-rouge">Pillow</code> object). However you should be careful when doing <em>destructive</em> transformations on bytes, because you can get rounding errors and saturation errors. Again - <em>inspect the steps and take nothing for granted.</em></p>

<p>Flip:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PilRandomFlip</span><span class="p">(</span><span class="n">PilTransform</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="o">=</span><span class="n">p</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="n">FLIP_LEFT_RIGHT</span><span class="p">)</span> <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="o">&lt;</span><span class="bp">self</span><span class="p">.</span><span class="n">p</span> <span class="k">else</span> <span class="n">x</span>
</code></pre></div></div>

<p>It’s therefore really important to think <em>when</em> in your transformation pipeline you do certain image transforms.</p>

<p>We can easily extend this to doing  the whole <em>dihedral</em> group of transformations (random horizontal flip, random vertical flip, and the four 90 degrees rotations) by passing a int between 0 and 6 to <code class="language-plaintext highlighter-rouge">transpose</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PilRandomDihedral</span><span class="p">(</span><span class="n">PilTransform</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.75</span><span class="p">):</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="mi">7</span><span class="o">/</span><span class="mi">8</span> <span class="c1">#Little hack to get the 1/8 identity dihedral transform taken into account.
</span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="o">&gt;</span><span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="p">:</span> <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
</code></pre></div></div>

<p>We can also do <em>random cropping</em>. A great way to do data augmentation is to grab a small piece of an image and zoom into that piece. We can do this by randomly cropping and then down sizing the selection.</p>

<p>Naively we can do this with two steps in Pillow - crop and resize:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span><span class="p">.</span><span class="n">crop</span><span class="p">((</span><span class="mi">60</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">320</span><span class="p">,</span><span class="mi">320</span><span class="p">)).</span><span class="n">resize</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">),</span> <span class="n">resample</span><span class="o">=</span><span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="n">BILINEAR</span><span class="p">)</span>
</code></pre></div></div>

<p>However this degrades quality. You can instead do it all in one step with Pillow’s <code class="language-plaintext highlighter-rouge">transform</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span><span class="p">.</span><span class="n">transform</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">),</span> <span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="n">EXTENT</span><span class="p">,</span> <span class="n">cnr2</span><span class="p">,</span> <span class="n">resample</span><span class="o">=</span><span class="n">resample</span><span class="p">)</span>
</code></pre></div></div>

<p>This is an example of doing multiple destructive transformations when the image is still in bytes. Do them all in one go, if possible, or wait until they are floats.</p>

<p><code class="language-plaintext highlighter-rouge">RandomResizeCrop</code> the usual data augmentation used on ImageNet (introduced <a href="https://arxiv.org/pdf/1409.4842.pdf">here</a>) that consists of selecting 8 to 100% of the image area and a scale between 3/4 and 4/3 as a crop, then resizing it to the desired size. It combines some zoom and a bit of squishing at a very low computational cost.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RandomResizedCrop</span><span class="p">(</span><span class="n">GeneralCrop</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.08</span><span class="p">,</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">3.</span><span class="o">/</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">4.</span><span class="o">/</span><span class="mf">3.</span><span class="p">),</span> <span class="n">resample</span><span class="o">=</span><span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="n">BILINEAR</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">resample</span><span class="o">=</span><span class="n">resample</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">ratio</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span><span class="n">ratio</span>
    
    <span class="k">def</span> <span class="nf">get_corners</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">wc</span><span class="p">,</span> <span class="n">hc</span><span class="p">):</span>
        <span class="n">area</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="n">h</span>
        <span class="c1">#Tries 10 times to get a proper crop inside the image.
</span>        <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">area</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">area</span>
            <span class="n">ratio</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
            <span class="n">new_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">area</span> <span class="o">*</span> <span class="n">ratio</span><span class="p">)))</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">area</span> <span class="o">/</span> <span class="n">ratio</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">new_w</span> <span class="o">&lt;=</span> <span class="n">w</span> <span class="ow">and</span> <span class="n">new_h</span> <span class="o">&lt;=</span> <span class="n">h</span><span class="p">:</span>
                <span class="n">left</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="n">new_w</span><span class="p">)</span>
                <span class="n">top</span>  <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h</span> <span class="o">-</span> <span class="n">new_h</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">left</span> <span class="o">+</span> <span class="n">new_w</span><span class="p">,</span> <span class="n">top</span> <span class="o">+</span> <span class="n">new_h</span><span class="p">)</span>
        
        <span class="c1"># Fallback to squish
</span>        <span class="k">if</span>   <span class="n">w</span><span class="o">/</span><span class="n">h</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">w</span><span class="o">/</span><span class="bp">self</span><span class="p">.</span><span class="n">ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">w</span><span class="o">/</span><span class="n">h</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">h</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">h</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>                     <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">w</span><span class="o">-</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="o">-</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="o">+</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="o">+</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<hr>

<p><strong><em>Jeremy says…</em></strong></p>

<blockquote>
  <p>The most useful transformation by far shown in competition winners, is to grab a small piece of the image of the image and zoom into it. This is called a <strong>random resize crop</strong>. This is also really useful to know in any domain. For example, in NLP a really useful thing to do is a grab different sized chunks of contiguous text. With audio, if you are doing speech recognition, grab different sized pieces of the utterances. If you can find a way to get different slices of your data, it’s a fantastically useful data augmentation approach. So this is by far the most important augmentation in every imagenet winner in the last 6 years or so.</p>
</blockquote>

<h3 id="perspective-transform">
<a class="anchor" href="#perspective-transform" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perspective Transform</h3>

<p>What <code class="language-plaintext highlighter-rouge">RandomResizeCrop</code> does, however, is it squishes the aspect ratio to some between 3:4 and 4:3. This can distort the image making objects expand outwards and inwards. Probably what they really want to do is something physically reasonable. If you are above or below something then your <em>perspective changes.</em> What would be even better is <strong>perspective warping</strong>.</p>

<p>To do perspective warping, we map the corners of the image to new points: for instance, if we want to tilt the image so that the top looks closer to us, the top/left corner needs to be shifted to the right and the top/right to the left. To avoid squishing, the bottom/left corner needs to be shifted to the left and the bottom/right corner to the right.</p>

<p>PIL can do this for us but it requires 8 coefficients we need to calculate. The math isn’t the most important here, as we’ve done it for you. We need to solve this <a href="https://web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/">system of linear equation</a>. The equation solver is called <code class="language-plaintext highlighter-rouge">torch.solve</code> in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">FloatTensor</span><span class="p">,</span><span class="n">LongTensor</span>


<span class="k">def</span> <span class="nf">find_coeffs</span><span class="p">(</span><span class="n">orig_pts</span><span class="p">,</span> <span class="n">targ_pts</span><span class="p">):</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1">#The equations we'll need to solve.
</span>    <span class="k">for</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">targ_pts</span><span class="p">,</span> <span class="n">orig_pts</span><span class="p">):</span>
        <span class="n">matrix</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">matrix</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">orig_pts</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">#The 8 scalars we seek are solution of AX = B
</span>    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">A</span><span class="p">)[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">])</span>
    
    
<span class="k">def</span> <span class="nf">warp</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">src_coords</span><span class="p">,</span> <span class="n">resample</span><span class="o">=</span><span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="n">BILINEAR</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">size</span>
    <span class="n">targ_coords</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="n">h</span><span class="p">),(</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">),(</span><span class="n">w</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">find_coeffs</span><span class="p">(</span><span class="n">src_coords</span><span class="p">,</span><span class="n">targ_coords</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">PIL</span><span class="p">.</span><span class="n">Image</span><span class="p">.</span><span class="n">PERSPECTIVE</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">c</span><span class="p">),</span> <span class="n">resample</span><span class="o">=</span><span class="n">resample</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>
</code></pre></div></div>

<p>We can add a transform to do this perspective warping automatically with the rand resize and crop:</p>

<p><img src="/blog/images/fastai/image-20200715142924523.png" alt="image-20200715142924523" style="zoom:50%;"></p>

<p><strong><em>Question:</em></strong> <em>How do you handle tabular, text, time series etc?</em></p>

<blockquote>
  <p>Text - read it. Time series - look at the signal. Tabular you would just visualize the augmented data the same way you would. All augmentations are domain specific. You need to know your data and domain well to invent your own augmentations. Make sure it makes sense and seems reasonable.</p>
</blockquote>

<p><strong><em>Question</em>:</strong> <em>What happens if the object of interest gets cropped out by image augmentation?</em></p>

<blockquote>
  <p>These are called <strong>noisy labels</strong>. Interesingly, the ImageNet winning stratedy with crop/zooming is to randomly pick 8-100% of the pixels. They very often have no tench. Or very often they have just the fin or just the eye. If we want to used crop/zooming well, we need to be very good at handling noisy labels (more in the next lesson). Also this tells you that if you already have noisy labels - don’t worry about it. All of the research we have tells us that we can handle noisy labels as long as it’s not biased.</p>

  <p>It will also learn to recognize all the things associated with a tench. So if there’s a middle aged many outside looking happy - it could well be a tench! :)</p>
</blockquote>

<h3 id="batch-data-augmentation">
<a class="anchor" href="#batch-data-augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Data Augmentation</h3>

<p>It’s actually possible to arbitrary <em>affine transformation</em> of images (rotating, zooming, shifting, warping etc) on the GPU. PyTorch provides all the functionality to make this happen. All the transformations need to happen <em>after</em> we create a batch. The key is to do them on a whole batch at a time. Nearly all PyTorch operations can be done batch-wise.</p>

<p>To do this we create a mini-batch of random numbers to create a mini-batch of augmented images.</p>

<p>An affine transform is basically a linear transform plus a translation. They are represented by matrices and multiple affine transforms can be composed by multiplying all their matrices together. <em>(<a href="https://eli.thegreenplace.net/2018/affine-transformations/">See this Blog post.</a>)</em></p>

<p>Let’s load an image. Its shape is <code class="language-plaintext highlighter-rouge">torch.Size([1, 3, 128, 128])</code>.</p>

<p>Once we have resized our images so that we can batch them together, we can apply more data augmentation on a batch level. For the affine/coord transforms, we proceed like this:</p>

<h4 id="1-generate-the-grid">
<a class="anchor" href="#1-generate-the-grid" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Generate the Grid</h4>

<p>A matrix is simply a function that takes a coordinate $(x, y)$ and maps them to some new location $(x’, y’)$. If we want to apply the same transformation to every pixel in an image, we first need to represent every pixel as a x,y coordinate.</p>

<p>Generate a grid map, using torch’s <code class="language-plaintext highlighter-rouge">affine_grid</code>, of the size of our batch (<code class="language-plaintext highlighter-rouge">bs x height x width x 2</code>) that contains the coordinates (-1 to 1) of a grid of size height x width (this will be the final size of the image, and doesn’t have to be the same as the current size in the batch).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">size</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">affine_grid</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  
  
<span class="n">grid</span> <span class="o">=</span> <span class="n">affine_grid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</code></pre></div></div>

<p>This has shape: <code class="language-plaintext highlighter-rouge">torch.Size([1, 128, 128, 2])</code>, and looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">1.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.9843</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.9685</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span> <span class="mf">0.9685</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.9843</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0000</span><span class="p">]],</span>
				  <span class="p">...,</span>
         <span class="p">[[</span><span class="o">-</span><span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.9843</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.9685</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">...,</span>
          <span class="p">[</span> <span class="mf">0.9685</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.9843</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">]]]])</span>
</code></pre></div></div>

<h4 id="step-2-affine-multiplication">
<a class="anchor" href="#step-2-affine-multiplication" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 2: Affine Multiplication</h4>

<p>Apply the affine transforms (which is a matrix multiplication) and the coord transforms to that grid map.</p>

<p>In 2D an affine transformation has the form y = Ax + b where A is a 2x2 matrix and b a vector with 2 coordinates. It’s usually represented by the 3x3 matrix</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>  <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>  <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>  <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>  <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
   <span class="mi">0</span>       <span class="mi">0</span>     <span class="mi">1</span>
</code></pre></div></div>
<p>because then the composition of two affine transforms can be computed with the matrix product of their 3x3 representations.</p>

<p>The matrix for a anti-clockwise rotation that has an angle of <code class="language-plaintext highlighter-rouge">theta</code> is:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>  <span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="mi">0</span>
<span class="o">-</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="mi">0</span>
<span class="mi">0</span>           <span class="mi">0</span>          <span class="mi">1</span>
</code></pre></div></div>

<p>then we draw a different <code class="language-plaintext highlighter-rouge">theta</code> for each version of the image in the batch to return a batch of rotation matrices (size <code class="language-plaintext highlighter-rouge">bsx3x3</code>).</p>

<p>You then multiply all 3 channels by the rotation matrix and add the translation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tfm_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">grid</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">m</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">m</span><span class="p">[:,</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">][:,</span><span class="bp">None</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="step-3-interpolate">
<a class="anchor" href="#step-3-interpolate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 3: Interpolate</h4>

<p>Interpolate the values of the final pixels we want from the initial images in the batch, according to the transformed grid map using Pytorch’s <code class="language-plaintext highlighter-rouge">F.grid_sample</code> function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rotate_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">degrees</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">affine_grid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">degrees</span><span class="p">,</span><span class="n">degrees</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rotation_matrix</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>
    <span class="n">tfm_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">grid</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">m</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">])</span>  <span class="o">+</span> <span class="n">m</span><span class="p">[:,</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">][:,</span><span class="bp">None</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">grid_sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tfm_grid</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Here is also a faster version using <code class="language-plaintext highlighter-rouge">F.affine_grid</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rotate_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">degrees</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">size</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">degrees</span><span class="p">,</span><span class="n">degrees</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rotation_matrix</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">affine_grid</span><span class="p">(</span><span class="n">m</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">],</span> <span class="n">size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">grid_sample</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">grid</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Results of this with random cropping and warping:</p>

<p><img src="/blog/images/fastai/image-20200716121014192.png" alt="image-20200716121014192" style="zoom:50%;"></p>

<p>We get these black borders around the image. But PyTorch <code class="language-plaintext highlighter-rouge">grid_sample</code> also has a <code class="language-plaintext highlighter-rouge">padding_mode</code> argument that lets you filling in this black space in different ways - <code class="language-plaintext highlighter-rouge">"zeros"</code>, <code class="language-plaintext highlighter-rouge">"border"</code>, or <code class="language-plaintext highlighter-rouge">"reflection"</code>. These can enrich and improve our augmented data even more. Here is reflection:</p>

<p><img src="/blog/images/fastai/image-20200716121404092.png" alt="image-20200716121404092" style="zoom:50%;"></p>

<h2 id="links-and-references">
<a class="anchor" href="#links-and-references" aria-hidden="true"><span class="octicon octicon-link"></span></a>Links and References</h2>

<ul>
  <li><a href="https://youtu.be/hPQKzsjTyyQ">Lesson 11 video</a></li>
  <li><a href="https://medium.com/@lankinen/fast-ai-lesson-11-notes-part-2-v3-6d28e17509f4">Laniken’s Lesson 11 Notes</a></li>
  <li>Notebooks:
    <ul>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/07a_lsuv.ipynb">07a_lsuv.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/08_data_block.ipynb">08_data_block.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/09_optimizers.ipynb">09_optimizers.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/09c_add_progress_bar.ipynb">09c_add_progress_bar.ipynb</a></li>
      <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl2/10_augmentation.ipynb">10_augmentation.ipynb</a></li>
    </ul>
  </li>
  <li>Papers:
    <ul>
      <li>LSUV Paper: <em><a href="https://arxiv.org/abs/1511.06422">All You Need is a Good Init</a></em>
</li>
      <li><a href="http://arxiv.org/abs/1706.05350"><em>L2 Regularization versus Batch and Weight Normalization [2017]</em></a></li>
      <li><a href="https://arxiv.org/abs/1810.12281"><em>Three mechanisms of weight decay regularization [2019]</em></a></li>
      <li><a href="http://arxiv.org/abs/1803.01814"><em>Norm matters: Efficient and accurate normalization schemes in deep networks [2018]</em></a></li>
      <li><a href="https://blog.janestreet.com/l2-regularization-and-batch-norm/"><em>Jane Street Tech Blog - L2 Regularization and Batch Norm [2019]</em></a></li>
      <li>Adam Paper - <a href="http://arxiv.org/abs/1412.6980"><em>Adam: A method for stochastic optimization [2015]</em></a>
</li>
      <li><a href="https://doi.org/10.1109/IJCNN.2017.7966082"><em>Nesterov’s accelerated gradient and momentum as approximations to regularised update descent [2017]</em></a></li>
      <li>LAMB Paper - <a href="https://arxiv.org/abs/1904.00962"><em>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</em> [2019]</a>
</li>
      <li>LARS Paper - <em><a href="https://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks [2017]</a></em> (LARS also uses weight statistics, not just gradient statistics.)</li>
      <li>
<em><a href="https://arxiv.org/abs/1804.04235">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost [2018]</a></em> (Adafactor combines stats over multiple sets of axes)</li>
      <li><em><a href="https://arxiv.org/abs/1902.09843">Adaptive Gradient Methods with Dynamic Bound of Learning Rate [2019]</a></em></li>
    </ul>
  </li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jimypbr/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/2020/08/16/fast-ai-v3-lesson-11-notes-data-block-api-the-generic-optimizer-data-augmentation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jim Briggs&#39; blog about ML, software, etc</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
