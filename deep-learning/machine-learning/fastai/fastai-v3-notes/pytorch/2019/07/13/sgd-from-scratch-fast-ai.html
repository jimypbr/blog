<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>SGD From Scratch in PyTorch | go-seq</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="SGD From Scratch in PyTorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A deeper dive into fast.ai v3 Lesson 2: SGD from Scratch." />
<meta property="og:description" content="A deeper dive into fast.ai v3 Lesson 2: SGD from Scratch." />
<link rel="canonical" href="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html" />
<meta property="og:url" content="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html" />
<meta property="og:site_name" content="go-seq" />
<meta property="og:image" content="https://jimypbr.github.io/blog/images/fastai/image-20190713170857047.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-13T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html","@type":"BlogPosting","headline":"SGD From Scratch in PyTorch","dateModified":"2019-07-13T00:00:00-05:00","datePublished":"2019-07-13T00:00:00-05:00","image":"https://jimypbr.github.io/blog/images/fastai/image-20190713170857047.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html"},"description":"A deeper dive into fast.ai v3 Lesson 2: SGD from Scratch.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimypbr.github.io/blog/feed.xml" title="go-seq" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       processEscapes: true
     }
   });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">go-seq</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">SGD From Scratch in PyTorch</h1><p class="page-description">A deeper dive into fast.ai v3 Lesson 2: SGD from Scratch.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-07-13T00:00:00-05:00" itemprop="datePublished">
        Jul 13, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine-learning">machine-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai-v3-notes">fastai-v3-notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#pytorch">pytorch</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#overview">Overview</a></li>
<li class="toc-entry toc-h2"><a href="#linear-regression">Linear Regression</a>
<ul>
<li class="toc-entry toc-h3"><a href="#loss-function">Loss Function</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#gradient-descent">Gradient Descent</a>
<ul>
<li class="toc-entry toc-h3"><a href="#animate-it">Animate it!</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a>
<ul>
<li class="toc-entry toc-h3"><a href="#animate-it-1">Animate it!</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#experiments-with-the-learning-rate-and-batch-size">Experiments with the Learning Rate and Batch Size</a>
<ul>
<li class="toc-entry toc-h3"><a href="#learning-rate">Learning Rate</a></li>
<li class="toc-entry toc-h3"><a href="#batch-size">Batch Size</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul><h2 id="overview">
<a class="anchor" href="#overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h2>

<p>In this post I explore Stochastic Gradient Descent (SGD) which is an <strong>optimization</strong> method commonly used in neural networks. This continues Lesson 2 of fast.ai on Stochastic Gradient Descent (SGD).  I will copy from the <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-sgd.ipynb">fast.ai notebook on SGD</a> and dig deeper into the what’s going on there.</p>

<h2 id="linear-regression">
<a class="anchor" href="#linear-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Regression</h2>

<p>We will start with the simplest model - the Linear model. Mathematically this is represented as:</p>

\[\vec{y} = X \vec{a} + \vec{b}\]

<p>Where $X$ is a matrix where each of the rows is a data point, $\vec{a}$ is the vector of model weights, and $\vec{b}$ is a bias vector. In the 1D case, these would correspond to the familiar ‘slope’ and ‘intercept’ of a line. We can make this more compact by combining the bias inside of the model weights and adding an extra column to $X$ with all values set to one. These are represented in Pytorch as <strong>tensors</strong>.</p>

<p>In Pytorch, a <code class="language-plaintext highlighter-rouge">tensor</code> is a data structure that encompasses arrays of any dimension. A vector is a tensor of rank 1, while a matrix is a tensor of rank 2. For simplicity we will stick to the case of a 1D linear model. In PyTorch $X$ would then be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">].</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>The model has two parameters and there are <code class="language-plaintext highlighter-rouge">n=100</code> datapoints. <code class="language-plaintext highlighter-rouge">x</code> therefore has shape <code class="language-plaintext highlighter-rouge">(100, 2)</code>. The <code class="language-plaintext highlighter-rouge">.uniform_(-1., 1)</code> generates floating point numbers between -1 and 1. The trailing <code class="language-plaintext highlighter-rouge">_</code> is PyTorch convention that the function operates <em>inplace</em>.</p>

<p>Let’s look at the first 5 values of <code class="language-plaintext highlighter-rouge">x</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">x</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.7893</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.7556</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0055</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2465</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0080</span><span class="p">,</span>  <span class="mf">1.0000</span><span class="p">]])</span>
</code></pre></div></div>

<p>Notice how the second column is all 1s - this is the bias.</p>

<p>We’ll now set the true values for the model weights, $a$,  to slope=3 and intersection=10:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">a_true</span> <span class="o">=</span> <span class="n">a</span>
</code></pre></div></div>

<p>With <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">a</code> set we can now generate some fake data with some small normally distributed random noise:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">@</span><span class="n">a</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.6</span>
</code></pre></div></div>

<p><img src="/blog/images/fastai/image-20190711224405542.png" alt="image-20190711224405542"></p>

<h3 id="loss-function">
<a class="anchor" href="#loss-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss Function</h3>

<p>We want to find <strong>parameters</strong> (weights) <code class="language-plaintext highlighter-rouge">a</code> such that they minimize the <em>error</em> between the points and the line <code class="language-plaintext highlighter-rouge">x@a</code>. Note that here <code class="language-plaintext highlighter-rouge">a</code> is unknown. For a regression problem the most common <em>error function</em> or <em>loss function</em> is the <strong>mean squared error</strong>. In python this function is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> 
    <span class="k">return</span> <span class="p">((</span><span class="n">y_hat</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>Where <code class="language-plaintext highlighter-rouge">y</code> is the true value and <code class="language-plaintext highlighter-rouge">y_hat</code> is the predicted value.</p>

<p>We start with guess at the value of the weights <code class="language-plaintext highlighter-rouge">a</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>We can make prediction for y, <code class="language-plaintext highlighter-rouge">y_hat</code>, and compute the error against the known values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">@</span><span class="n">a</span>
<span class="o">&gt;</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">92.9139</span><span class="p">)</span>
</code></pre></div></div>

<p>So far we have specified the <em>model</em> (linear regression) and the <em>evaluation criteria</em> (or <em>loss function</em>). Now we need to handle <em>optimization</em>; that is, how do we find the best values for <code class="language-plaintext highlighter-rouge">a</code>? How do we find the best <em>fitting</em> linear regression.</p>

<h2 id="gradient-descent">
<a class="anchor" href="#gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Descent</h2>

<p>We would like to find the values of <code class="language-plaintext highlighter-rouge">a</code> that minimize <code class="language-plaintext highlighter-rouge">mse_loss</code>. <strong>Gradient descent</strong> is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the <em>function gradient</em>. Here is gradient descent implemented in PyTorch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">():</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">@</span><span class="n">a</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
      	<span class="c1"># don't compute the gradient in here
</span>        <span class="n">a</span><span class="p">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
        
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span> 
  <span class="n">update</span><span class="p">()</span>
</code></pre></div></div>

<p>We are going to create a loop. We’re going to loop through 100 times, and we’re going to call a function called <code class="language-plaintext highlighter-rouge">update</code>. That function is going to:</p>

<ul>
  <li>Calculate <code class="language-plaintext highlighter-rouge">y_hat</code> (i.e. our prediction)</li>
  <li>Calculate loss (i.e. our mean squared error)</li>
  <li>
<strong>Calculate the gradient</strong>. In PyTorch, calculating the gradient is done by using a method called <code class="language-plaintext highlighter-rouge">backward</code>. Mean squared error was just a simple standard mathematical function. PyTorch keeps track of how it was calculated and lets us automatically calculate the derivative. So if you do a mathematical operation on a tensor in PyTorch, you can call <code class="language-plaintext highlighter-rouge">backward</code> to calculate the derivative and the derivative gets stuck inside an attribute called <code class="language-plaintext highlighter-rouge">.grad</code>.</li>
  <li>Then take the weights <code class="language-plaintext highlighter-rouge">a</code> and subtract the gradient from them (<code class="language-plaintext highlighter-rouge">sub_</code>). There is an underscore there because that’s going to do it in-place. It’s going to actually update those coefficients <code class="language-plaintext highlighter-rouge">a</code> to subtract the gradients from them. Why do we subtract? Because the gradient tells us if the whole thing moves downwards, the loss goes up. If the whole thing moves upwards, the loss goes down. So we want to do the opposite of the thing that makes it go up. We want our loss to be small. That’s why we subtract.</li>
  <li>
<code class="language-plaintext highlighter-rouge">lr</code> is our learning rate. All it is is the thing that we multiply by the gradient.</li>
</ul>

<h3 id="animate-it">
<a class="anchor" href="#animate-it" aria-hidden="true"><span class="octicon octicon-link"></span></a>Animate it!</h3>

<p>Here is an animation of the training gradient descent with learning rate <code class="language-plaintext highlighter-rouge">LR=0.1</code></p>

<p><img src="/blog/images/fastai/line_gd_0.1.gif" alt="gd-1e-1"></p>

<p>Notice how it seems to spring up to find the intercept first then adjusts to get the slope right. The starting guess at the intercept is 1, while the real value is 10. At the start this would cause the biggest loss so the we would expect the gradient on the intercept parameter to be higher than the gradient on the slope parameter.</p>

<p>It sucessfully recovers, more or less, the weights that we generated the data with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">3.0332</span><span class="p">,</span> <span class="mf">9.9738</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="stochastic-gradient-descent">
<a class="anchor" href="#stochastic-gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Gradient Descent</h2>

<p>The gradient descent algorithm calculates the loss across the entire dataset <em>every</em> iteration. For this problem this works great, but it won’t scale. If we were training on imagenet then we’d have to compute the loss on 1.5 million images just to do a single update of the parameters. This would be both incredibly slow and also impossible to fit into computer memory. Instead we grab random <em>mini-batches</em> of 64, or so, data points and compute the loss and gradient with those and then update the weights. As code this looks almost identical to before, but with some random indexes added to <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_mini</span><span class="p">(</span><span class="n">rand_idx</span><span class="p">):</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">]</span><span class="o">@</span><span class="n">a</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">],</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">a</span><span class="p">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">a</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div></div>

<p>Using mini-batches approximates the gradient, but also adds random noise to the optimiser causing the parameters to ‘jump around’ a little. This can make it require more iterations to converge. We will see this visually in the next section. On the other hand, some random noise is a good thing in training neural networks because it allows the optimiser to better explore the high dimensional parameter space and potentially find a solution with a lower loss.</p>

<h3 id="animate-it-1">
<a class="anchor" href="#animate-it-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Animate it!</h3>

<p>Here is an animation of the training with batch size of 16:</p>

<p><img src="/blog/images/fastai/line_sgd_0.1_16.gif" alt="line_sgd_0.1_32"></p>

<p>It converges on the same answer as gradient descent, but it is a little slower and has a bit of jitter that isn’t in the gradient descent animation.</p>

<h2 id="experiments-with-the-learning-rate-and-batch-size">
<a class="anchor" href="#experiments-with-the-learning-rate-and-batch-size" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiments with the Learning Rate and Batch Size</h2>

<p>We can gain a better understanding of how SGD works by playing with the parameters, learning rate and batch size, and visualising the learning process.</p>

<h3 id="learning-rate">
<a class="anchor" href="#learning-rate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Rate</h3>

<p>Here the learning rate in SGD is varied, keeping the batch size fixed at 16.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th>Animation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">LR=1e-2</code><br><code class="language-plaintext highlighter-rouge">bs=16</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.01_16.gif" alt="sgd_16_1e-2"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">LR=1e-1</code><br><code class="language-plaintext highlighter-rouge">bs=16</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_16.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">LR=1.0</code> <br><code class="language-plaintext highlighter-rouge">bs=16</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_1.0_16.gif" alt="sgd_16_1e-0"></td>
    </tr>
  </tbody>
</table>

<p>With the learning rate of <code class="language-plaintext highlighter-rouge">0.01</code> it too small and it takes an age, but it does eventually converge on the right answer. With a learning rate of <code class="language-plaintext highlighter-rouge">1.0</code> the whole thing goes off the rails and it can’t get anywhere near the right answer.</p>

<h3 id="batch-size">
<a class="anchor" href="#batch-size" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Size</h3>

<p>Here the batch size in SGD is varied, holding the learning rate fixed at <code class="language-plaintext highlighter-rouge">LR=0.1</code>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th>Animation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=1</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_1.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=2</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_2.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=4</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_4.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=8</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_8.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=16</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_16.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=32</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_32.gif" alt="sgd_16_1e-1"></td>
    </tr>
    <tr>
      <td style="text-align: center">SGD <code class="language-plaintext highlighter-rouge">bs=64</code>
</td>
      <td><img src="/blog/images/fastai/line_sgd_0.1_64.gif" alt="sgd_16_1e-1"></td>
    </tr>
  </tbody>
</table>

<p>All of the instances do converge to the right answer in this case (though in general that wouldn’t be the case for all problems). For <code class="language-plaintext highlighter-rouge">bs=1</code> it jumps around a lot even after it gets into the right place. This is because the weights are updated using only one data point every iteration. So it jitters around the right solution and will never stop jittering with more iterations.</p>

<p>However with increasing batch size the jitter gets less and less. At batch size of 64 the animation is almost identical to the gradient descent animation. This makes sense since <code class="language-plaintext highlighter-rouge">n=100</code> so with <code class="language-plaintext highlighter-rouge">bs=64</code> we have almost gone back to the full gradient descent algorithm (which would be <code class="language-plaintext highlighter-rouge">bs=n</code> ).</p>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<ul>
  <li><a href="https://youtu.be/ccMHJeQU4Qw">Link to Lesson 2 lecture</a></li>
  <li><a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-sgd.ipynb">SGD From Scratch Notebook</a></li>
  <li>
<a href="https://github.com/hiromis/notes/blob/master/Lesson2.md">Lesson2 Detailed Notes by @hiromi</a>.</li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jimypbr/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/deep-learning/machine-learning/fastai/fastai-v3-notes/pytorch/2019/07/13/sgd-from-scratch-fast-ai.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jim Briggs&#39; blog about ML, software, etc</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
