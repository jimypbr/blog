<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fast.ai v3 Lesson 1 Notes: Image Classification | go-seq</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Fast.ai v3 Lesson 1 Notes: Image Classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My personal notes on Lesson 1 of part 1 of fast.ai v3 – Image classification" />
<meta property="og:description" content="My personal notes on Lesson 1 of part 1 of fast.ai v3 – Image classification" />
<link rel="canonical" href="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/2019/07/06/fast-ai-lesson-1-notes-image-classification.html" />
<meta property="og:url" content="https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/2019/07/06/fast-ai-lesson-1-notes-image-classification.html" />
<meta property="og:site_name" content="go-seq" />
<meta property="og:image" content="https://jimypbr.github.io/blog/images/fastai/image-20190706182251357.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-06T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/2019/07/06/fast-ai-lesson-1-notes-image-classification.html","@type":"BlogPosting","headline":"Fast.ai v3 Lesson 1 Notes: Image Classification","dateModified":"2019-07-06T00:00:00-05:00","datePublished":"2019-07-06T00:00:00-05:00","image":"https://jimypbr.github.io/blog/images/fastai/image-20190706182251357.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimypbr.github.io/blog/deep-learning/machine-learning/fastai/2019/07/06/fast-ai-lesson-1-notes-image-classification.html"},"description":"My personal notes on Lesson 1 of part 1 of fast.ai v3 – Image classification","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimypbr.github.io/blog/feed.xml" title="go-seq" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       processEscapes: true
     }
   });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">go-seq</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fast.ai v3 Lesson 1 Notes: Image Classification</h1><p class="page-description">My personal notes on Lesson 1 of part 1 of fast.ai v3 -- Image classification</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-07-06T00:00:00-05:00" itemprop="datePublished">
        Jul 6, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#machine-learning">machine-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#fastai">fastai</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#overview-of-lesson">Overview of Lesson</a></li>
<li class="toc-entry toc-h2"><a href="#task-1---world-class-image-classifier">Task 1 - World Class Image Classifier</a></li>
<li class="toc-entry toc-h2"><a href="#load-the-data">Load the Data</a></li>
<li class="toc-entry toc-h2"><a href="#training-a-model-using-a-pretrained-resnet">Training a Model using a Pretrained ResNet</a></li>
<li class="toc-entry toc-h2"><a href="#getting-started-with-the-notebooks">Getting Started With the Notebooks</a></li>
<li class="toc-entry toc-h2"><a href="#jeremy-says">Jeremy Says…</a></li>
<li class="toc-entry toc-h2"><a href="#q--a">Q &amp; A</a></li>
<li class="toc-entry toc-h2"><a href="#links-and-references">Links and References</a></li>
</ul><p><img src="/blog/images/fastai/image-20190706182251357.png" alt="image-20190706182251357"></p>

<h2 id="overview-of-lesson">
<a class="anchor" href="#overview-of-lesson" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview of Lesson</h2>

<p>This is the introductory lesson to fastai part 1!</p>

<p>The key outcome of this lesson is that we’ll have trained an image classifier which can recognize pet breeds at state of the art accuracy. The key to this success is the use of <em>transfer learning</em>, which will be a key platform for much of this course. We’ll also see how to analyze the model to understand its failure modes. In this case, we’ll see that the places where the model is making mistakes is in the same areas that even breeding experts can make mistakes.</p>

<h2 id="task-1---world-class-image-classifier">
<a class="anchor" href="#task-1---world-class-image-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Task 1 - World Class Image Classifier</h2>

<ul>
  <li>
    <p>Fastai opts to teach deep learning <em>backwards</em> - rather than starting at the level of neurons they start with learning to use the state of the art algorithms and networks from the beginning. Learning to become a practitioner with the best practices first and then gradually learning the technical details later.</p>
  </li>
  <li>
    <p>Task 1: Training a world class image classification model.</p>
  </li>
  <li>
    <p>Image classification has been one of deep learning’s biggest successes so far.</p>
  </li>
  <li>
    <p>10 years ago separating cat and dog images <em>was a hard problem</em>. With classical methods researchers were scoring ~80%. With today’s algorithms it’s actually too easy and scores on the cats vs dogs dataset are almost 100%. That’s why we used the harder dataset of cat and dog breeds.</p>
  </li>
  <li>
    <p>Cat breeds and dog breeds dataset from Oxford: <a href="https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset">Cats and Dogs Breeds Classification Oxford Dataset | Kaggle</a>.</p>
  </li>
  <li>
    <p>This task found in the Jupyter notebook: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb">lesson1-pets.pynb</a></p>
  </li>
</ul>

<h2 id="load-the-data">
<a class="anchor" href="#load-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Load the Data</h2>

<ul>
  <li>
    <p>FastAI has its own way of handling different datasets: the <em>DataBunch API</em>.</p>
  </li>
  <li>
    <p>This integrates the loading of different types of data, the labeling, splitting into train/val/test, and the data transformations for standardisation, normalisation, and training augmentation.</p>
  </li>
  <li>
    <p>To load the cats and dogs dataset:</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">pat</span> <span class="o">=</span> <span class="s">r'/([^/]+)_\d+.jpg$'</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ImageDataBunch</span><span class="p">.</span><span class="n">from_name_re</span><span class="p">(</span><span class="n">path_img</span><span class="p">,</span> 
                                   <span class="n">fnames</span><span class="p">,</span> 
                                   <span class="n">pat</span><span class="p">,</span> 
                                   <span class="n">ds_tfms</span><span class="o">=</span><span class="n">get_transforms</span><span class="p">(),</span>
                                   <span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span>
                                   <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                   <span class="p">).</span><span class="n">normalize</span><span class="p">(</span><span class="n">imagenet_stats</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>What is this doing? Let’s look at the docs:</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ImageDataBunch</span><span class="p">(</span><span class="n">fastai</span><span class="p">.</span><span class="n">basic_data</span><span class="p">.</span><span class="n">DataBunch</span><span class="p">)</span>
 <span class="o">|</span>  <span class="n">DataBunch</span> <span class="n">suitable</span> <span class="k">for</span> <span class="n">computer</span> <span class="n">vision</span><span class="p">.</span>
 <span class="p">...</span>
 <span class="o">|</span>  <span class="n">from_name_re</span><span class="p">(</span><span class="n">path</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">pathlib</span><span class="p">.</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> 
                 <span class="n">fnames</span><span class="p">:</span><span class="n">Collection</span><span class="p">[</span><span class="n">pathlib</span><span class="p">.</span><span class="n">Path</span><span class="p">],</span> 
                 <span class="n">pat</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> 
                 <span class="n">valid_pct</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">from</span> <span class="n">builtins</span><span class="p">.</span><span class="nb">type</span>
 <span class="o">|</span>      <span class="n">Create</span> <span class="k">from</span> <span class="nb">list</span> <span class="n">of</span> <span class="sb">`fnames`</span> <span class="ow">in</span> <span class="sb">`path`</span> <span class="k">with</span> <span class="n">re</span> <span class="n">expression</span> <span class="sb">`pat`</span><span class="p">.</span>
</code></pre></div>    </div>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">get_transforms</code> is a function that returns a list of default image transformations for data augmentation.</li>
  <li>
<code class="language-plaintext highlighter-rouge">size=224</code> resizes the images to <code class="language-plaintext highlighter-rouge">224x224</code>. This is the size that the network we are using (resnet34) has been trained on.</li>
  <li>
<code class="language-plaintext highlighter-rouge">bs</code> is the batchsize.</li>
  <li>
<code class="language-plaintext highlighter-rouge">normalize(imagenet_stats)</code> normalizes the images so that the pixel values are between 0 and 1 (necessary for using the neural network). The network has been pretrained on imagenet data so we need to normalize our new data with respect to the imagenet data. This way the images are placed near the distribution that the network was trained on and so gives the network data that it is ‘used to’ seeing.</li>
</ul>

<h2 id="training-a-model-using-a-pretrained-resnet">
<a class="anchor" href="#training-a-model-using-a-pretrained-resnet" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training a Model using a Pretrained ResNet</h2>

<ul>
  <li>
    <p>‘ResNet’ is the name of a particular kind of Convolutional Neural Network (CNN). Details of it will be covered later.</p>
  </li>
  <li>
    <p>The ResNet we will use has been <strong>pretrained</strong>. This means that it was trained to solve another image classification problem (namely <em>ImageNet</em>) and we are reusing the learned weights of that network as a starting point for a new imaging problem.</p>
  </li>
  <li>
    <p>Why ResNet and not some other architecture? From looking at benchmarks it has been found that ResNet generally ‘just works well’ for image tasks. (See also question in Q &amp; A section below).</p>
  </li>
  <li>
    <p>Here’s how to create a CNN with the fastai library:</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">create_cnn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">data</code> is the DataBunch object of the cats/dogs data we created earlier.</p>
  </li>
  <li>
    <p>Here we are using a variant of ResNet called <code class="language-plaintext highlighter-rouge">resnet34</code>. The 34 simply means it has 34 layers. There are others avaiable with 18, 50, and more layers.</p>
  </li>
  <li>
    <p><strong>One-cycle policy</strong>:</p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">learn.fit_one_cycle(4)</code></p>
      </li>
      <li>
        <p>People train neural networks using Stochastic Gradient Descent (SGD). Here the training set is divided into random batches (say of size 64) and the network weights are updated after each batch. After the network has seen all the batches, this is called an <em>epoch</em>. The rate at which the weights are changed is called the <strong>learning rate</strong>. Typically people set this to a single value that remains unchanged during an epoch. Not here though.</p>
      </li>
      <li>
        <p>The <em>One-cycle policy</em> is a way of training the neural network using SGD faster by varying the learning rate and solver momentum over a group of epochs.</p>
      </li>
      <li>
        <p>Sylvain explains (<a href="https://sgugger.github.io/the-1cycle-policy.html">source</a>):</p>

        <blockquote>
          <p>He [Leslie] recommends to do a cycle with two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.</p>
        </blockquote>
      </li>
      <li>
        <p>Here are plots of how the learning rate and momentum vary over the iterations (batches):</p>

        <p><img src="/images/fastai/image-20190912170631570.png" alt="image-20190912170631570"></p>
      </li>
      <li>
        <p>The peak of the learning rate has a value of 1x the inputted learning rate. The bottom value is 0.1x the inputted learning rate. The bottom of the momentum value is 0.85x the inputted momentum value.</p>
      </li>
      <li>
        <p>The momentum varies contra to the learning rate. What’s the intuition behind this? When the learning rate is high we want momentum to be lower. This enables the SGD to quickly change directions and find a flatter region in parameter space.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Learning Rate Finder</strong></p>

    <ul>
      <li>
        <p>The method is basically successively increasing $\eta$ every batch using either a linear or exponential schedule and looking the loss. While $\eta$ has a good value, the loss will be decreasing. When $\eta$ gets too large the loss will start to increase. You can plot the loss versus $\eta$ and see by eye a learning rate that is largest where the loss is decreasing fastest.</p>
      </li>
      <li>
        <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>learn.lr_find()
learn.recorder.plot()
</code></pre></div>        </div>
      </li>
      <li>
        <p><img src="https://github.com/hiromis/notes/raw/master/lesson1/11.png" alt="img"></p>
      </li>
      <li>More on this will be covered in the next lesson.</li>
    </ul>
  </li>
</ul>

<h2 id="getting-started-with-the-notebooks">
<a class="anchor" href="#getting-started-with-the-notebooks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started With the Notebooks</h2>

<ul>
  <li>All the course notebooks for part 1 are found here: <a href="https://github.com/fastai/course-v3/tree/master/nbs/dl1">notebooks [github]</a>.</li>
  <li>The course guide can be found here: <a href="https://course.fast.ai/index.html">course guide</a>.</li>
  <li>For running and experimenting with the fastai notebooks I personally like to use:
    <ul>
      <li><a href="https://course.fast.ai/start_kaggle.html">kaggle kernels</a></li>
      <li>or <a href="https://course.fast.ai/start_gcp.html">google colab</a>.</li>
    </ul>
  </li>
</ul>

<h2 id="jeremy-says">
<a class="anchor" href="#jeremy-says" aria-hidden="true"><span class="octicon octicon-link"></span></a>Jeremy Says…</h2>

<ol>
  <li>Don’t try to stop and understand everything.</li>
  <li>Don’t waste your time, learn Jupyter keyboard shortcuts. Learn 4 to 5 each day.</li>
  <li>Please run the code, really run the code. Don’t go deep on theory. Play with the code, see what goes in and what comes out.</li>
  <li>Pick one project. Do it really well. Make it fantastic.</li>
  <li>Run this notebook (lesson1-pets.ipynb), but then get your own dataset and run it! (extra emphasis: do this!) If you have a lot of categories, don’t run confusion matrix, run… <code class="language-plaintext highlighter-rouge">interp.most_confused(min_val=n)</code>
</li>
</ol>

<p>(<a href="https://forums.fast.ai/t/things-jeremy-says-to-do/36682">Source: Robert Bracco</a>)</p>

<h2 id="q--a">
<a class="anchor" href="#q--a" aria-hidden="true"><span class="octicon octicon-link"></span></a>Q &amp; A</h2>

<ul>
  <li>
    <p><em>As GPU mem will be in power of 2, doesn’t size 256 sound more practical considering GPU utilization compared to 224?</em></p>

    <blockquote>
      <p>The brief answer is that the models are designed so that the final layer is of size 7 by 7, so we actually want something where if you go 7 times 2 a bunch of times (224 = 7x2x2x2x2x2), then you end up with something that’s a good size. 
Objects often appear in the middle of an image in the ImageNet dataset. After 5 maxpools, a 224x224 will be 7x7 meaning that it will have a centerpoint. A 256x256 image will be 8x8 and not have a distinct centerpoint.</p>
    </blockquote>
  </li>
  <li>
    <p><em>Why resnet and not inception architecture?</em></p>

    <blockquote>
      <p>Resnet is Good Enough!
See the DAWN benchmarks - the top 4 are all Resnet.You can consider different models for different use cases.
For example, if you want to do edge computing, mobile apps, Jeremy still suggests running the model on the local server and port results to the mobile device. But if you want to run something on the low powered device, there are special architectures for that.</p>

      <p>Inception is pretty memory intensive.
fastai wants to show you ways to run your model without much fine-tuning and still achieve good results.
The kind of stuff that always tends to work. Resnet works well on a lot of image classification applications.</p>
    </blockquote>
  </li>
  <li>
    <p><em>Will the library use multi GPUs in parallel by default?</em></p>

    <blockquote>
      <p>The library will use multiple CPUs by default but just one GPU by default. We probably won’t be looking at multi GPU until part 2. It’s easy to do and you’ll find it on the forum, but most people won’t be needing to use that now.</p>
    </blockquote>
  </li>
</ul>

<h2 id="links-and-references">
<a class="anchor" href="#links-and-references" aria-hidden="true"><span class="octicon octicon-link"></span></a>Links and References</h2>

<ul>
  <li><a href="https://youtu.be/XfoYk_Z5AkI">Link to Lesson 1 lecture</a></li>
  <li>Homework notebooks:
    <ul>
      <li>Notebook 1: <a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb">lesson1-pets.pynb</a>
</li>
    </ul>
  </li>
  <li>
<a href="https://github.com/hiromis/notes/blob/master/Lesson1.md">Detailed lesson notes</a> - thanks to @hiromi</li>
  <li><a href="https://dawn.cs.stanford.edu/benchmark/">Stanford DAWN Deep Learning Benchmark (DAWNBench) ·</a></li>
  <li><a href="https://arxiv.org/abs/1311.2901">[1311.2901] Visualizing and Understanding Convolutional Networks</a></li>
  <li><a href="https://sgugger.github.io/the-1cycle-policy.html">Another data science student’s blog – The 1cycle policy</a></li>
  <li><a href="https://arxiv.org/pdf/1506.01186.pdf">Learning Rate Finder Paper</a></li>
</ul>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jimypbr/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/deep-learning/machine-learning/fastai/2019/07/06/fast-ai-lesson-1-notes-image-classification.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jim Briggs&#39; blog about ML, software, etc</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jimypbr" title="jimypbr"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
